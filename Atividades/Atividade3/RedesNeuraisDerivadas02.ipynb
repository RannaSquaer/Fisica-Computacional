{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227dc304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def generate_data(nx, qtde, pmax):\n",
    "    x = np.linspace(-1, 1, nx).reshape(-1, 1)\n",
    "    y = []\n",
    "    dy = []\n",
    "    \n",
    "    for _ in range(qtde//2):\n",
    "        # definindo variáveis aleaórias\n",
    "        A = np.random.randint(-1, 1)\n",
    "        a = np.random.randint(0, 2)\n",
    "        b = np.random.randint(0, 2)\n",
    "        \n",
    "        # polinomio de grau p aleatório\n",
    "        p3 = A*(x**2 - a**2)*x\n",
    "        p4 = A*(x**2 - a**2)*(x**2 - b**2)\n",
    "    \n",
    "        #noise = 0.1 * np.random.randn(len(x)).reshape(-1, 1)\n",
    "        \n",
    "        y.append(polinomio / np.max(np.abs(polinomio)) + noise)\n",
    "        \n",
    "        # derivada\n",
    "        noise = 0.1 * np.random.randn(len(x)).reshape(-1, 1)\n",
    "        dy.append(np.polyval(np.polyder(coeffs), x) / np.max(np.abs(polinomio)) + noise)\n",
    "    \n",
    "    # empilha dados\n",
    "    y = np.hstack(y).T\n",
    "    dy = np.hstack(dy).T\n",
    "    return y, dy\n",
    "\n",
    "# gerar dados\n",
    "y, dy = generate_data(5, 100, 10)\n",
    "\n",
    "print(y.shape)\n",
    "print(dy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a828e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.61267832\n",
      "Iteration 2, loss = 2.60842902\n",
      "Iteration 3, loss = 2.60431186\n",
      "Iteration 4, loss = 2.60032200\n",
      "Iteration 5, loss = 2.59645145\n",
      "Iteration 6, loss = 2.59268937\n",
      "Iteration 7, loss = 2.58902115\n",
      "Iteration 8, loss = 2.58542883\n",
      "Iteration 9, loss = 2.58189265\n",
      "Iteration 10, loss = 2.57839218\n",
      "Iteration 11, loss = 2.57490679\n",
      "Iteration 12, loss = 2.57141552\n",
      "Iteration 13, loss = 2.56789667\n",
      "Iteration 14, loss = 2.56432735\n",
      "Iteration 15, loss = 2.56068300\n",
      "Iteration 16, loss = 2.55693713\n",
      "Iteration 17, loss = 2.55306148\n",
      "Iteration 18, loss = 2.54902659\n",
      "Iteration 19, loss = 2.54480253\n",
      "Iteration 20, loss = 2.54035975\n",
      "Iteration 21, loss = 2.53566980\n",
      "Iteration 22, loss = 2.53070574\n",
      "Iteration 23, loss = 2.52544212\n",
      "Iteration 24, loss = 2.51985493\n",
      "Iteration 25, loss = 2.51392241\n",
      "Iteration 26, loss = 2.50762694\n",
      "Iteration 27, loss = 2.50095770\n",
      "Iteration 28, loss = 2.49391290\n",
      "Iteration 29, loss = 2.48650077\n",
      "Iteration 30, loss = 2.47873892\n",
      "Iteration 31, loss = 2.47065270\n",
      "Iteration 32, loss = 2.46227331\n",
      "Iteration 33, loss = 2.45363629\n",
      "Iteration 34, loss = 2.44478076\n",
      "Iteration 35, loss = 2.43574934\n",
      "Iteration 36, loss = 2.42658807\n",
      "Iteration 37, loss = 2.41734507\n",
      "Iteration 38, loss = 2.40806683\n",
      "Iteration 39, loss = 2.39879233\n",
      "Iteration 40, loss = 2.38954728\n",
      "Iteration 41, loss = 2.38034212\n",
      "Iteration 42, loss = 2.37117500\n",
      "Iteration 43, loss = 2.36203747\n",
      "Iteration 44, loss = 2.35291946\n",
      "Iteration 45, loss = 2.34381228\n",
      "Iteration 46, loss = 2.33470986\n",
      "Iteration 47, loss = 2.32560895\n",
      "Iteration 48, loss = 2.31650879\n",
      "Iteration 49, loss = 2.30741060\n",
      "Iteration 50, loss = 2.29831713\n",
      "Iteration 51, loss = 2.28923215\n",
      "Iteration 52, loss = 2.28016010\n",
      "Iteration 53, loss = 2.27110573\n",
      "Iteration 54, loss = 2.26207390\n",
      "Iteration 55, loss = 2.25306944\n",
      "Iteration 56, loss = 2.24409707\n",
      "Iteration 57, loss = 2.23516134\n",
      "Iteration 58, loss = 2.22626670\n",
      "Iteration 59, loss = 2.21741744\n",
      "Iteration 60, loss = 2.20861777\n",
      "Iteration 61, loss = 2.19987187\n",
      "Iteration 62, loss = 2.19118388\n",
      "Iteration 63, loss = 2.18255793\n",
      "Iteration 64, loss = 2.17399817\n",
      "Iteration 65, loss = 2.16550875\n",
      "Iteration 66, loss = 2.15709381\n",
      "Iteration 67, loss = 2.14875750\n",
      "Iteration 68, loss = 2.14050389\n",
      "Iteration 69, loss = 2.13233699\n",
      "Iteration 70, loss = 2.12426069\n",
      "Iteration 71, loss = 2.11627871\n",
      "Iteration 72, loss = 2.10839457\n",
      "Iteration 73, loss = 2.10061150\n",
      "Iteration 74, loss = 2.09293248\n",
      "Iteration 75, loss = 2.08536011\n",
      "Iteration 76, loss = 2.07789667\n",
      "Iteration 77, loss = 2.07054402\n",
      "Iteration 78, loss = 2.06330367\n",
      "Iteration 79, loss = 2.05617673\n",
      "Iteration 80, loss = 2.04916394\n",
      "Iteration 81, loss = 2.04226569\n",
      "Iteration 82, loss = 2.03548201\n",
      "Iteration 83, loss = 2.02881264\n",
      "Iteration 84, loss = 2.02225701\n",
      "Iteration 85, loss = 2.01581427\n",
      "Iteration 86, loss = 2.00948337\n",
      "Iteration 87, loss = 2.00326300\n",
      "Iteration 88, loss = 1.99715167\n",
      "Iteration 89, loss = 1.99114774\n",
      "Iteration 90, loss = 1.98524940\n",
      "Iteration 91, loss = 1.97945471\n",
      "Iteration 92, loss = 1.97376162\n",
      "Iteration 93, loss = 1.96816799\n",
      "Iteration 94, loss = 1.96267157\n",
      "Iteration 95, loss = 1.95727005\n",
      "Iteration 96, loss = 1.95196101\n",
      "Iteration 97, loss = 1.94674198\n",
      "Iteration 98, loss = 1.94161039\n",
      "Iteration 99, loss = 1.93656356\n",
      "Iteration 100, loss = 1.93159867\n",
      "Iteration 101, loss = 1.92671277\n",
      "Iteration 102, loss = 1.92190265\n",
      "Iteration 103, loss = 1.91716484\n",
      "Iteration 104, loss = 1.91249543\n",
      "Iteration 105, loss = 1.90788999\n",
      "Iteration 106, loss = 1.90334331\n",
      "Iteration 107, loss = 1.89884924\n",
      "Iteration 108, loss = 1.89440045\n",
      "Iteration 109, loss = 1.88998879\n",
      "Iteration 110, loss = 1.88560722\n",
      "Iteration 111, loss = 1.88125712\n",
      "Iteration 112, loss = 1.87696966\n",
      "Iteration 113, loss = 1.87284534\n",
      "Iteration 114, loss = 1.86901727\n",
      "Iteration 115, loss = 1.86529932\n",
      "Iteration 116, loss = 1.86130535\n",
      "Iteration 117, loss = 1.85723417\n",
      "Iteration 118, loss = 1.85336616\n",
      "Iteration 119, loss = 1.84968171\n",
      "Iteration 120, loss = 1.84607839\n",
      "Iteration 121, loss = 1.84249629\n",
      "Iteration 122, loss = 1.83891295\n",
      "Iteration 123, loss = 1.83532644\n",
      "Iteration 124, loss = 1.83175246\n",
      "Iteration 125, loss = 1.82823005\n",
      "Iteration 126, loss = 1.82481390\n",
      "Iteration 127, loss = 1.82151310\n",
      "Iteration 128, loss = 1.81822800\n",
      "Iteration 129, loss = 1.81488407\n",
      "Iteration 130, loss = 1.81155520\n",
      "Iteration 131, loss = 1.80831408\n",
      "Iteration 132, loss = 1.80514512\n",
      "Iteration 133, loss = 1.80200133\n",
      "Iteration 134, loss = 1.79885249\n",
      "Iteration 135, loss = 1.79569480\n",
      "Iteration 136, loss = 1.79254578\n",
      "Iteration 137, loss = 1.78942759\n",
      "Iteration 138, loss = 1.78633457\n",
      "Iteration 139, loss = 1.78321637\n",
      "Iteration 140, loss = 1.78002007\n",
      "Iteration 141, loss = 1.77672772\n",
      "Iteration 142, loss = 1.77333278\n",
      "Iteration 143, loss = 1.76991936\n",
      "Iteration 144, loss = 1.76704451\n",
      "Iteration 145, loss = 1.76470495\n",
      "Iteration 146, loss = 1.76157357\n",
      "Iteration 147, loss = 1.75846787\n",
      "Iteration 148, loss = 1.75570027\n",
      "Iteration 149, loss = 1.75294346\n",
      "Iteration 150, loss = 1.75003436\n",
      "Iteration 151, loss = 1.74706068\n",
      "Iteration 152, loss = 1.74423038\n",
      "Iteration 153, loss = 1.74160740\n",
      "Iteration 154, loss = 1.73890905\n",
      "Iteration 155, loss = 1.73599849\n",
      "Iteration 156, loss = 1.73312471\n",
      "Iteration 157, loss = 1.73041871\n",
      "Iteration 158, loss = 1.72777921\n",
      "Iteration 159, loss = 1.72509617\n",
      "Iteration 160, loss = 1.72236099\n",
      "Iteration 161, loss = 1.71963232\n",
      "Iteration 162, loss = 1.71694148\n",
      "Iteration 163, loss = 1.71425129\n",
      "Iteration 164, loss = 1.71153488\n",
      "Iteration 165, loss = 1.70882575\n",
      "Iteration 166, loss = 1.70615375\n",
      "Iteration 167, loss = 1.70350145\n",
      "Iteration 168, loss = 1.70083373\n",
      "Iteration 169, loss = 1.69813438\n",
      "Iteration 170, loss = 1.69541266\n",
      "Iteration 171, loss = 1.69268631\n",
      "Iteration 172, loss = 1.68996028\n",
      "Iteration 173, loss = 1.68722222\n",
      "Iteration 174, loss = 1.68445923\n",
      "Iteration 175, loss = 1.68167059\n",
      "Iteration 176, loss = 1.67885773\n",
      "Iteration 177, loss = 1.67601011\n",
      "Iteration 178, loss = 1.67310691\n",
      "Iteration 179, loss = 1.67012521\n",
      "Iteration 180, loss = 1.66703887\n",
      "Iteration 181, loss = 1.66382249\n",
      "Iteration 182, loss = 1.66056804\n",
      "Iteration 183, loss = 1.65757420\n",
      "Iteration 184, loss = 1.65426419\n",
      "Iteration 185, loss = 1.65121870\n",
      "Iteration 186, loss = 1.64827169\n",
      "Iteration 187, loss = 1.64510995\n",
      "Iteration 188, loss = 1.64198565\n",
      "Iteration 189, loss = 1.63889389\n",
      "Iteration 190, loss = 1.63561112\n",
      "Iteration 191, loss = 1.63239953\n",
      "Iteration 192, loss = 1.62915156\n",
      "Iteration 193, loss = 1.62573977\n",
      "Iteration 194, loss = 1.62224958\n",
      "Iteration 195, loss = 1.61871938\n",
      "Iteration 196, loss = 1.61503223\n",
      "Iteration 197, loss = 1.61130961\n",
      "Iteration 198, loss = 1.60767997\n",
      "Iteration 199, loss = 1.60422912\n",
      "Iteration 200, loss = 1.60112154\n",
      "Iteration 201, loss = 1.59812702\n",
      "Iteration 202, loss = 1.59474264\n",
      "Iteration 203, loss = 1.59111078\n",
      "Iteration 204, loss = 1.58762247\n",
      "Iteration 205, loss = 1.58432191\n",
      "Iteration 206, loss = 1.58106962\n",
      "Iteration 207, loss = 1.57775646\n",
      "Iteration 208, loss = 1.57436568\n",
      "Iteration 209, loss = 1.57096716\n",
      "Iteration 210, loss = 1.56760895\n",
      "Iteration 211, loss = 1.56426615\n",
      "Iteration 212, loss = 1.56090095\n",
      "Iteration 213, loss = 1.55747203\n",
      "Iteration 214, loss = 1.55394973\n",
      "Iteration 215, loss = 1.55034585\n",
      "Iteration 216, loss = 1.54667912\n",
      "Iteration 217, loss = 1.54294625\n",
      "Iteration 218, loss = 1.53910538\n",
      "Iteration 219, loss = 1.53507040\n",
      "Iteration 220, loss = 1.53074079\n",
      "Iteration 221, loss = 1.52603726\n",
      "Iteration 222, loss = 1.52091103\n",
      "Iteration 223, loss = 1.51534577\n",
      "Iteration 224, loss = 1.50937599\n",
      "Iteration 225, loss = 1.50310597\n",
      "Iteration 226, loss = 1.49668321\n",
      "Iteration 227, loss = 1.49020814\n",
      "Iteration 228, loss = 1.48367761\n",
      "Iteration 229, loss = 1.47707746\n",
      "Iteration 230, loss = 1.47050404\n",
      "Iteration 231, loss = 1.46412405\n",
      "Iteration 232, loss = 1.45804006\n",
      "Iteration 233, loss = 1.45220899\n",
      "Iteration 234, loss = 1.44646672\n",
      "Iteration 235, loss = 1.44065804\n",
      "Iteration 236, loss = 1.43476254\n",
      "Iteration 237, loss = 1.42888412\n",
      "Iteration 238, loss = 1.42311878\n",
      "Iteration 239, loss = 1.41744772\n",
      "Iteration 240, loss = 1.41176972\n",
      "Iteration 241, loss = 1.40600569\n",
      "Iteration 242, loss = 1.40014844\n",
      "Iteration 243, loss = 1.39424386\n",
      "Iteration 244, loss = 1.38835442\n",
      "Iteration 245, loss = 1.38253803\n",
      "Iteration 246, loss = 1.37684010\n",
      "Iteration 247, loss = 1.37128287\n",
      "Iteration 248, loss = 1.36585561\n",
      "Iteration 249, loss = 1.36052320\n",
      "Iteration 250, loss = 1.35524869\n",
      "Iteration 251, loss = 1.35000685\n",
      "Iteration 252, loss = 1.34478224\n",
      "Iteration 253, loss = 1.33956677\n",
      "Iteration 254, loss = 1.33436348\n",
      "Iteration 255, loss = 1.32918478\n",
      "Iteration 256, loss = 1.32404239\n",
      "Iteration 257, loss = 1.31894152\n",
      "Iteration 258, loss = 1.31388401\n",
      "Iteration 259, loss = 1.30887151\n",
      "Iteration 260, loss = 1.30390312\n",
      "Iteration 261, loss = 1.29897281\n",
      "Iteration 262, loss = 1.29407257\n",
      "Iteration 263, loss = 1.28919739\n",
      "Iteration 264, loss = 1.28434547\n",
      "Iteration 265, loss = 1.27951586\n",
      "Iteration 266, loss = 1.27470910\n",
      "Iteration 267, loss = 1.26992875\n",
      "Iteration 268, loss = 1.26517977\n",
      "Iteration 269, loss = 1.26046558\n",
      "Iteration 270, loss = 1.25578801\n",
      "Iteration 271, loss = 1.25114857\n",
      "Iteration 272, loss = 1.24654791\n",
      "Iteration 273, loss = 1.24198468\n",
      "Iteration 274, loss = 1.23745670\n",
      "Iteration 275, loss = 1.23296286\n",
      "Iteration 276, loss = 1.22850314\n",
      "Iteration 277, loss = 1.22407779\n",
      "Iteration 278, loss = 1.21968767\n",
      "Iteration 279, loss = 1.21533439\n",
      "Iteration 280, loss = 1.21101925\n",
      "Iteration 281, loss = 1.20674231\n",
      "Iteration 282, loss = 1.20250293\n",
      "Iteration 283, loss = 1.19830029\n",
      "Iteration 284, loss = 1.19413302\n",
      "Iteration 285, loss = 1.18999938\n",
      "Iteration 286, loss = 1.18589804\n",
      "Iteration 287, loss = 1.18182821\n",
      "Iteration 288, loss = 1.17778914\n",
      "Iteration 289, loss = 1.17378026\n",
      "Iteration 290, loss = 1.16980141\n",
      "Iteration 291, loss = 1.16585240\n",
      "Iteration 292, loss = 1.16193279\n",
      "Iteration 293, loss = 1.15804207\n",
      "Iteration 294, loss = 1.15417982\n",
      "Iteration 295, loss = 1.15034539\n",
      "Iteration 296, loss = 1.14653813\n",
      "Iteration 297, loss = 1.14275754\n",
      "Iteration 298, loss = 1.13900320\n",
      "Iteration 299, loss = 1.13527465\n",
      "Iteration 300, loss = 1.13157156\n",
      "Iteration 301, loss = 1.12789370\n",
      "Iteration 302, loss = 1.12424075\n",
      "Iteration 303, loss = 1.12061239\n",
      "Iteration 304, loss = 1.11700836\n",
      "Iteration 305, loss = 1.11342832\n",
      "Iteration 306, loss = 1.10987187\n",
      "Iteration 307, loss = 1.10633868\n",
      "Iteration 308, loss = 1.10282841\n",
      "Iteration 309, loss = 1.09934069\n",
      "Iteration 310, loss = 1.09587519\n",
      "Iteration 311, loss = 1.09243161\n",
      "Iteration 312, loss = 1.08900962\n",
      "Iteration 313, loss = 1.08560896\n",
      "Iteration 314, loss = 1.08222938\n",
      "Iteration 315, loss = 1.07887061\n",
      "Iteration 316, loss = 1.07553244\n",
      "Iteration 317, loss = 1.07221466\n",
      "Iteration 318, loss = 1.06891704\n",
      "Iteration 319, loss = 1.06563940\n",
      "Iteration 320, loss = 1.06238156\n",
      "Iteration 321, loss = 1.05914333\n",
      "Iteration 322, loss = 1.05592455\n",
      "Iteration 323, loss = 1.05272506\n",
      "Iteration 324, loss = 1.04954472\n",
      "Iteration 325, loss = 1.04638339\n",
      "Iteration 326, loss = 1.04324095\n",
      "Iteration 327, loss = 1.04011730\n",
      "Iteration 328, loss = 1.03701234\n",
      "Iteration 329, loss = 1.03392596\n",
      "Iteration 330, loss = 1.03085807\n",
      "Iteration 331, loss = 1.02780860\n",
      "Iteration 332, loss = 1.02477747\n",
      "Iteration 333, loss = 1.02176458\n",
      "Iteration 334, loss = 1.01876988\n",
      "Iteration 335, loss = 1.01579328\n",
      "Iteration 336, loss = 1.01283471\n",
      "Iteration 337, loss = 1.00989411\n",
      "Iteration 338, loss = 1.00697142\n",
      "Iteration 339, loss = 1.00406657\n",
      "Iteration 340, loss = 1.00117950\n",
      "Iteration 341, loss = 0.99831014\n",
      "Iteration 342, loss = 0.99545845\n",
      "Iteration 343, loss = 0.99262436\n",
      "Iteration 344, loss = 0.98980781\n",
      "Iteration 345, loss = 0.98700873\n",
      "Iteration 346, loss = 0.98422706\n",
      "Iteration 347, loss = 0.98146274\n",
      "Iteration 348, loss = 0.97871568\n",
      "Iteration 349, loss = 0.97598583\n",
      "Iteration 350, loss = 0.97327311\n",
      "Iteration 351, loss = 0.97057743\n",
      "Iteration 352, loss = 0.96789873\n",
      "Iteration 353, loss = 0.96523691\n",
      "Iteration 354, loss = 0.96259189\n",
      "Iteration 355, loss = 0.95996356\n",
      "Iteration 356, loss = 0.95735184\n",
      "Iteration 357, loss = 0.95475661\n",
      "Iteration 358, loss = 0.95217776\n",
      "Iteration 359, loss = 0.94961516\n",
      "Iteration 360, loss = 0.94706868\n",
      "Iteration 361, loss = 0.94453817\n",
      "Iteration 362, loss = 0.94202347\n",
      "Iteration 363, loss = 0.93952438\n",
      "Iteration 364, loss = 0.93704071\n",
      "Iteration 365, loss = 0.93457221\n",
      "Iteration 366, loss = 0.93211860\n",
      "Iteration 367, loss = 0.92967954\n",
      "Iteration 368, loss = 0.92725463\n",
      "Iteration 369, loss = 0.92484339\n",
      "Iteration 370, loss = 0.92244519\n",
      "Iteration 371, loss = 0.92005927\n",
      "Iteration 372, loss = 0.91768463\n",
      "Iteration 373, loss = 0.91531999\n",
      "Iteration 374, loss = 0.91296373\n",
      "Iteration 375, loss = 0.91061383\n",
      "Iteration 376, loss = 0.90826795\n",
      "Iteration 377, loss = 0.90592400\n",
      "Iteration 378, loss = 0.90358168\n",
      "Iteration 379, loss = 0.90124606\n",
      "Iteration 380, loss = 0.89893275\n",
      "Iteration 381, loss = 0.89666605\n",
      "Iteration 382, loss = 0.89445061\n",
      "Iteration 383, loss = 0.89223712\n",
      "Iteration 384, loss = 0.88997695\n",
      "Iteration 385, loss = 0.88770459\n",
      "Iteration 386, loss = 0.88547653\n",
      "Iteration 387, loss = 0.88329509\n",
      "Iteration 388, loss = 0.88113072\n",
      "Iteration 389, loss = 0.87895960\n",
      "Iteration 390, loss = 0.87677701\n",
      "Iteration 391, loss = 0.87459823\n",
      "Iteration 392, loss = 0.87245070\n",
      "Iteration 393, loss = 0.87034542\n",
      "Iteration 394, loss = 0.86824978\n",
      "Iteration 395, loss = 0.86613367\n",
      "Iteration 396, loss = 0.86402466\n",
      "Iteration 397, loss = 0.86195161\n",
      "Iteration 398, loss = 0.85990156\n",
      "Iteration 399, loss = 0.85785098\n",
      "Iteration 400, loss = 0.85579795\n",
      "Iteration 401, loss = 0.85376319\n",
      "Iteration 402, loss = 0.85175952\n",
      "Iteration 403, loss = 0.84976559\n",
      "Iteration 404, loss = 0.84776599\n",
      "Iteration 405, loss = 0.84578232\n",
      "Iteration 406, loss = 0.84382426\n",
      "Iteration 407, loss = 0.84187479\n",
      "Iteration 408, loss = 0.83992601\n",
      "Iteration 409, loss = 0.83799143\n",
      "Iteration 410, loss = 0.83607880\n",
      "Iteration 411, loss = 0.83417328\n",
      "Iteration 412, loss = 0.83227083\n",
      "Iteration 413, loss = 0.83038549\n",
      "Iteration 414, loss = 0.82851532\n",
      "Iteration 415, loss = 0.82664942\n",
      "Iteration 416, loss = 0.82479196\n",
      "Iteration 417, loss = 0.82295121\n",
      "Iteration 418, loss = 0.82112006\n",
      "Iteration 419, loss = 0.81929422\n",
      "Iteration 420, loss = 0.81748220\n",
      "Iteration 421, loss = 0.81568274\n",
      "Iteration 422, loss = 0.81388948\n",
      "Iteration 423, loss = 0.81210677\n",
      "Iteration 424, loss = 0.81033751\n",
      "Iteration 425, loss = 0.80857579\n",
      "Iteration 426, loss = 0.80682333\n",
      "Iteration 427, loss = 0.80508420\n",
      "Iteration 428, loss = 0.80335486\n",
      "Iteration 429, loss = 0.80163767\n",
      "Iteration 430, loss = 0.79994405\n",
      "Iteration 431, loss = 0.79828928\n",
      "Iteration 432, loss = 0.79672181\n",
      "Iteration 433, loss = 0.79509961\n",
      "Iteration 434, loss = 0.79335582\n",
      "Iteration 435, loss = 0.79152098\n",
      "Iteration 436, loss = 0.78998037\n",
      "Iteration 437, loss = 0.78843600\n",
      "Iteration 438, loss = 0.78661564\n",
      "Iteration 439, loss = 0.78504463\n",
      "Iteration 440, loss = 0.78352860\n",
      "Iteration 441, loss = 0.78176786\n",
      "Iteration 442, loss = 0.78023090\n",
      "Iteration 443, loss = 0.77870611\n",
      "Iteration 444, loss = 0.77699775\n",
      "Iteration 445, loss = 0.77550703\n",
      "Iteration 446, loss = 0.77396622\n",
      "Iteration 447, loss = 0.77231315\n",
      "Iteration 448, loss = 0.77085997\n",
      "Iteration 449, loss = 0.76930556\n",
      "Iteration 450, loss = 0.76771280\n",
      "Iteration 451, loss = 0.76627899\n",
      "Iteration 452, loss = 0.76472338\n",
      "Iteration 453, loss = 0.76319154\n",
      "Iteration 454, loss = 0.76176213\n",
      "Iteration 455, loss = 0.76022190\n",
      "Iteration 456, loss = 0.75874266\n",
      "Iteration 457, loss = 0.75731150\n",
      "Iteration 458, loss = 0.75580156\n",
      "Iteration 459, loss = 0.75436173\n",
      "Iteration 460, loss = 0.75293248\n",
      "Iteration 461, loss = 0.75145902\n",
      "Iteration 462, loss = 0.75004689\n",
      "Iteration 463, loss = 0.74862727\n",
      "Iteration 464, loss = 0.74718937\n",
      "Iteration 465, loss = 0.74579811\n",
      "Iteration 466, loss = 0.74439480\n",
      "Iteration 467, loss = 0.74298812\n",
      "Iteration 468, loss = 0.74161520\n",
      "Iteration 469, loss = 0.74023212\n",
      "Iteration 470, loss = 0.73885235\n",
      "Iteration 471, loss = 0.73749736\n",
      "Iteration 472, loss = 0.73613606\n",
      "Iteration 473, loss = 0.73477997\n",
      "Iteration 474, loss = 0.73344309\n",
      "Iteration 475, loss = 0.73210375\n",
      "Iteration 476, loss = 0.73076931\n",
      "Iteration 477, loss = 0.72945059\n",
      "Iteration 478, loss = 0.72813278\n",
      "Iteration 479, loss = 0.72681882\n",
      "Iteration 480, loss = 0.72551798\n",
      "Iteration 481, loss = 0.72422097\n",
      "Iteration 482, loss = 0.72292689\n",
      "Iteration 483, loss = 0.72164356\n",
      "Iteration 484, loss = 0.72036638\n",
      "Iteration 485, loss = 0.71909192\n",
      "Iteration 486, loss = 0.71782577\n",
      "Iteration 487, loss = 0.71656727\n",
      "Iteration 488, loss = 0.71531214\n",
      "Iteration 489, loss = 0.71406311\n",
      "Iteration 490, loss = 0.71282207\n",
      "Iteration 491, loss = 0.71158568\n",
      "Iteration 492, loss = 0.71035393\n",
      "Iteration 493, loss = 0.70912939\n",
      "Iteration 494, loss = 0.70791075\n",
      "Iteration 495, loss = 0.70669639\n",
      "Iteration 496, loss = 0.70548786\n",
      "Iteration 497, loss = 0.70428575\n",
      "Iteration 498, loss = 0.70308844\n",
      "Iteration 499, loss = 0.70189591\n",
      "Iteration 500, loss = 0.70070930\n",
      "Iteration 501, loss = 0.69952816\n",
      "Iteration 502, loss = 0.69835156\n",
      "Iteration 503, loss = 0.69717998\n",
      "Iteration 504, loss = 0.69601390\n",
      "Iteration 505, loss = 0.69485268\n",
      "Iteration 506, loss = 0.69369595\n",
      "Iteration 507, loss = 0.69254414\n",
      "Iteration 508, loss = 0.69139734\n",
      "Iteration 509, loss = 0.69025504\n",
      "Iteration 510, loss = 0.68911710\n",
      "Iteration 511, loss = 0.68798382\n",
      "Iteration 512, loss = 0.68685511\n",
      "Iteration 513, loss = 0.68573061\n",
      "Iteration 514, loss = 0.68461028\n",
      "Iteration 515, loss = 0.68349429\n",
      "Iteration 516, loss = 0.68238252\n",
      "Iteration 517, loss = 0.68127473\n",
      "Iteration 518, loss = 0.68017091\n",
      "Iteration 519, loss = 0.67907117\n",
      "Iteration 520, loss = 0.67797544\n",
      "Iteration 521, loss = 0.67688358\n",
      "Iteration 522, loss = 0.67579564\n",
      "Iteration 523, loss = 0.67471173\n",
      "Iteration 524, loss = 0.67363188\n",
      "Iteration 525, loss = 0.67255608\n",
      "Iteration 526, loss = 0.67148445\n",
      "Iteration 527, loss = 0.67041714\n",
      "Iteration 528, loss = 0.66935426\n",
      "Iteration 529, loss = 0.66829587\n",
      "Iteration 530, loss = 0.66724208\n",
      "Iteration 531, loss = 0.66619299\n",
      "Iteration 532, loss = 0.66514863\n",
      "Iteration 533, loss = 0.66410896\n",
      "Iteration 534, loss = 0.66307390\n",
      "Iteration 535, loss = 0.66204338\n",
      "Iteration 536, loss = 0.66101724\n",
      "Iteration 537, loss = 0.65999528\n",
      "Iteration 538, loss = 0.65897731\n",
      "Iteration 539, loss = 0.65796313\n",
      "Iteration 540, loss = 0.65695259\n",
      "Iteration 541, loss = 0.65594553\n",
      "Iteration 542, loss = 0.65494184\n",
      "Iteration 543, loss = 0.65394144\n",
      "Iteration 544, loss = 0.65294431\n",
      "Iteration 545, loss = 0.65195043\n",
      "Iteration 546, loss = 0.65095980\n",
      "Iteration 547, loss = 0.64997245\n",
      "Iteration 548, loss = 0.64898840\n",
      "Iteration 549, loss = 0.64800768\n",
      "Iteration 550, loss = 0.64703029\n",
      "Iteration 551, loss = 0.64605622\n",
      "Iteration 552, loss = 0.64508546\n",
      "Iteration 553, loss = 0.64411799\n",
      "Iteration 554, loss = 0.64315375\n",
      "Iteration 555, loss = 0.64219272\n",
      "Iteration 556, loss = 0.64123482\n",
      "Iteration 557, loss = 0.64028001\n",
      "Iteration 558, loss = 0.63932824\n",
      "Iteration 559, loss = 0.63837945\n",
      "Iteration 560, loss = 0.63743360\n",
      "Iteration 561, loss = 0.63649063\n",
      "Iteration 562, loss = 0.63555053\n",
      "Iteration 563, loss = 0.63461324\n",
      "Iteration 564, loss = 0.63367874\n",
      "Iteration 565, loss = 0.63274699\n",
      "Iteration 566, loss = 0.63181798\n",
      "Iteration 567, loss = 0.63089169\n",
      "Iteration 568, loss = 0.62996808\n",
      "Iteration 569, loss = 0.62904713\n",
      "Iteration 570, loss = 0.62812882\n",
      "Iteration 571, loss = 0.62721313\n",
      "Iteration 572, loss = 0.62630003\n",
      "Iteration 573, loss = 0.62538948\n",
      "Iteration 574, loss = 0.62448147\n",
      "Iteration 575, loss = 0.62357596\n",
      "Iteration 576, loss = 0.62267293\n",
      "Iteration 577, loss = 0.62177235\n",
      "Iteration 578, loss = 0.62087418\n",
      "Iteration 579, loss = 0.61997840\n",
      "Iteration 580, loss = 0.61908498\n",
      "Iteration 581, loss = 0.61819391\n",
      "Iteration 582, loss = 0.61730515\n",
      "Iteration 583, loss = 0.61641867\n",
      "Iteration 584, loss = 0.61553447\n",
      "Iteration 585, loss = 0.61465250\n",
      "Iteration 586, loss = 0.61377276\n",
      "Iteration 587, loss = 0.61289522\n",
      "Iteration 588, loss = 0.61201986\n",
      "Iteration 589, loss = 0.61114665\n",
      "Iteration 590, loss = 0.61027559\n",
      "Iteration 591, loss = 0.60940663\n",
      "Iteration 592, loss = 0.60853978\n",
      "Iteration 593, loss = 0.60767500\n",
      "Iteration 594, loss = 0.60681227\n",
      "Iteration 595, loss = 0.60595157\n",
      "Iteration 596, loss = 0.60509290\n",
      "Iteration 597, loss = 0.60423621\n",
      "Iteration 598, loss = 0.60338150\n",
      "Iteration 599, loss = 0.60252875\n",
      "Iteration 600, loss = 0.60167794\n",
      "Iteration 601, loss = 0.60082904\n",
      "Iteration 602, loss = 0.59998204\n",
      "Iteration 603, loss = 0.59913692\n",
      "Iteration 604, loss = 0.59829366\n",
      "Iteration 605, loss = 0.59745225\n",
      "Iteration 606, loss = 0.59661266\n",
      "Iteration 607, loss = 0.59577488\n",
      "Iteration 608, loss = 0.59493888\n",
      "Iteration 609, loss = 0.59410465\n",
      "Iteration 610, loss = 0.59327217\n",
      "Iteration 611, loss = 0.59244142\n",
      "Iteration 612, loss = 0.59161239\n",
      "Iteration 613, loss = 0.59078504\n",
      "Iteration 614, loss = 0.58995936\n",
      "Iteration 615, loss = 0.58913534\n",
      "Iteration 616, loss = 0.58831294\n",
      "Iteration 617, loss = 0.58749215\n",
      "Iteration 618, loss = 0.58667295\n",
      "Iteration 619, loss = 0.58585531\n",
      "Iteration 620, loss = 0.58503921\n",
      "Iteration 621, loss = 0.58422463\n",
      "Iteration 622, loss = 0.58341153\n",
      "Iteration 623, loss = 0.58259991\n",
      "Iteration 624, loss = 0.58178971\n",
      "Iteration 625, loss = 0.58098093\n",
      "Iteration 626, loss = 0.58017352\n",
      "Iteration 627, loss = 0.57936746\n",
      "Iteration 628, loss = 0.57856271\n",
      "Iteration 629, loss = 0.57775924\n",
      "Iteration 630, loss = 0.57695700\n",
      "Iteration 631, loss = 0.57615595\n",
      "Iteration 632, loss = 0.57535606\n",
      "Iteration 633, loss = 0.57455726\n",
      "Iteration 634, loss = 0.57375952\n",
      "Iteration 635, loss = 0.57296276\n",
      "Iteration 636, loss = 0.57216693\n",
      "Iteration 637, loss = 0.57137196\n",
      "Iteration 638, loss = 0.57057777\n",
      "Iteration 639, loss = 0.56978428\n",
      "Iteration 640, loss = 0.56899138\n",
      "Iteration 641, loss = 0.56819897\n",
      "Iteration 642, loss = 0.56740694\n",
      "Iteration 643, loss = 0.56661513\n",
      "Iteration 644, loss = 0.56582340\n",
      "Iteration 645, loss = 0.56503157\n",
      "Iteration 646, loss = 0.56423944\n",
      "Iteration 647, loss = 0.56344676\n",
      "Iteration 648, loss = 0.56265328\n",
      "Iteration 649, loss = 0.56185868\n",
      "Iteration 650, loss = 0.56106260\n",
      "Iteration 651, loss = 0.56026463\n",
      "Iteration 652, loss = 0.55946431\n",
      "Iteration 653, loss = 0.55866110\n",
      "Iteration 654, loss = 0.55785443\n",
      "Iteration 655, loss = 0.55704364\n",
      "Iteration 656, loss = 0.55622809\n",
      "Iteration 657, loss = 0.55540715\n",
      "Iteration 658, loss = 0.55458027\n",
      "Iteration 659, loss = 0.55374718\n",
      "Iteration 660, loss = 0.55290794\n",
      "Iteration 661, loss = 0.55206324\n",
      "Iteration 662, loss = 0.55121447\n",
      "Iteration 663, loss = 0.55036379\n",
      "Iteration 664, loss = 0.54951400\n",
      "Iteration 665, loss = 0.54866813\n",
      "Iteration 666, loss = 0.54782882\n",
      "Iteration 667, loss = 0.54699778\n",
      "Iteration 668, loss = 0.54617549\n",
      "Iteration 669, loss = 0.54536117\n",
      "Iteration 670, loss = 0.54455319\n",
      "Iteration 671, loss = 0.54374961\n",
      "Iteration 672, loss = 0.54294858\n",
      "Iteration 673, loss = 0.54214859\n",
      "Iteration 674, loss = 0.54134856\n",
      "Iteration 675, loss = 0.54054772\n",
      "Iteration 676, loss = 0.53974550\n",
      "Iteration 677, loss = 0.53894151\n",
      "Iteration 678, loss = 0.53813546\n",
      "Iteration 679, loss = 0.53732721\n",
      "Iteration 680, loss = 0.53651683\n",
      "Iteration 681, loss = 0.53570463\n",
      "Iteration 682, loss = 0.53489116\n",
      "Iteration 683, loss = 0.53407723\n",
      "Iteration 684, loss = 0.53326369\n",
      "Iteration 685, loss = 0.53245136\n",
      "Iteration 686, loss = 0.53164075\n",
      "Iteration 687, loss = 0.53083202\n",
      "Iteration 688, loss = 0.53002489\n",
      "Iteration 689, loss = 0.52921877\n",
      "Iteration 690, loss = 0.52841291\n",
      "Iteration 691, loss = 0.52760660\n",
      "Iteration 692, loss = 0.52679926\n",
      "Iteration 693, loss = 0.52599053\n",
      "Iteration 694, loss = 0.52518024\n",
      "Iteration 695, loss = 0.52436833\n",
      "Iteration 696, loss = 0.52355480\n",
      "Iteration 697, loss = 0.52273965\n",
      "Iteration 698, loss = 0.52192291\n",
      "Iteration 699, loss = 0.52110466\n",
      "Iteration 700, loss = 0.52028512\n",
      "Iteration 701, loss = 0.51946467\n",
      "Iteration 702, loss = 0.51864381\n",
      "Iteration 703, loss = 0.51782308\n",
      "Iteration 704, loss = 0.51700294\n",
      "Iteration 705, loss = 0.51618363\n",
      "Iteration 706, loss = 0.51536517\n",
      "Iteration 707, loss = 0.51454738\n",
      "Iteration 708, loss = 0.51372997\n",
      "Iteration 709, loss = 0.51291262\n",
      "Iteration 710, loss = 0.51209504\n",
      "Iteration 711, loss = 0.51127701\n",
      "Iteration 712, loss = 0.51045830\n",
      "Iteration 713, loss = 0.50963860\n",
      "Iteration 714, loss = 0.50881749\n",
      "Iteration 715, loss = 0.50799445\n",
      "Iteration 716, loss = 0.50716877\n",
      "Iteration 717, loss = 0.50633961\n",
      "Iteration 718, loss = 0.50550585\n",
      "Iteration 719, loss = 0.50466596\n",
      "Iteration 720, loss = 0.50381782\n",
      "Iteration 721, loss = 0.50295840\n",
      "Iteration 722, loss = 0.50208346\n",
      "Iteration 723, loss = 0.50118707\n",
      "Iteration 724, loss = 0.50026083\n",
      "Iteration 725, loss = 0.49929307\n",
      "Iteration 726, loss = 0.49826840\n",
      "Iteration 727, loss = 0.49717004\n",
      "Iteration 728, loss = 0.49599035\n",
      "Iteration 729, loss = 0.49476127\n",
      "Iteration 730, loss = 0.49361241\n",
      "Iteration 731, loss = 0.49274363\n",
      "Iteration 732, loss = 0.49204439\n",
      "Iteration 733, loss = 0.49114432\n",
      "Iteration 734, loss = 0.49001407\n",
      "Iteration 735, loss = 0.48881930\n",
      "Iteration 736, loss = 0.48769314\n",
      "Iteration 737, loss = 0.48668678\n",
      "Iteration 738, loss = 0.48577746\n",
      "Iteration 739, loss = 0.48489503\n",
      "Iteration 740, loss = 0.48396816\n",
      "Iteration 741, loss = 0.48296449\n",
      "Iteration 742, loss = 0.48196196\n",
      "Iteration 743, loss = 0.48105153\n",
      "Iteration 744, loss = 0.48027044\n",
      "Iteration 745, loss = 0.47959304\n",
      "Iteration 746, loss = 0.47887282\n",
      "Iteration 747, loss = 0.47757066\n",
      "Iteration 748, loss = 0.47629374\n",
      "Iteration 749, loss = 0.47558669\n",
      "Iteration 750, loss = 0.47473196\n",
      "Iteration 751, loss = 0.47356010\n",
      "Iteration 752, loss = 0.47272732\n",
      "Iteration 753, loss = 0.47191240\n",
      "Iteration 754, loss = 0.47080768\n",
      "Iteration 755, loss = 0.46991654\n",
      "Iteration 756, loss = 0.46912865\n",
      "Iteration 757, loss = 0.46811374\n",
      "Iteration 758, loss = 0.46719874\n",
      "Iteration 759, loss = 0.46639170\n",
      "Iteration 760, loss = 0.46541153\n",
      "Iteration 761, loss = 0.46448420\n",
      "Iteration 762, loss = 0.46366338\n",
      "Iteration 763, loss = 0.46272147\n",
      "Iteration 764, loss = 0.46180827\n",
      "Iteration 765, loss = 0.46097544\n",
      "Iteration 766, loss = 0.46005311\n",
      "Iteration 767, loss = 0.45914005\n",
      "Iteration 768, loss = 0.45830309\n",
      "Iteration 769, loss = 0.45740958\n",
      "Iteration 770, loss = 0.45650191\n",
      "Iteration 771, loss = 0.45565551\n",
      "Iteration 772, loss = 0.45477946\n",
      "Iteration 773, loss = 0.45387680\n",
      "Iteration 774, loss = 0.45302265\n",
      "Iteration 775, loss = 0.45216363\n",
      "Iteration 776, loss = 0.45127308\n",
      "Iteration 777, loss = 0.45041101\n",
      "Iteration 778, loss = 0.44955908\n",
      "Iteration 779, loss = 0.44868140\n",
      "Iteration 780, loss = 0.44781649\n",
      "Iteration 781, loss = 0.44697080\n",
      "Iteration 782, loss = 0.44610866\n",
      "Iteration 783, loss = 0.44524372\n",
      "Iteration 784, loss = 0.44439708\n",
      "Iteration 785, loss = 0.44354791\n",
      "Iteration 786, loss = 0.44269030\n",
      "Iteration 787, loss = 0.44184327\n",
      "Iteration 788, loss = 0.44100250\n",
      "Iteration 789, loss = 0.44015414\n",
      "Iteration 790, loss = 0.43930728\n",
      "Iteration 791, loss = 0.43847008\n",
      "Iteration 792, loss = 0.43763254\n",
      "Iteration 793, loss = 0.43679127\n",
      "Iteration 794, loss = 0.43595502\n",
      "Iteration 795, loss = 0.43512375\n",
      "Iteration 796, loss = 0.43429065\n",
      "Iteration 797, loss = 0.43345779\n",
      "Iteration 798, loss = 0.43263021\n",
      "Iteration 799, loss = 0.43180510\n",
      "Iteration 800, loss = 0.43097868\n",
      "Iteration 801, loss = 0.43015363\n",
      "Iteration 802, loss = 0.42933280\n",
      "Iteration 803, loss = 0.42851395\n",
      "Iteration 804, loss = 0.42769492\n",
      "Iteration 805, loss = 0.42687743\n",
      "Iteration 806, loss = 0.42606305\n",
      "Iteration 807, loss = 0.42525039\n",
      "Iteration 808, loss = 0.42443831\n",
      "Iteration 809, loss = 0.42362793\n",
      "Iteration 810, loss = 0.42282018\n",
      "Iteration 811, loss = 0.42201416\n",
      "Iteration 812, loss = 0.42120900\n",
      "Iteration 813, loss = 0.42040534\n",
      "Iteration 814, loss = 0.41960402\n",
      "Iteration 815, loss = 0.41880464\n",
      "Iteration 816, loss = 0.41800644\n",
      "Iteration 817, loss = 0.41720955\n",
      "Iteration 818, loss = 0.41641460\n",
      "Iteration 819, loss = 0.41562162\n",
      "Iteration 820, loss = 0.41483012\n",
      "Iteration 821, loss = 0.41403999\n",
      "Iteration 822, loss = 0.41325155\n",
      "Iteration 823, loss = 0.41246496\n",
      "Iteration 824, loss = 0.41167999\n",
      "Iteration 825, loss = 0.41089646\n",
      "Iteration 826, loss = 0.41011453\n",
      "Iteration 827, loss = 0.40933436\n",
      "Iteration 828, loss = 0.40855586\n",
      "Iteration 829, loss = 0.40777886\n",
      "Iteration 830, loss = 0.40700340\n",
      "Iteration 831, loss = 0.40622961\n",
      "Iteration 832, loss = 0.40545751\n",
      "Iteration 833, loss = 0.40468697\n",
      "Iteration 834, loss = 0.40391796\n",
      "Iteration 835, loss = 0.40315054\n",
      "Iteration 836, loss = 0.40238478\n",
      "Iteration 837, loss = 0.40162061\n",
      "Iteration 838, loss = 0.40085799\n",
      "Iteration 839, loss = 0.40009692\n",
      "Iteration 840, loss = 0.39933746\n",
      "Iteration 841, loss = 0.39857959\n",
      "Iteration 842, loss = 0.39782328\n",
      "Iteration 843, loss = 0.39706850\n",
      "Iteration 844, loss = 0.39631529\n",
      "Iteration 845, loss = 0.39556366\n",
      "Iteration 846, loss = 0.39481357\n",
      "Iteration 847, loss = 0.39406500\n",
      "Iteration 848, loss = 0.39331798\n",
      "Iteration 849, loss = 0.39257250\n",
      "Iteration 850, loss = 0.39182855\n",
      "Iteration 851, loss = 0.39108611\n",
      "Iteration 852, loss = 0.39034518\n",
      "Iteration 853, loss = 0.38960577\n",
      "Iteration 854, loss = 0.38886787\n",
      "Iteration 855, loss = 0.38813144\n",
      "Iteration 856, loss = 0.38739650\n",
      "Iteration 857, loss = 0.38666305\n",
      "Iteration 858, loss = 0.38593107\n",
      "Iteration 859, loss = 0.38520054\n",
      "Iteration 860, loss = 0.38447147\n",
      "Iteration 861, loss = 0.38374384\n",
      "Iteration 862, loss = 0.38301765\n",
      "Iteration 863, loss = 0.38229288\n",
      "Iteration 864, loss = 0.38156953\n",
      "Iteration 865, loss = 0.38084758\n",
      "Iteration 866, loss = 0.38012702\n",
      "Iteration 867, loss = 0.37940783\n",
      "Iteration 868, loss = 0.37869002\n",
      "Iteration 869, loss = 0.37797356\n",
      "Iteration 870, loss = 0.37725845\n",
      "Iteration 871, loss = 0.37654465\n",
      "Iteration 872, loss = 0.37583218\n",
      "Iteration 873, loss = 0.37512099\n",
      "Iteration 874, loss = 0.37441109\n",
      "Iteration 875, loss = 0.37370245\n",
      "Iteration 876, loss = 0.37299505\n",
      "Iteration 877, loss = 0.37228888\n",
      "Iteration 878, loss = 0.37158391\n",
      "Iteration 879, loss = 0.37088012\n",
      "Iteration 880, loss = 0.37017748\n",
      "Iteration 881, loss = 0.36947597\n",
      "Iteration 882, loss = 0.36877557\n",
      "Iteration 883, loss = 0.36807623\n",
      "Iteration 884, loss = 0.36737793\n",
      "Iteration 885, loss = 0.36668064\n",
      "Iteration 886, loss = 0.36598430\n",
      "Iteration 887, loss = 0.36528888\n",
      "Iteration 888, loss = 0.36459433\n",
      "Iteration 889, loss = 0.36390059\n",
      "Iteration 890, loss = 0.36320760\n",
      "Iteration 891, loss = 0.36251530\n",
      "Iteration 892, loss = 0.36182361\n",
      "Iteration 893, loss = 0.36113244\n",
      "Iteration 894, loss = 0.36044169\n",
      "Iteration 895, loss = 0.35975126\n",
      "Iteration 896, loss = 0.35906101\n",
      "Iteration 897, loss = 0.35837080\n",
      "Iteration 898, loss = 0.35768045\n",
      "Iteration 899, loss = 0.35698978\n",
      "Iteration 900, loss = 0.35629854\n",
      "Iteration 901, loss = 0.35560649\n",
      "Iteration 902, loss = 0.35491333\n",
      "Iteration 903, loss = 0.35421869\n",
      "Iteration 904, loss = 0.35352220\n",
      "Iteration 905, loss = 0.35282341\n",
      "Iteration 906, loss = 0.35212186\n",
      "Iteration 907, loss = 0.35141706\n",
      "Iteration 908, loss = 0.35070858\n",
      "Iteration 909, loss = 0.34999607\n",
      "Iteration 910, loss = 0.34927945\n",
      "Iteration 911, loss = 0.34855900\n",
      "Iteration 912, loss = 0.34783558\n",
      "Iteration 913, loss = 0.34711064\n",
      "Iteration 914, loss = 0.34638616\n",
      "Iteration 915, loss = 0.34566415\n",
      "Iteration 916, loss = 0.34494610\n",
      "Iteration 917, loss = 0.34423256\n",
      "Iteration 918, loss = 0.34352344\n",
      "Iteration 919, loss = 0.34281866\n",
      "Iteration 920, loss = 0.34211860\n",
      "Iteration 921, loss = 0.34142399\n",
      "Iteration 922, loss = 0.34073540\n",
      "Iteration 923, loss = 0.34005297\n",
      "Iteration 924, loss = 0.33937644\n",
      "Iteration 925, loss = 0.33870527\n",
      "Iteration 926, loss = 0.33803861\n",
      "Iteration 927, loss = 0.33737530\n",
      "Iteration 928, loss = 0.33671405\n",
      "Iteration 929, loss = 0.33605376\n",
      "Iteration 930, loss = 0.33539376\n",
      "Iteration 931, loss = 0.33473385\n",
      "Iteration 932, loss = 0.33407420\n",
      "Iteration 933, loss = 0.33341509\n",
      "Iteration 934, loss = 0.33275678\n",
      "Iteration 935, loss = 0.33209944\n",
      "Iteration 936, loss = 0.33144325\n",
      "Iteration 937, loss = 0.33078836\n",
      "Iteration 938, loss = 0.33013500\n",
      "Iteration 939, loss = 0.32948335\n",
      "Iteration 940, loss = 0.32883359\n",
      "Iteration 941, loss = 0.32818586\n",
      "Iteration 942, loss = 0.32754020\n",
      "Iteration 943, loss = 0.32689658\n",
      "Iteration 944, loss = 0.32625486\n",
      "Iteration 945, loss = 0.32561487\n",
      "Iteration 946, loss = 0.32497643\n",
      "Iteration 947, loss = 0.32433943\n",
      "Iteration 948, loss = 0.32370388\n",
      "Iteration 949, loss = 0.32306977\n",
      "Iteration 950, loss = 0.32243711\n",
      "Iteration 951, loss = 0.32180585\n",
      "Iteration 952, loss = 0.32117589\n",
      "Iteration 953, loss = 0.32054717\n",
      "Iteration 954, loss = 0.31991967\n",
      "Iteration 955, loss = 0.31929342\n",
      "Iteration 956, loss = 0.31866847\n",
      "Iteration 957, loss = 0.31804488\n",
      "Iteration 958, loss = 0.31742272\n",
      "Iteration 959, loss = 0.31680202\n",
      "Iteration 960, loss = 0.31618280\n",
      "Iteration 961, loss = 0.31556503\n",
      "Iteration 962, loss = 0.31494869\n",
      "Iteration 963, loss = 0.31433374\n",
      "Iteration 964, loss = 0.31372015\n",
      "Iteration 965, loss = 0.31310789\n",
      "Iteration 966, loss = 0.31249694\n",
      "Iteration 967, loss = 0.31188726\n",
      "Iteration 968, loss = 0.31127879\n",
      "Iteration 969, loss = 0.31067151\n",
      "Iteration 970, loss = 0.31006536\n",
      "Iteration 971, loss = 0.30946032\n",
      "Iteration 972, loss = 0.30885636\n",
      "Iteration 973, loss = 0.30825341\n",
      "Iteration 974, loss = 0.30765142\n",
      "Iteration 975, loss = 0.30705029\n",
      "Iteration 976, loss = 0.30644987\n",
      "Iteration 977, loss = 0.30585001\n",
      "Iteration 978, loss = 0.30525045\n",
      "Iteration 979, loss = 0.30465086\n",
      "Iteration 980, loss = 0.30405080\n",
      "Iteration 981, loss = 0.30344963\n",
      "Iteration 982, loss = 0.30284644\n",
      "Iteration 983, loss = 0.30223993\n",
      "Iteration 984, loss = 0.30162836\n",
      "Iteration 985, loss = 0.30100975\n",
      "Iteration 986, loss = 0.30038332\n",
      "Iteration 987, loss = 0.29975361\n",
      "Iteration 988, loss = 0.29913264\n",
      "Iteration 989, loss = 0.29851940\n",
      "Iteration 990, loss = 0.29789976\n",
      "Iteration 991, loss = 0.29729388\n",
      "Iteration 992, loss = 0.29670504\n",
      "Iteration 993, loss = 0.29611855\n",
      "Iteration 994, loss = 0.29554208\n",
      "Iteration 995, loss = 0.29497198\n",
      "Iteration 996, loss = 0.29439446\n",
      "Iteration 997, loss = 0.29382270\n",
      "Iteration 998, loss = 0.29325297\n",
      "Iteration 999, loss = 0.29268161\n",
      "Iteration 1000, loss = 0.29211549\n",
      "Iteration 1001, loss = 0.29154590\n",
      "Iteration 1002, loss = 0.29097839\n",
      "Iteration 1003, loss = 0.29041717\n",
      "Iteration 1004, loss = 0.28985505\n",
      "Iteration 1005, loss = 0.28929639\n",
      "Iteration 1006, loss = 0.28873941\n",
      "Iteration 1007, loss = 0.28818322\n",
      "Iteration 1008, loss = 0.28763158\n",
      "Iteration 1009, loss = 0.28707925\n",
      "Iteration 1010, loss = 0.28652840\n",
      "Iteration 1011, loss = 0.28597969\n",
      "Iteration 1012, loss = 0.28543075\n",
      "Iteration 1013, loss = 0.28488467\n",
      "Iteration 1014, loss = 0.28433966\n",
      "Iteration 1015, loss = 0.28379568\n",
      "Iteration 1016, loss = 0.28325453\n",
      "Iteration 1017, loss = 0.28271479\n",
      "Iteration 1018, loss = 0.28217736\n",
      "Iteration 1019, loss = 0.28164136\n",
      "Iteration 1020, loss = 0.28110626\n",
      "Iteration 1021, loss = 0.28057345\n",
      "Iteration 1022, loss = 0.28004160\n",
      "Iteration 1023, loss = 0.27951133\n",
      "Iteration 1024, loss = 0.27898273\n",
      "Iteration 1025, loss = 0.27845539\n",
      "Iteration 1026, loss = 0.27793004\n",
      "Iteration 1027, loss = 0.27740607\n",
      "Iteration 1028, loss = 0.27688366\n",
      "Iteration 1029, loss = 0.27636281\n",
      "Iteration 1030, loss = 0.27584317\n",
      "Iteration 1031, loss = 0.27532506\n",
      "Iteration 1032, loss = 0.27480827\n",
      "Iteration 1033, loss = 0.27429258\n",
      "Iteration 1034, loss = 0.27377828\n",
      "Iteration 1035, loss = 0.27326515\n",
      "Iteration 1036, loss = 0.27275329\n",
      "Iteration 1037, loss = 0.27224270\n",
      "Iteration 1038, loss = 0.27173320\n",
      "Iteration 1039, loss = 0.27122496\n",
      "Iteration 1040, loss = 0.27071781\n",
      "Iteration 1041, loss = 0.27021170\n",
      "Iteration 1042, loss = 0.26970665\n",
      "Iteration 1043, loss = 0.26920249\n",
      "Iteration 1044, loss = 0.26869920\n",
      "Iteration 1045, loss = 0.26819671\n",
      "Iteration 1046, loss = 0.26769483\n",
      "Iteration 1047, loss = 0.26719352\n",
      "Iteration 1048, loss = 0.26669261\n",
      "Iteration 1049, loss = 0.26619198\n",
      "Iteration 1050, loss = 0.26569147\n",
      "Iteration 1051, loss = 0.26519086\n",
      "Iteration 1052, loss = 0.26468999\n",
      "Iteration 1053, loss = 0.26418864\n",
      "Iteration 1054, loss = 0.26368656\n",
      "Iteration 1055, loss = 0.26318354\n",
      "Iteration 1056, loss = 0.26267936\n",
      "Iteration 1057, loss = 0.26217383\n",
      "Iteration 1058, loss = 0.26166687\n",
      "Iteration 1059, loss = 0.26115849\n",
      "Iteration 1060, loss = 0.26064888\n",
      "Iteration 1061, loss = 0.26013849\n",
      "Iteration 1062, loss = 0.25962799\n",
      "Iteration 1063, loss = 0.25911834\n",
      "Iteration 1064, loss = 0.25861069\n",
      "Iteration 1065, loss = 0.25810623\n",
      "Iteration 1066, loss = 0.25760605\n",
      "Iteration 1067, loss = 0.25711096\n",
      "Iteration 1068, loss = 0.25662134\n",
      "Iteration 1069, loss = 0.25613713\n",
      "Iteration 1070, loss = 0.25565787\n",
      "Iteration 1071, loss = 0.25518291\n",
      "Iteration 1072, loss = 0.25471155\n",
      "Iteration 1073, loss = 0.25424320\n",
      "Iteration 1074, loss = 0.25377744\n",
      "Iteration 1075, loss = 0.25331401\n",
      "Iteration 1076, loss = 0.25285272\n",
      "Iteration 1077, loss = 0.25239341\n",
      "Iteration 1078, loss = 0.25193594\n",
      "Iteration 1079, loss = 0.25148015\n",
      "Iteration 1080, loss = 0.25102595\n",
      "Iteration 1081, loss = 0.25057329\n",
      "Iteration 1082, loss = 0.25012218\n",
      "Iteration 1083, loss = 0.24967269\n",
      "Iteration 1084, loss = 0.24922494\n",
      "Iteration 1085, loss = 0.24877906\n",
      "Iteration 1086, loss = 0.24833515\n",
      "Iteration 1087, loss = 0.24789329\n",
      "Iteration 1088, loss = 0.24745348\n",
      "Iteration 1089, loss = 0.24701568\n",
      "Iteration 1090, loss = 0.24657979\n",
      "Iteration 1091, loss = 0.24614566\n",
      "Iteration 1092, loss = 0.24571319\n",
      "Iteration 1093, loss = 0.24528227\n",
      "Iteration 1094, loss = 0.24485283\n",
      "Iteration 1095, loss = 0.24442487\n",
      "Iteration 1096, loss = 0.24399839\n",
      "Iteration 1097, loss = 0.24357351\n",
      "Iteration 1098, loss = 0.24315048\n",
      "Iteration 1099, loss = 0.24273020\n",
      "Iteration 1100, loss = 0.24231495\n",
      "Iteration 1101, loss = 0.24191431\n",
      "Iteration 1102, loss = 0.24153721\n",
      "Iteration 1103, loss = 0.24125142\n",
      "Iteration 1104, loss = 0.24081924\n",
      "Iteration 1105, loss = 0.24042308\n",
      "Iteration 1106, loss = 0.23983805\n",
      "Iteration 1107, loss = 0.23952243\n",
      "Iteration 1108, loss = 0.23936131\n",
      "Iteration 1109, loss = 0.23865936\n",
      "Iteration 1110, loss = 0.23827388\n",
      "Iteration 1111, loss = 0.23807808\n",
      "Iteration 1112, loss = 0.23741322\n",
      "Iteration 1113, loss = 0.23712551\n",
      "Iteration 1114, loss = 0.23692950\n",
      "Iteration 1115, loss = 0.23620892\n",
      "Iteration 1116, loss = 0.23613706\n",
      "Iteration 1117, loss = 0.23594936\n",
      "Iteration 1118, loss = 0.23515247\n",
      "Iteration 1119, loss = 0.23523764\n",
      "Iteration 1120, loss = 0.23450087\n",
      "Iteration 1121, loss = 0.23419332\n",
      "Iteration 1122, loss = 0.23371770\n",
      "Iteration 1123, loss = 0.23317719\n",
      "Iteration 1124, loss = 0.23299169\n",
      "Iteration 1125, loss = 0.23235283\n",
      "Iteration 1126, loss = 0.23228476\n",
      "Iteration 1127, loss = 0.23174025\n",
      "Iteration 1128, loss = 0.23141332\n",
      "Iteration 1129, loss = 0.23093782\n",
      "Iteration 1130, loss = 0.23056638\n",
      "Iteration 1131, loss = 0.23029144\n",
      "Iteration 1132, loss = 0.22973353\n",
      "Iteration 1133, loss = 0.22956230\n",
      "Iteration 1134, loss = 0.22895744\n",
      "Iteration 1135, loss = 0.22874708\n",
      "Iteration 1136, loss = 0.22821898\n",
      "Iteration 1137, loss = 0.22794536\n",
      "Iteration 1138, loss = 0.22756206\n",
      "Iteration 1139, loss = 0.22715745\n",
      "Iteration 1140, loss = 0.22684444\n",
      "Iteration 1141, loss = 0.22639292\n",
      "Iteration 1142, loss = 0.22609511\n",
      "Iteration 1143, loss = 0.22567397\n",
      "Iteration 1144, loss = 0.22537384\n",
      "Iteration 1145, loss = 0.22497990\n",
      "Iteration 1146, loss = 0.22462672\n",
      "Iteration 1147, loss = 0.22428180\n",
      "Iteration 1148, loss = 0.22389428\n",
      "Iteration 1149, loss = 0.22357586\n",
      "Iteration 1150, loss = 0.22318790\n",
      "Iteration 1151, loss = 0.22286826\n",
      "Iteration 1152, loss = 0.22250186\n",
      "Iteration 1153, loss = 0.22216090\n",
      "Iteration 1154, loss = 0.22180963\n",
      "Iteration 1155, loss = 0.22145528\n",
      "Iteration 1156, loss = 0.22112979\n",
      "Iteration 1157, loss = 0.22076310\n",
      "Iteration 1158, loss = 0.22044102\n",
      "Iteration 1159, loss = 0.22009016\n",
      "Iteration 1160, loss = 0.21975278\n",
      "Iteration 1161, loss = 0.21941810\n",
      "Iteration 1162, loss = 0.21907178\n",
      "Iteration 1163, loss = 0.21874502\n",
      "Iteration 1164, loss = 0.21840295\n",
      "Iteration 1165, loss = 0.21807497\n",
      "Iteration 1166, loss = 0.21774116\n",
      "Iteration 1167, loss = 0.21740735\n",
      "Iteration 1168, loss = 0.21708452\n",
      "Iteration 1169, loss = 0.21674886\n",
      "Iteration 1170, loss = 0.21642458\n",
      "Iteration 1171, loss = 0.21609869\n",
      "Iteration 1172, loss = 0.21577053\n",
      "Iteration 1173, loss = 0.21545012\n",
      "Iteration 1174, loss = 0.21512439\n",
      "Iteration 1175, loss = 0.21480381\n",
      "Iteration 1176, loss = 0.21448423\n",
      "Iteration 1177, loss = 0.21416278\n",
      "Iteration 1178, loss = 0.21384649\n",
      "Iteration 1179, loss = 0.21352799\n",
      "Iteration 1180, loss = 0.21321205\n",
      "Iteration 1181, loss = 0.21289815\n",
      "Iteration 1182, loss = 0.21258266\n",
      "Iteration 1183, loss = 0.21227107\n",
      "Iteration 1184, loss = 0.21195931\n",
      "Iteration 1185, loss = 0.21164795\n",
      "Iteration 1186, loss = 0.21133967\n",
      "Iteration 1187, loss = 0.21103051\n",
      "Iteration 1188, loss = 0.21072341\n",
      "Iteration 1189, loss = 0.21041778\n",
      "Iteration 1190, loss = 0.21011195\n",
      "Iteration 1191, loss = 0.20980847\n",
      "Iteration 1192, loss = 0.20950569\n",
      "Iteration 1193, loss = 0.20920333\n",
      "Iteration 1194, loss = 0.20890301\n",
      "Iteration 1195, loss = 0.20860327\n",
      "Iteration 1196, loss = 0.20830433\n",
      "Iteration 1197, loss = 0.20800713\n",
      "Iteration 1198, loss = 0.20771045\n",
      "Iteration 1199, loss = 0.20741477\n",
      "Iteration 1200, loss = 0.20712067\n",
      "Iteration 1201, loss = 0.20682706\n",
      "Iteration 1202, loss = 0.20653461\n",
      "Iteration 1203, loss = 0.20624347\n",
      "Iteration 1204, loss = 0.20595302\n",
      "Iteration 1205, loss = 0.20566364\n",
      "Iteration 1206, loss = 0.20537549\n",
      "Iteration 1207, loss = 0.20508812\n",
      "Iteration 1208, loss = 0.20480176\n",
      "Iteration 1209, loss = 0.20451658\n",
      "Iteration 1210, loss = 0.20423223\n",
      "Iteration 1211, loss = 0.20394885\n",
      "Iteration 1212, loss = 0.20366659\n",
      "Iteration 1213, loss = 0.20338522\n",
      "Iteration 1214, loss = 0.20310477\n",
      "Iteration 1215, loss = 0.20282539\n",
      "Iteration 1216, loss = 0.20254695\n",
      "Iteration 1217, loss = 0.20226939\n",
      "Iteration 1218, loss = 0.20199285\n",
      "Iteration 1219, loss = 0.20171729\n",
      "Iteration 1220, loss = 0.20144259\n",
      "Iteration 1221, loss = 0.20116885\n",
      "Iteration 1222, loss = 0.20089610\n",
      "Iteration 1223, loss = 0.20062422\n",
      "Iteration 1224, loss = 0.20035326\n",
      "Iteration 1225, loss = 0.20008327\n",
      "Iteration 1226, loss = 0.19981418\n",
      "Iteration 1227, loss = 0.19954597\n",
      "Iteration 1228, loss = 0.19927868\n",
      "Iteration 1229, loss = 0.19901232\n",
      "Iteration 1230, loss = 0.19874683\n",
      "Iteration 1231, loss = 0.19848223\n",
      "Iteration 1232, loss = 0.19821854\n",
      "Iteration 1233, loss = 0.19795573\n",
      "Iteration 1234, loss = 0.19769379\n",
      "Iteration 1235, loss = 0.19743273\n",
      "Iteration 1236, loss = 0.19717255\n",
      "Iteration 1237, loss = 0.19691323\n",
      "Iteration 1238, loss = 0.19665477\n",
      "Iteration 1239, loss = 0.19639718\n",
      "Iteration 1240, loss = 0.19614045\n",
      "Iteration 1241, loss = 0.19588456\n",
      "Iteration 1242, loss = 0.19562951\n",
      "Iteration 1243, loss = 0.19537531\n",
      "Iteration 1244, loss = 0.19512196\n",
      "Iteration 1245, loss = 0.19486943\n",
      "Iteration 1246, loss = 0.19461773\n",
      "Iteration 1247, loss = 0.19436686\n",
      "Iteration 1248, loss = 0.19411682\n",
      "Iteration 1249, loss = 0.19386759\n",
      "Iteration 1250, loss = 0.19361917\n",
      "Iteration 1251, loss = 0.19337157\n",
      "Iteration 1252, loss = 0.19312477\n",
      "Iteration 1253, loss = 0.19287878\n",
      "Iteration 1254, loss = 0.19263358\n",
      "Iteration 1255, loss = 0.19238917\n",
      "Iteration 1256, loss = 0.19214556\n",
      "Iteration 1257, loss = 0.19190274\n",
      "Iteration 1258, loss = 0.19166069\n",
      "Iteration 1259, loss = 0.19141943\n",
      "Iteration 1260, loss = 0.19117894\n",
      "Iteration 1261, loss = 0.19093922\n",
      "Iteration 1262, loss = 0.19070027\n",
      "Iteration 1263, loss = 0.19046209\n",
      "Iteration 1264, loss = 0.19022466\n",
      "Iteration 1265, loss = 0.18998799\n",
      "Iteration 1266, loss = 0.18975208\n",
      "Iteration 1267, loss = 0.18951692\n",
      "Iteration 1268, loss = 0.18928250\n",
      "Iteration 1269, loss = 0.18904883\n",
      "Iteration 1270, loss = 0.18881590\n",
      "Iteration 1271, loss = 0.18858370\n",
      "Iteration 1272, loss = 0.18835224\n",
      "Iteration 1273, loss = 0.18812151\n",
      "Iteration 1274, loss = 0.18789151\n",
      "Iteration 1275, loss = 0.18766224\n",
      "Iteration 1276, loss = 0.18743369\n",
      "Iteration 1277, loss = 0.18720585\n",
      "Iteration 1278, loss = 0.18697874\n",
      "Iteration 1279, loss = 0.18675234\n",
      "Iteration 1280, loss = 0.18652665\n",
      "Iteration 1281, loss = 0.18630167\n",
      "Iteration 1282, loss = 0.18607740\n",
      "Iteration 1283, loss = 0.18585383\n",
      "Iteration 1284, loss = 0.18563096\n",
      "Iteration 1285, loss = 0.18540880\n",
      "Iteration 1286, loss = 0.18518734\n",
      "Iteration 1287, loss = 0.18496657\n",
      "Iteration 1288, loss = 0.18474650\n",
      "Iteration 1289, loss = 0.18452712\n",
      "Iteration 1290, loss = 0.18430844\n",
      "Iteration 1291, loss = 0.18409044\n",
      "Iteration 1292, loss = 0.18387313\n",
      "Iteration 1293, loss = 0.18365651\n",
      "Iteration 1294, loss = 0.18344058\n",
      "Iteration 1295, loss = 0.18322533\n",
      "Iteration 1296, loss = 0.18301076\n",
      "Iteration 1297, loss = 0.18279687\n",
      "Iteration 1298, loss = 0.18258367\n",
      "Iteration 1299, loss = 0.18237114\n",
      "Iteration 1300, loss = 0.18215929\n",
      "Iteration 1301, loss = 0.18194811\n",
      "Iteration 1302, loss = 0.18173761\n",
      "Iteration 1303, loss = 0.18152778\n",
      "Iteration 1304, loss = 0.18131862\n",
      "Iteration 1305, loss = 0.18111013\n",
      "Iteration 1306, loss = 0.18090231\n",
      "Iteration 1307, loss = 0.18069515\n",
      "Iteration 1308, loss = 0.18048866\n",
      "Iteration 1309, loss = 0.18028284\n",
      "Iteration 1310, loss = 0.18007767\n",
      "Iteration 1311, loss = 0.17987316\n",
      "Iteration 1312, loss = 0.17966931\n",
      "Iteration 1313, loss = 0.17946611\n",
      "Iteration 1314, loss = 0.17926357\n",
      "Iteration 1315, loss = 0.17906168\n",
      "Iteration 1316, loss = 0.17886043\n",
      "Iteration 1317, loss = 0.17865983\n",
      "Iteration 1318, loss = 0.17845988\n",
      "Iteration 1319, loss = 0.17826056\n",
      "Iteration 1320, loss = 0.17806189\n",
      "Iteration 1321, loss = 0.17786385\n",
      "Iteration 1322, loss = 0.17766644\n",
      "Iteration 1323, loss = 0.17746967\n",
      "Iteration 1324, loss = 0.17727352\n",
      "Iteration 1325, loss = 0.17707800\n",
      "Iteration 1326, loss = 0.17688310\n",
      "Iteration 1327, loss = 0.17668882\n",
      "Iteration 1328, loss = 0.17649515\n",
      "Iteration 1329, loss = 0.17630211\n",
      "Iteration 1330, loss = 0.17610967\n",
      "Iteration 1331, loss = 0.17591784\n",
      "Iteration 1332, loss = 0.17572662\n",
      "Iteration 1333, loss = 0.17553600\n",
      "Iteration 1334, loss = 0.17534599\n",
      "Iteration 1335, loss = 0.17515658\n",
      "Iteration 1336, loss = 0.17496778\n",
      "Iteration 1337, loss = 0.17477961\n",
      "Iteration 1338, loss = 0.17459212\n",
      "Iteration 1339, loss = 0.17440545\n",
      "Iteration 1340, loss = 0.17421987\n",
      "Iteration 1341, loss = 0.17403631\n",
      "Iteration 1342, loss = 0.17385613\n",
      "Iteration 1343, loss = 0.17368608\n",
      "Iteration 1344, loss = 0.17352876\n",
      "Iteration 1345, loss = 0.17343211\n",
      "Iteration 1346, loss = 0.17331578\n",
      "Iteration 1347, loss = 0.17335180\n",
      "Iteration 1348, loss = 0.17297147\n",
      "Iteration 1349, loss = 0.17260463\n",
      "Iteration 1350, loss = 0.17243384\n",
      "Iteration 1351, loss = 0.17242742\n",
      "Iteration 1352, loss = 0.17271525\n",
      "Iteration 1353, loss = 0.17217991\n",
      "Iteration 1354, loss = 0.17184154\n",
      "Iteration 1355, loss = 0.17226828\n",
      "Iteration 1356, loss = 0.17180017\n",
      "Iteration 1357, loss = 0.17144205\n",
      "Iteration 1358, loss = 0.17123998\n",
      "Iteration 1359, loss = 0.17107616\n",
      "Iteration 1360, loss = 0.17084333\n",
      "Iteration 1361, loss = 0.17046033\n",
      "Iteration 1362, loss = 0.17043994\n",
      "Iteration 1363, loss = 0.17026590\n",
      "Iteration 1364, loss = 0.16992922\n",
      "Iteration 1365, loss = 0.16992338\n",
      "Iteration 1366, loss = 0.16979852\n",
      "Iteration 1367, loss = 0.16946999\n",
      "Iteration 1368, loss = 0.16938760\n",
      "Iteration 1369, loss = 0.16924441\n",
      "Iteration 1370, loss = 0.16894506\n",
      "Iteration 1371, loss = 0.16880044\n",
      "Iteration 1372, loss = 0.16870035\n",
      "Iteration 1373, loss = 0.16845485\n",
      "Iteration 1374, loss = 0.16831376\n",
      "Iteration 1375, loss = 0.16820487\n",
      "Iteration 1376, loss = 0.16792581\n",
      "Iteration 1377, loss = 0.16776140\n",
      "Iteration 1378, loss = 0.16765139\n",
      "Iteration 1379, loss = 0.16742224\n",
      "Iteration 1380, loss = 0.16729312\n",
      "Iteration 1381, loss = 0.16716663\n",
      "Iteration 1382, loss = 0.16691606\n",
      "Iteration 1383, loss = 0.16677702\n",
      "Iteration 1384, loss = 0.16663947\n",
      "Iteration 1385, loss = 0.16642063\n",
      "Iteration 1386, loss = 0.16630018\n",
      "Iteration 1387, loss = 0.16616444\n",
      "Iteration 1388, loss = 0.16593596\n",
      "Iteration 1389, loss = 0.16580855\n",
      "Iteration 1390, loss = 0.16567415\n",
      "Iteration 1391, loss = 0.16544802\n",
      "Iteration 1392, loss = 0.16532472\n",
      "Iteration 1393, loss = 0.16518951\n",
      "Iteration 1394, loss = 0.16496871\n",
      "Iteration 1395, loss = 0.16485363\n",
      "Iteration 1396, loss = 0.16471871\n",
      "Iteration 1397, loss = 0.16449397\n",
      "Iteration 1398, loss = 0.16437751\n",
      "Iteration 1399, loss = 0.16423660\n",
      "Iteration 1400, loss = 0.16402451\n",
      "Iteration 1401, loss = 0.16391400\n",
      "Iteration 1402, loss = 0.16377593\n",
      "Iteration 1403, loss = 0.16356266\n",
      "Iteration 1404, loss = 0.16344677\n",
      "Iteration 1405, loss = 0.16330678\n",
      "Iteration 1406, loss = 0.16310223\n",
      "Iteration 1407, loss = 0.16298647\n",
      "Iteration 1408, loss = 0.16285056\n",
      "Iteration 1409, loss = 0.16264873\n",
      "Iteration 1410, loss = 0.16253055\n",
      "Iteration 1411, loss = 0.16239495\n",
      "Iteration 1412, loss = 0.16219708\n",
      "Iteration 1413, loss = 0.16207648\n",
      "Iteration 1414, loss = 0.16194234\n",
      "Iteration 1415, loss = 0.16175041\n",
      "Iteration 1416, loss = 0.16162790\n",
      "Iteration 1417, loss = 0.16149540\n",
      "Iteration 1418, loss = 0.16130825\n",
      "Iteration 1419, loss = 0.16118229\n",
      "Iteration 1420, loss = 0.16105105\n",
      "Iteration 1421, loss = 0.16086960\n",
      "Iteration 1422, loss = 0.16073939\n",
      "Iteration 1423, loss = 0.16061014\n",
      "Iteration 1424, loss = 0.16043534\n",
      "Iteration 1425, loss = 0.16030216\n",
      "Iteration 1426, loss = 0.16017400\n",
      "Iteration 1427, loss = 0.16000512\n",
      "Iteration 1428, loss = 0.15986743\n",
      "Iteration 1429, loss = 0.15973944\n",
      "Iteration 1430, loss = 0.15957834\n",
      "Iteration 1431, loss = 0.15943868\n",
      "Iteration 1432, loss = 0.15931052\n",
      "Iteration 1433, loss = 0.15915618\n",
      "Iteration 1434, loss = 0.15901385\n",
      "Iteration 1435, loss = 0.15888449\n",
      "Iteration 1436, loss = 0.15873708\n",
      "Iteration 1437, loss = 0.15859406\n",
      "Iteration 1438, loss = 0.15846318\n",
      "Iteration 1439, loss = 0.15832173\n",
      "Iteration 1440, loss = 0.15817918\n",
      "Iteration 1441, loss = 0.15804648\n",
      "Iteration 1442, loss = 0.15790963\n",
      "Iteration 1443, loss = 0.15776892\n",
      "Iteration 1444, loss = 0.15763445\n",
      "Iteration 1445, loss = 0.15750066\n",
      "Iteration 1446, loss = 0.15736279\n",
      "Iteration 1447, loss = 0.15722725\n",
      "Iteration 1448, loss = 0.15709496\n",
      "Iteration 1449, loss = 0.15696013\n",
      "Iteration 1450, loss = 0.15682465\n",
      "Iteration 1451, loss = 0.15669275\n",
      "Iteration 1452, loss = 0.15656052\n",
      "Iteration 1453, loss = 0.15642625\n",
      "Iteration 1454, loss = 0.15629431\n",
      "Iteration 1455, loss = 0.15616376\n",
      "Iteration 1456, loss = 0.15603150\n",
      "Iteration 1457, loss = 0.15589986\n",
      "Iteration 1458, loss = 0.15577012\n",
      "Iteration 1459, loss = 0.15563995\n",
      "Iteration 1460, loss = 0.15550935\n",
      "Iteration 1461, loss = 0.15538004\n",
      "Iteration 1462, loss = 0.15525144\n",
      "Iteration 1463, loss = 0.15512241\n",
      "Iteration 1464, loss = 0.15499371\n",
      "Iteration 1465, loss = 0.15486606\n",
      "Iteration 1466, loss = 0.15473864\n",
      "Iteration 1467, loss = 0.15461105\n",
      "Iteration 1468, loss = 0.15448410\n",
      "Iteration 1469, loss = 0.15435788\n",
      "Iteration 1470, loss = 0.15423171\n",
      "Iteration 1471, loss = 0.15410567\n",
      "Iteration 1472, loss = 0.15398032\n",
      "Iteration 1473, loss = 0.15385543\n",
      "Iteration 1474, loss = 0.15373062\n",
      "Iteration 1475, loss = 0.15360614\n",
      "Iteration 1476, loss = 0.15348223\n",
      "Iteration 1477, loss = 0.15335867\n",
      "Iteration 1478, loss = 0.15323529\n",
      "Iteration 1479, loss = 0.15311228\n",
      "Iteration 1480, loss = 0.15298975\n",
      "Iteration 1481, loss = 0.15286755\n",
      "Iteration 1482, loss = 0.15274558\n",
      "Iteration 1483, loss = 0.15262398\n",
      "Iteration 1484, loss = 0.15250281\n",
      "Iteration 1485, loss = 0.15238197\n",
      "Iteration 1486, loss = 0.15226138\n",
      "Iteration 1487, loss = 0.15214116\n",
      "Iteration 1488, loss = 0.15202133\n",
      "Iteration 1489, loss = 0.15190182\n",
      "Iteration 1490, loss = 0.15178260\n",
      "Iteration 1491, loss = 0.15166372\n",
      "Iteration 1492, loss = 0.15154521\n",
      "Iteration 1493, loss = 0.15142703\n",
      "Iteration 1494, loss = 0.15130914\n",
      "Iteration 1495, loss = 0.15119158\n",
      "Iteration 1496, loss = 0.15107437\n",
      "Iteration 1497, loss = 0.15095749\n",
      "Iteration 1498, loss = 0.15084091\n",
      "Iteration 1499, loss = 0.15072465\n",
      "Iteration 1500, loss = 0.15060872\n",
      "Iteration 1501, loss = 0.15049312\n",
      "Iteration 1502, loss = 0.15037782\n",
      "Iteration 1503, loss = 0.15026283\n",
      "Iteration 1504, loss = 0.15014817\n",
      "Iteration 1505, loss = 0.15003382\n",
      "Iteration 1506, loss = 0.14991979\n",
      "Iteration 1507, loss = 0.14980605\n",
      "Iteration 1508, loss = 0.14969263\n",
      "Iteration 1509, loss = 0.14957952\n",
      "Iteration 1510, loss = 0.14946671\n",
      "Iteration 1511, loss = 0.14935421\n",
      "Iteration 1512, loss = 0.14924201\n",
      "Iteration 1513, loss = 0.14913011\n",
      "Iteration 1514, loss = 0.14901852\n",
      "Iteration 1515, loss = 0.14890722\n",
      "Iteration 1516, loss = 0.14879622\n",
      "Iteration 1517, loss = 0.14868552\n",
      "Iteration 1518, loss = 0.14857511\n",
      "Iteration 1519, loss = 0.14846500\n",
      "Iteration 1520, loss = 0.14835518\n",
      "Iteration 1521, loss = 0.14824564\n",
      "Iteration 1522, loss = 0.14813640\n",
      "Iteration 1523, loss = 0.14802745\n",
      "Iteration 1524, loss = 0.14791878\n",
      "Iteration 1525, loss = 0.14781040\n",
      "Iteration 1526, loss = 0.14770230\n",
      "Iteration 1527, loss = 0.14759448\n",
      "Iteration 1528, loss = 0.14748694\n",
      "Iteration 1529, loss = 0.14737969\n",
      "Iteration 1530, loss = 0.14727271\n",
      "Iteration 1531, loss = 0.14716600\n",
      "Iteration 1532, loss = 0.14705957\n",
      "Iteration 1533, loss = 0.14695342\n",
      "Iteration 1534, loss = 0.14684753\n",
      "Iteration 1535, loss = 0.14674192\n",
      "Iteration 1536, loss = 0.14663657\n",
      "Iteration 1537, loss = 0.14653150\n",
      "Iteration 1538, loss = 0.14642668\n",
      "Iteration 1539, loss = 0.14632213\n",
      "Iteration 1540, loss = 0.14621785\n",
      "Iteration 1541, loss = 0.14611382\n",
      "Iteration 1542, loss = 0.14601005\n",
      "Iteration 1543, loss = 0.14590654\n",
      "Iteration 1544, loss = 0.14580329\n",
      "Iteration 1545, loss = 0.14570029\n",
      "Iteration 1546, loss = 0.14559754\n",
      "Iteration 1547, loss = 0.14549504\n",
      "Iteration 1548, loss = 0.14539279\n",
      "Iteration 1549, loss = 0.14529079\n",
      "Iteration 1550, loss = 0.14518904\n",
      "Iteration 1551, loss = 0.14508752\n",
      "Iteration 1552, loss = 0.14498625\n",
      "Iteration 1553, loss = 0.14488522\n",
      "Iteration 1554, loss = 0.14478443\n",
      "Iteration 1555, loss = 0.14468387\n",
      "Iteration 1556, loss = 0.14458355\n",
      "Iteration 1557, loss = 0.14448346\n",
      "Iteration 1558, loss = 0.14438360\n",
      "Iteration 1559, loss = 0.14428396\n",
      "Iteration 1560, loss = 0.14418456\n",
      "Iteration 1561, loss = 0.14408538\n",
      "Iteration 1562, loss = 0.14398642\n",
      "Iteration 1563, loss = 0.14388767\n",
      "Iteration 1564, loss = 0.14378915\n",
      "Iteration 1565, loss = 0.14369084\n",
      "Iteration 1566, loss = 0.14359275\n",
      "Iteration 1567, loss = 0.14349487\n",
      "Iteration 1568, loss = 0.14339719\n",
      "Iteration 1569, loss = 0.14329972\n",
      "Iteration 1570, loss = 0.14320246\n",
      "Iteration 1571, loss = 0.14310540\n",
      "Iteration 1572, loss = 0.14300853\n",
      "Iteration 1573, loss = 0.14291187\n",
      "Iteration 1574, loss = 0.14281539\n",
      "Iteration 1575, loss = 0.14271911\n",
      "Iteration 1576, loss = 0.14262302\n",
      "Iteration 1577, loss = 0.14252711\n",
      "Iteration 1578, loss = 0.14243139\n",
      "Iteration 1579, loss = 0.14233585\n",
      "Iteration 1580, loss = 0.14224049\n",
      "Iteration 1581, loss = 0.14214531\n",
      "Iteration 1582, loss = 0.14205030\n",
      "Iteration 1583, loss = 0.14195546\n",
      "Iteration 1584, loss = 0.14186079\n",
      "Iteration 1585, loss = 0.14176629\n",
      "Iteration 1586, loss = 0.14167195\n",
      "Iteration 1587, loss = 0.14157777\n",
      "Iteration 1588, loss = 0.14148375\n",
      "Iteration 1589, loss = 0.14138989\n",
      "Iteration 1590, loss = 0.14129618\n",
      "Iteration 1591, loss = 0.14120262\n",
      "Iteration 1592, loss = 0.14110921\n",
      "Iteration 1593, loss = 0.14101596\n",
      "Iteration 1594, loss = 0.14092285\n",
      "Iteration 1595, loss = 0.14082988\n",
      "Iteration 1596, loss = 0.14073706\n",
      "Iteration 1597, loss = 0.14064438\n",
      "Iteration 1598, loss = 0.14055184\n",
      "Iteration 1599, loss = 0.14045944\n",
      "Iteration 1600, loss = 0.14036718\n",
      "Iteration 1601, loss = 0.14027506\n",
      "Iteration 1602, loss = 0.14018308\n",
      "Iteration 1603, loss = 0.14009125\n",
      "Iteration 1604, loss = 0.13999955\n",
      "Iteration 1605, loss = 0.13990799\n",
      "Iteration 1606, loss = 0.13981658\n",
      "Iteration 1607, loss = 0.13972531\n",
      "Iteration 1608, loss = 0.13963419\n",
      "Iteration 1609, loss = 0.13954322\n",
      "Iteration 1610, loss = 0.13945240\n",
      "Iteration 1611, loss = 0.13936174\n",
      "Iteration 1612, loss = 0.13927123\n",
      "Iteration 1613, loss = 0.13918089\n",
      "Iteration 1614, loss = 0.13909072\n",
      "Iteration 1615, loss = 0.13900071\n",
      "Iteration 1616, loss = 0.13891089\n",
      "Iteration 1617, loss = 0.13882124\n",
      "Iteration 1618, loss = 0.13873178\n",
      "Iteration 1619, loss = 0.13864251\n",
      "Iteration 1620, loss = 0.13855344\n",
      "Iteration 1621, loss = 0.13846456\n",
      "Iteration 1622, loss = 0.13837590\n",
      "Iteration 1623, loss = 0.13828744\n",
      "Iteration 1624, loss = 0.13819920\n",
      "Iteration 1625, loss = 0.13811118\n",
      "Iteration 1626, loss = 0.13802338\n",
      "Iteration 1627, loss = 0.13793581\n",
      "Iteration 1628, loss = 0.13784847\n",
      "Iteration 1629, loss = 0.13776136\n",
      "Iteration 1630, loss = 0.13767449\n",
      "Iteration 1631, loss = 0.13758786\n",
      "Iteration 1632, loss = 0.13750146\n",
      "Iteration 1633, loss = 0.13741530\n",
      "Iteration 1634, loss = 0.13732939\n",
      "Iteration 1635, loss = 0.13724371\n",
      "Iteration 1636, loss = 0.13715826\n",
      "Iteration 1637, loss = 0.13707306\n",
      "Iteration 1638, loss = 0.13698809\n",
      "Iteration 1639, loss = 0.13690335\n",
      "Iteration 1640, loss = 0.13681884\n",
      "Iteration 1641, loss = 0.13673455\n",
      "Iteration 1642, loss = 0.13665049\n",
      "Iteration 1643, loss = 0.13656665\n",
      "Iteration 1644, loss = 0.13648303\n",
      "Iteration 1645, loss = 0.13639962\n",
      "Iteration 1646, loss = 0.13631643\n",
      "Iteration 1647, loss = 0.13623343\n",
      "Iteration 1648, loss = 0.13615065\n",
      "Iteration 1649, loss = 0.13606806\n",
      "Iteration 1650, loss = 0.13598567\n",
      "Iteration 1651, loss = 0.13590347\n",
      "Iteration 1652, loss = 0.13582147\n",
      "Iteration 1653, loss = 0.13573965\n",
      "Iteration 1654, loss = 0.13565802\n",
      "Iteration 1655, loss = 0.13557658\n",
      "Iteration 1656, loss = 0.13549532\n",
      "Iteration 1657, loss = 0.13541425\n",
      "Iteration 1658, loss = 0.13533338\n",
      "Iteration 1659, loss = 0.13525274\n",
      "Iteration 1660, loss = 0.13517237\n",
      "Iteration 1661, loss = 0.13509239\n",
      "Iteration 1662, loss = 0.13501300\n",
      "Iteration 1663, loss = 0.13493475\n",
      "Iteration 1664, loss = 0.13485850\n",
      "Iteration 1665, loss = 0.13478701\n",
      "Iteration 1666, loss = 0.13472302\n",
      "Iteration 1667, loss = 0.13468038\n",
      "Iteration 1668, loss = 0.13465639\n",
      "Iteration 1669, loss = 0.13470285\n",
      "Iteration 1670, loss = 0.13468693\n",
      "Iteration 1671, loss = 0.13469109\n",
      "Iteration 1672, loss = 0.13441645\n",
      "Iteration 1673, loss = 0.13418699\n",
      "Iteration 1674, loss = 0.13408893\n",
      "Iteration 1675, loss = 0.13408063\n",
      "Iteration 1676, loss = 0.13405458\n",
      "Iteration 1677, loss = 0.13393636\n",
      "Iteration 1678, loss = 0.13387324\n",
      "Iteration 1679, loss = 0.13375731\n",
      "Iteration 1680, loss = 0.13365499\n",
      "Iteration 1681, loss = 0.13354636\n",
      "Iteration 1682, loss = 0.13350374\n",
      "Iteration 1683, loss = 0.13351155\n",
      "Iteration 1684, loss = 0.13338589\n",
      "Iteration 1685, loss = 0.13325159\n",
      "Iteration 1686, loss = 0.13314802\n",
      "Iteration 1687, loss = 0.13311036\n",
      "Iteration 1688, loss = 0.13308269\n",
      "Iteration 1689, loss = 0.13296233\n",
      "Iteration 1690, loss = 0.13287217\n",
      "Iteration 1691, loss = 0.13278218\n",
      "Iteration 1692, loss = 0.13270357\n",
      "Iteration 1693, loss = 0.13264423\n",
      "Iteration 1694, loss = 0.13257455\n",
      "Iteration 1695, loss = 0.13250941\n",
      "Iteration 1696, loss = 0.13239807\n",
      "Iteration 1697, loss = 0.13231055\n",
      "Iteration 1698, loss = 0.13224868\n",
      "Iteration 1699, loss = 0.13217490\n",
      "Iteration 1700, loss = 0.13209858\n",
      "Iteration 1701, loss = 0.13202756\n",
      "Iteration 1702, loss = 0.13195200\n",
      "Iteration 1703, loss = 0.13187033\n",
      "Iteration 1704, loss = 0.13178929\n",
      "Iteration 1705, loss = 0.13171628\n",
      "Iteration 1706, loss = 0.13164970\n",
      "Iteration 1707, loss = 0.13157628\n",
      "Iteration 1708, loss = 0.13149973\n",
      "Iteration 1709, loss = 0.13142903\n",
      "Iteration 1710, loss = 0.13135274\n",
      "Iteration 1711, loss = 0.13127576\n",
      "Iteration 1712, loss = 0.13120202\n",
      "Iteration 1713, loss = 0.13113040\n",
      "Iteration 1714, loss = 0.13106036\n",
      "Iteration 1715, loss = 0.13098678\n",
      "Iteration 1716, loss = 0.13091407\n",
      "Iteration 1717, loss = 0.13084301\n",
      "Iteration 1718, loss = 0.13077029\n",
      "Iteration 1719, loss = 0.13069556\n",
      "Iteration 1720, loss = 0.13062346\n",
      "Iteration 1721, loss = 0.13055213\n",
      "Iteration 1722, loss = 0.13048033\n",
      "Iteration 1723, loss = 0.13040903\n",
      "Iteration 1724, loss = 0.13033780\n",
      "Iteration 1725, loss = 0.13026740\n",
      "Iteration 1726, loss = 0.13019665\n",
      "Iteration 1727, loss = 0.13012489\n",
      "Iteration 1728, loss = 0.13005378\n",
      "Iteration 1729, loss = 0.12998337\n",
      "Iteration 1730, loss = 0.12991214\n",
      "Iteration 1731, loss = 0.12984099\n",
      "Iteration 1732, loss = 0.12977046\n",
      "Iteration 1733, loss = 0.12970007\n",
      "Iteration 1734, loss = 0.12962976\n",
      "Iteration 1735, loss = 0.12955954\n",
      "Iteration 1736, loss = 0.12948941\n",
      "Iteration 1737, loss = 0.12941971\n",
      "Iteration 1738, loss = 0.12935015\n",
      "Iteration 1739, loss = 0.12928036\n",
      "Iteration 1740, loss = 0.12921084\n",
      "Iteration 1741, loss = 0.12914169\n",
      "Iteration 1742, loss = 0.12907256\n",
      "Iteration 1743, loss = 0.12900351\n",
      "Iteration 1744, loss = 0.12893474\n",
      "Iteration 1745, loss = 0.12886623\n",
      "Iteration 1746, loss = 0.12879825\n",
      "Iteration 1747, loss = 0.12873078\n",
      "Iteration 1748, loss = 0.12866443\n",
      "Iteration 1749, loss = 0.12859930\n",
      "Iteration 1750, loss = 0.12853837\n",
      "Iteration 1751, loss = 0.12847946\n",
      "Iteration 1752, loss = 0.12843360\n",
      "Iteration 1753, loss = 0.12838678\n",
      "Iteration 1754, loss = 0.12837478\n",
      "Iteration 1755, loss = 0.12832423\n",
      "Iteration 1756, loss = 0.12831733\n",
      "Iteration 1757, loss = 0.12818633\n",
      "Iteration 1758, loss = 0.12807519\n",
      "Iteration 1759, loss = 0.12794038\n",
      "Iteration 1760, loss = 0.12784684\n",
      "Iteration 1761, loss = 0.12778295\n",
      "Iteration 1762, loss = 0.12773882\n",
      "Iteration 1763, loss = 0.12771037\n",
      "Iteration 1764, loss = 0.12764952\n",
      "Iteration 1765, loss = 0.12759126\n",
      "Iteration 1766, loss = 0.12748961\n",
      "Iteration 1767, loss = 0.12739628\n",
      "Iteration 1768, loss = 0.12731236\n",
      "Iteration 1769, loss = 0.12724510\n",
      "Iteration 1770, loss = 0.12718801\n",
      "Iteration 1771, loss = 0.12713406\n",
      "Iteration 1772, loss = 0.12708349\n",
      "Iteration 1773, loss = 0.12701366\n",
      "Iteration 1774, loss = 0.12694430\n",
      "Iteration 1775, loss = 0.12686501\n",
      "Iteration 1776, loss = 0.12678888\n",
      "Iteration 1777, loss = 0.12671679\n",
      "Iteration 1778, loss = 0.12665025\n",
      "Iteration 1779, loss = 0.12658691\n",
      "Iteration 1780, loss = 0.12652624\n",
      "Iteration 1781, loss = 0.12646751\n",
      "Iteration 1782, loss = 0.12640444\n",
      "Iteration 1783, loss = 0.12634196\n",
      "Iteration 1784, loss = 0.12627525\n",
      "Iteration 1785, loss = 0.12620897\n",
      "Iteration 1786, loss = 0.12614011\n",
      "Iteration 1787, loss = 0.12607287\n",
      "Iteration 1788, loss = 0.12600430\n",
      "Iteration 1789, loss = 0.12593737\n",
      "Iteration 1790, loss = 0.12587140\n",
      "Iteration 1791, loss = 0.12580595\n",
      "Iteration 1792, loss = 0.12574114\n",
      "Iteration 1793, loss = 0.12567705\n",
      "Iteration 1794, loss = 0.12561323\n",
      "Iteration 1795, loss = 0.12554961\n",
      "Iteration 1796, loss = 0.12548662\n",
      "Iteration 1797, loss = 0.12542390\n",
      "Iteration 1798, loss = 0.12536199\n",
      "Iteration 1799, loss = 0.12530096\n",
      "Iteration 1800, loss = 0.12524256\n",
      "Iteration 1801, loss = 0.12518566\n",
      "Iteration 1802, loss = 0.12513664\n",
      "Iteration 1803, loss = 0.12508939\n",
      "Iteration 1804, loss = 0.12506549\n",
      "Iteration 1805, loss = 0.12502862\n",
      "Iteration 1806, loss = 0.12503585\n",
      "Iteration 1807, loss = 0.12495632\n",
      "Iteration 1808, loss = 0.12490787\n",
      "Iteration 1809, loss = 0.12475631\n",
      "Iteration 1810, loss = 0.12463586\n",
      "Iteration 1811, loss = 0.12453996\n",
      "Iteration 1812, loss = 0.12448147\n",
      "Iteration 1813, loss = 0.12445086\n",
      "Iteration 1814, loss = 0.12441679\n",
      "Iteration 1815, loss = 0.12438321\n",
      "Iteration 1816, loss = 0.12429947\n",
      "Iteration 1817, loss = 0.12421710\n",
      "Iteration 1818, loss = 0.12411954\n",
      "Iteration 1819, loss = 0.12404012\n",
      "Iteration 1820, loss = 0.12398013\n",
      "Iteration 1821, loss = 0.12393313\n",
      "Iteration 1822, loss = 0.12389191\n",
      "Iteration 1823, loss = 0.12383533\n",
      "Iteration 1824, loss = 0.12377510\n",
      "Iteration 1825, loss = 0.12369785\n",
      "Iteration 1826, loss = 0.12362380\n",
      "Iteration 1827, loss = 0.12355038\n",
      "Iteration 1828, loss = 0.12348405\n",
      "Iteration 1829, loss = 0.12342458\n",
      "Iteration 1830, loss = 0.12336928\n",
      "Iteration 1831, loss = 0.12331633\n",
      "Iteration 1832, loss = 0.12325989\n",
      "Iteration 1833, loss = 0.12320351\n",
      "Iteration 1834, loss = 0.12314034\n",
      "Iteration 1835, loss = 0.12307857\n",
      "Iteration 1836, loss = 0.12301209\n",
      "Iteration 1837, loss = 0.12294716\n",
      "Iteration 1838, loss = 0.12288154\n",
      "Iteration 1839, loss = 0.12281768\n",
      "Iteration 1840, loss = 0.12275468\n",
      "Iteration 1841, loss = 0.12269277\n",
      "Iteration 1842, loss = 0.12263166\n",
      "Iteration 1843, loss = 0.12257113\n",
      "Iteration 1844, loss = 0.12251101\n",
      "Iteration 1845, loss = 0.12245128\n",
      "Iteration 1846, loss = 0.12239199\n",
      "Iteration 1847, loss = 0.12233332\n",
      "Iteration 1848, loss = 0.12227605\n",
      "Iteration 1849, loss = 0.12222039\n",
      "Iteration 1850, loss = 0.12216984\n",
      "Iteration 1851, loss = 0.12212334\n",
      "Iteration 1852, loss = 0.12209725\n",
      "Iteration 1853, loss = 0.12207411\n",
      "Iteration 1854, loss = 0.12211559\n",
      "Iteration 1855, loss = 0.12207359\n",
      "Iteration 1856, loss = 0.12210558\n",
      "Iteration 1857, loss = 0.12190742\n",
      "Iteration 1858, loss = 0.12175339\n",
      "Iteration 1859, loss = 0.12162544\n",
      "Iteration 1860, loss = 0.12157005\n",
      "Iteration 1861, loss = 0.12156760\n",
      "Iteration 1862, loss = 0.12154839\n",
      "Iteration 1863, loss = 0.12152194\n",
      "Iteration 1864, loss = 0.12140040\n",
      "Iteration 1865, loss = 0.12129767\n",
      "Iteration 1866, loss = 0.12120799\n",
      "Iteration 1867, loss = 0.12115151\n",
      "Iteration 1868, loss = 0.12112068\n",
      "Iteration 1869, loss = 0.12108209\n",
      "Iteration 1870, loss = 0.12103725\n",
      "Iteration 1871, loss = 0.12095292\n",
      "Iteration 1872, loss = 0.12086742\n",
      "Iteration 1873, loss = 0.12078915\n",
      "Iteration 1874, loss = 0.12073023\n",
      "Iteration 1875, loss = 0.12068635\n",
      "Iteration 1876, loss = 0.12064079\n",
      "Iteration 1877, loss = 0.12059171\n",
      "Iteration 1878, loss = 0.12052527\n",
      "Iteration 1879, loss = 0.12045690\n",
      "Iteration 1880, loss = 0.12038547\n",
      "Iteration 1881, loss = 0.12032013\n",
      "Iteration 1882, loss = 0.12025885\n",
      "Iteration 1883, loss = 0.12020101\n",
      "Iteration 1884, loss = 0.12014600\n",
      "Iteration 1885, loss = 0.12009132\n",
      "Iteration 1886, loss = 0.12003983\n",
      "Iteration 1887, loss = 0.11998611\n",
      "Iteration 1888, loss = 0.11993564\n",
      "Iteration 1889, loss = 0.11987722\n",
      "Iteration 1890, loss = 0.11982423\n",
      "Iteration 1891, loss = 0.11976105\n",
      "Iteration 1892, loss = 0.11970739\n",
      "Iteration 1893, loss = 0.11964623\n",
      "Iteration 1894, loss = 0.11959204\n",
      "Iteration 1895, loss = 0.11952936\n",
      "Iteration 1896, loss = 0.11947680\n",
      "Iteration 1897, loss = 0.11941473\n",
      "Iteration 1898, loss = 0.11936578\n",
      "Iteration 1899, loss = 0.11930407\n",
      "Iteration 1900, loss = 0.11925605\n",
      "Iteration 1901, loss = 0.11919285\n",
      "Iteration 1902, loss = 0.11914703\n",
      "Iteration 1903, loss = 0.11907929\n",
      "Iteration 1904, loss = 0.11902994\n",
      "Iteration 1905, loss = 0.11895930\n",
      "Iteration 1906, loss = 0.11890461\n",
      "Iteration 1907, loss = 0.11883269\n",
      "Iteration 1908, loss = 0.11877276\n",
      "Iteration 1909, loss = 0.11870000\n",
      "Iteration 1910, loss = 0.11863837\n",
      "Iteration 1911, loss = 0.11856983\n",
      "Iteration 1912, loss = 0.11850826\n",
      "Iteration 1913, loss = 0.11844327\n",
      "Iteration 1914, loss = 0.11838378\n",
      "Iteration 1915, loss = 0.11832210\n",
      "Iteration 1916, loss = 0.11826692\n",
      "Iteration 1917, loss = 0.11820947\n",
      "Iteration 1918, loss = 0.11816255\n",
      "Iteration 1919, loss = 0.11811298\n",
      "Iteration 1920, loss = 0.11808820\n",
      "Iteration 1921, loss = 0.11804506\n",
      "Iteration 1922, loss = 0.11805519\n",
      "Iteration 1923, loss = 0.11798009\n",
      "Iteration 1924, loss = 0.11795884\n",
      "Iteration 1925, loss = 0.11782742\n",
      "Iteration 1926, loss = 0.11772898\n",
      "Iteration 1927, loss = 0.11760596\n",
      "Iteration 1928, loss = 0.11751472\n",
      "Iteration 1929, loss = 0.11744387\n",
      "Iteration 1930, loss = 0.11739134\n",
      "Iteration 1931, loss = 0.11735359\n",
      "Iteration 1932, loss = 0.11731347\n",
      "Iteration 1933, loss = 0.11729129\n",
      "Iteration 1934, loss = 0.11723564\n",
      "Iteration 1935, loss = 0.11719590\n",
      "Iteration 1936, loss = 0.11709592\n",
      "Iteration 1937, loss = 0.11701427\n",
      "Iteration 1938, loss = 0.11690840\n",
      "Iteration 1939, loss = 0.11682031\n",
      "Iteration 1940, loss = 0.11673961\n",
      "Iteration 1941, loss = 0.11666857\n",
      "Iteration 1942, loss = 0.11660420\n",
      "Iteration 1943, loss = 0.11654442\n",
      "Iteration 1944, loss = 0.11649052\n",
      "Iteration 1945, loss = 0.11643962\n",
      "Iteration 1946, loss = 0.11640807\n",
      "Iteration 1947, loss = 0.11636662\n",
      "Iteration 1948, loss = 0.11637721\n",
      "Iteration 1949, loss = 0.11632223\n",
      "Iteration 1950, loss = 0.11632546\n",
      "Iteration 1951, loss = 0.11618716\n",
      "Iteration 1952, loss = 0.11608407\n",
      "Iteration 1953, loss = 0.11591925\n",
      "Iteration 1954, loss = 0.11579518\n",
      "Iteration 1955, loss = 0.11569301\n",
      "Iteration 1956, loss = 0.11561707\n",
      "Iteration 1957, loss = 0.11556322\n",
      "Iteration 1958, loss = 0.11552030\n",
      "Iteration 1959, loss = 0.11550009\n",
      "Iteration 1960, loss = 0.11545722\n",
      "Iteration 1961, loss = 0.11544318\n",
      "Iteration 1962, loss = 0.11535324\n",
      "Iteration 1963, loss = 0.11527791\n",
      "Iteration 1964, loss = 0.11512194\n",
      "Iteration 1965, loss = 0.11499000\n",
      "Iteration 1966, loss = 0.11485566\n",
      "Iteration 1967, loss = 0.11474703\n",
      "Iteration 1968, loss = 0.11465554\n",
      "Iteration 1969, loss = 0.11457699\n",
      "Iteration 1970, loss = 0.11450981\n",
      "Iteration 1971, loss = 0.11445389\n",
      "Iteration 1972, loss = 0.11442822\n",
      "Iteration 1973, loss = 0.11441948\n",
      "Iteration 1974, loss = 0.11449227\n",
      "Iteration 1975, loss = 0.11449745\n",
      "Iteration 1976, loss = 0.11451418\n",
      "Iteration 1977, loss = 0.11424838\n",
      "Iteration 1978, loss = 0.11395445\n",
      "Iteration 1979, loss = 0.11372443\n",
      "Iteration 1980, loss = 0.11362293\n",
      "Iteration 1981, loss = 0.11362177\n",
      "Iteration 1982, loss = 0.11365149\n",
      "Iteration 1983, loss = 0.11366221\n",
      "Iteration 1984, loss = 0.11350455\n",
      "Iteration 1985, loss = 0.11330726\n",
      "Iteration 1986, loss = 0.11309476\n",
      "Iteration 1987, loss = 0.11295533\n",
      "Iteration 1988, loss = 0.11289287\n",
      "Iteration 1989, loss = 0.11287308\n",
      "Iteration 1990, loss = 0.11285851\n",
      "Iteration 1991, loss = 0.11280695\n",
      "Iteration 1992, loss = 0.11275075\n",
      "Iteration 1993, loss = 0.11262780\n",
      "Iteration 1994, loss = 0.11247473\n",
      "Iteration 1995, loss = 0.11230676\n",
      "Iteration 1996, loss = 0.11216851\n",
      "Iteration 1997, loss = 0.11206478\n",
      "Iteration 1998, loss = 0.11199195\n",
      "Iteration 1999, loss = 0.11194087\n",
      "Iteration 2000, loss = 0.11190327\n",
      "Iteration 2001, loss = 0.11188607\n",
      "Iteration 2002, loss = 0.11186457\n",
      "Iteration 2003, loss = 0.11185139\n",
      "Iteration 2004, loss = 0.11177683\n",
      "Iteration 2005, loss = 0.11166796\n",
      "Iteration 2006, loss = 0.11148144\n",
      "Iteration 2007, loss = 0.11130948\n",
      "Iteration 2008, loss = 0.11117436\n",
      "Iteration 2009, loss = 0.11109127\n",
      "Iteration 2010, loss = 0.11104863\n",
      "Iteration 2011, loss = 0.11102271\n",
      "Iteration 2012, loss = 0.11099944\n",
      "Iteration 2013, loss = 0.11094237\n",
      "Iteration 2014, loss = 0.11085437\n",
      "Iteration 2015, loss = 0.11072439\n",
      "Iteration 2016, loss = 0.11059763\n",
      "Iteration 2017, loss = 0.11048907\n",
      "Iteration 2018, loss = 0.11041138\n",
      "Iteration 2019, loss = 0.11035827\n",
      "Iteration 2020, loss = 0.11031551\n",
      "Iteration 2021, loss = 0.11027463\n",
      "Iteration 2022, loss = 0.11021631\n",
      "Iteration 2023, loss = 0.11014643\n",
      "Iteration 2024, loss = 0.11005469\n",
      "Iteration 2025, loss = 0.10996083\n",
      "Iteration 2026, loss = 0.10986883\n",
      "Iteration 2027, loss = 0.10978814\n",
      "Iteration 2028, loss = 0.10971856\n",
      "Iteration 2029, loss = 0.10965935\n",
      "Iteration 2030, loss = 0.10960678\n",
      "Iteration 2031, loss = 0.10955526\n",
      "Iteration 2032, loss = 0.10950408\n",
      "Iteration 2033, loss = 0.10944724\n",
      "Iteration 2034, loss = 0.10938847\n",
      "Iteration 2035, loss = 0.10932124\n",
      "Iteration 2036, loss = 0.10925222\n",
      "Iteration 2037, loss = 0.10917901\n",
      "Iteration 2038, loss = 0.10910811\n",
      "Iteration 2039, loss = 0.10903870\n",
      "Iteration 2040, loss = 0.10897291\n",
      "Iteration 2041, loss = 0.10890988\n",
      "Iteration 2042, loss = 0.10884942\n",
      "Iteration 2043, loss = 0.10879100\n",
      "Iteration 2044, loss = 0.10873412\n",
      "Iteration 2045, loss = 0.10867858\n",
      "Iteration 2046, loss = 0.10862404\n",
      "Iteration 2047, loss = 0.10857108\n",
      "Iteration 2048, loss = 0.10851966\n",
      "Iteration 2049, loss = 0.10847185\n",
      "Iteration 2050, loss = 0.10842740\n",
      "Iteration 2051, loss = 0.10839240\n",
      "Iteration 2052, loss = 0.10836347\n",
      "Iteration 2053, loss = 0.10835599\n",
      "Iteration 2054, loss = 0.10834671\n",
      "Iteration 2055, loss = 0.10836139\n",
      "Iteration 2056, loss = 0.10832088\n",
      "Iteration 2057, loss = 0.10826912\n",
      "Iteration 2058, loss = 0.10812912\n",
      "Iteration 2059, loss = 0.10799326\n",
      "Iteration 2060, loss = 0.10787128\n",
      "Iteration 2061, loss = 0.10780220\n",
      "Iteration 2062, loss = 0.10777592\n",
      "Iteration 2063, loss = 0.10776090\n",
      "Iteration 2064, loss = 0.10773533\n",
      "Iteration 2065, loss = 0.10765726\n",
      "Iteration 2066, loss = 0.10756518\n",
      "Iteration 2067, loss = 0.10748014\n",
      "Iteration 2068, loss = 0.10742498\n",
      "Iteration 2069, loss = 0.10739283\n",
      "Iteration 2070, loss = 0.10736306\n",
      "Iteration 2071, loss = 0.10732118\n",
      "Iteration 2072, loss = 0.10724883\n",
      "Iteration 2073, loss = 0.10717200\n",
      "Iteration 2074, loss = 0.10710215\n",
      "Iteration 2075, loss = 0.10704942\n",
      "Iteration 2076, loss = 0.10700879\n",
      "Iteration 2077, loss = 0.10696733\n",
      "Iteration 2078, loss = 0.10691792\n",
      "Iteration 2079, loss = 0.10685592\n",
      "Iteration 2080, loss = 0.10679260\n",
      "Iteration 2081, loss = 0.10673359\n",
      "Iteration 2082, loss = 0.10668231\n",
      "Iteration 2083, loss = 0.10663638\n",
      "Iteration 2084, loss = 0.10659032\n",
      "Iteration 2085, loss = 0.10654081\n",
      "Iteration 2086, loss = 0.10648577\n",
      "Iteration 2087, loss = 0.10642891\n",
      "Iteration 2088, loss = 0.10637295\n",
      "Iteration 2089, loss = 0.10632055\n",
      "Iteration 2090, loss = 0.10627113\n",
      "Iteration 2091, loss = 0.10622363\n",
      "Iteration 2092, loss = 0.10617532\n",
      "Iteration 2093, loss = 0.10612587\n",
      "Iteration 2094, loss = 0.10607409\n",
      "Iteration 2095, loss = 0.10602349\n",
      "Iteration 2096, loss = 0.10597181\n",
      "Iteration 2097, loss = 0.10592577\n",
      "Iteration 2098, loss = 0.10587704\n",
      "Iteration 2099, loss = 0.10583573\n",
      "Iteration 2100, loss = 0.10578560\n",
      "Iteration 2101, loss = 0.10574320\n",
      "Iteration 2102, loss = 0.10568656\n",
      "Iteration 2103, loss = 0.10563749\n",
      "Iteration 2104, loss = 0.10557563\n",
      "Iteration 2105, loss = 0.10552011\n",
      "Iteration 2106, loss = 0.10545865\n",
      "Iteration 2107, loss = 0.10540177\n",
      "Iteration 2108, loss = 0.10534497\n",
      "Iteration 2109, loss = 0.10529096\n",
      "Iteration 2110, loss = 0.10523870\n",
      "Iteration 2111, loss = 0.10518799\n",
      "Iteration 2112, loss = 0.10513844\n",
      "Iteration 2113, loss = 0.10508938\n",
      "Iteration 2114, loss = 0.10504123\n",
      "Iteration 2115, loss = 0.10499165\n",
      "Iteration 2116, loss = 0.10494354\n",
      "Iteration 2117, loss = 0.10489224\n",
      "Iteration 2118, loss = 0.10484281\n",
      "Iteration 2119, loss = 0.10478979\n",
      "Iteration 2120, loss = 0.10473854\n",
      "Iteration 2121, loss = 0.10468464\n",
      "Iteration 2122, loss = 0.10463226\n",
      "Iteration 2123, loss = 0.10457858\n",
      "Iteration 2124, loss = 0.10452593\n",
      "Iteration 2125, loss = 0.10447293\n",
      "Iteration 2126, loss = 0.10442045\n",
      "Iteration 2127, loss = 0.10436805\n",
      "Iteration 2128, loss = 0.10431591\n",
      "Iteration 2129, loss = 0.10426393\n",
      "Iteration 2130, loss = 0.10421213\n",
      "Iteration 2131, loss = 0.10416047\n",
      "Iteration 2132, loss = 0.10410893\n",
      "Iteration 2133, loss = 0.10405745\n",
      "Iteration 2134, loss = 0.10400600\n",
      "Iteration 2135, loss = 0.10395455\n",
      "Iteration 2136, loss = 0.10390308\n",
      "Iteration 2137, loss = 0.10385161\n",
      "Iteration 2138, loss = 0.10380014\n",
      "Iteration 2139, loss = 0.10374879\n",
      "Iteration 2140, loss = 0.10369746\n",
      "Iteration 2141, loss = 0.10364652\n",
      "Iteration 2142, loss = 0.10359562\n",
      "Iteration 2143, loss = 0.10354571\n",
      "Iteration 2144, loss = 0.10349568\n",
      "Iteration 2145, loss = 0.10344809\n",
      "Iteration 2146, loss = 0.10339942\n",
      "Iteration 2147, loss = 0.10335641\n",
      "Iteration 2148, loss = 0.10330845\n",
      "Iteration 2149, loss = 0.10327145\n",
      "Iteration 2150, loss = 0.10321981\n",
      "Iteration 2151, loss = 0.10318290\n",
      "Iteration 2152, loss = 0.10312205\n",
      "Iteration 2153, loss = 0.10307314\n",
      "Iteration 2154, loss = 0.10300621\n",
      "Iteration 2155, loss = 0.10294577\n",
      "Iteration 2156, loss = 0.10288066\n",
      "Iteration 2157, loss = 0.10281946\n",
      "Iteration 2158, loss = 0.10275981\n",
      "Iteration 2159, loss = 0.10270308\n",
      "Iteration 2160, loss = 0.10264884\n",
      "Iteration 2161, loss = 0.10259699\n",
      "Iteration 2162, loss = 0.10254728\n",
      "Iteration 2163, loss = 0.10249737\n",
      "Iteration 2164, loss = 0.10244792\n",
      "Iteration 2165, loss = 0.10239514\n",
      "Iteration 2166, loss = 0.10234173\n",
      "Iteration 2167, loss = 0.10228516\n",
      "Iteration 2168, loss = 0.10222837\n",
      "Iteration 2169, loss = 0.10217067\n",
      "Iteration 2170, loss = 0.10211379\n",
      "Iteration 2171, loss = 0.10205761\n",
      "Iteration 2172, loss = 0.10200249\n",
      "Iteration 2173, loss = 0.10194794\n",
      "Iteration 2174, loss = 0.10189358\n",
      "Iteration 2175, loss = 0.10183914\n",
      "Iteration 2176, loss = 0.10178449\n",
      "Iteration 2177, loss = 0.10172977\n",
      "Iteration 2178, loss = 0.10167492\n",
      "Iteration 2179, loss = 0.10162022\n",
      "Iteration 2180, loss = 0.10156494\n",
      "Iteration 2181, loss = 0.10150976\n",
      "Iteration 2182, loss = 0.10145358\n",
      "Iteration 2183, loss = 0.10139740\n",
      "Iteration 2184, loss = 0.10134043\n",
      "Iteration 2185, loss = 0.10128351\n",
      "Iteration 2186, loss = 0.10122625\n",
      "Iteration 2187, loss = 0.10116904\n",
      "Iteration 2188, loss = 0.10111165\n",
      "Iteration 2189, loss = 0.10105414\n",
      "Iteration 2190, loss = 0.10099639\n",
      "Iteration 2191, loss = 0.10093843\n",
      "Iteration 2192, loss = 0.10088027\n",
      "Iteration 2193, loss = 0.10082196\n",
      "Iteration 2194, loss = 0.10076349\n",
      "Iteration 2195, loss = 0.10070482\n",
      "Iteration 2196, loss = 0.10064590\n",
      "Iteration 2197, loss = 0.10058668\n",
      "Iteration 2198, loss = 0.10052716\n",
      "Iteration 2199, loss = 0.10046735\n",
      "Iteration 2200, loss = 0.10040726\n",
      "Iteration 2201, loss = 0.10034689\n",
      "Iteration 2202, loss = 0.10028620\n",
      "Iteration 2203, loss = 0.10022518\n",
      "Iteration 2204, loss = 0.10016382\n",
      "Iteration 2205, loss = 0.10010210\n",
      "Iteration 2206, loss = 0.10004001\n",
      "Iteration 2207, loss = 0.09997755\n",
      "Iteration 2208, loss = 0.09991471\n",
      "Iteration 2209, loss = 0.09985146\n",
      "Iteration 2210, loss = 0.09978781\n",
      "Iteration 2211, loss = 0.09972374\n",
      "Iteration 2212, loss = 0.09965924\n",
      "Iteration 2213, loss = 0.09959429\n",
      "Iteration 2214, loss = 0.09952892\n",
      "Iteration 2215, loss = 0.09946311\n",
      "Iteration 2216, loss = 0.09939694\n",
      "Iteration 2217, loss = 0.09933043\n",
      "Iteration 2218, loss = 0.09926386\n",
      "Iteration 2219, loss = 0.09919721\n",
      "Iteration 2220, loss = 0.09913166\n",
      "Iteration 2221, loss = 0.09906653\n",
      "Iteration 2222, loss = 0.09900653\n",
      "Iteration 2223, loss = 0.09894618\n",
      "Iteration 2224, loss = 0.09890277\n",
      "Iteration 2225, loss = 0.09884447\n",
      "Iteration 2226, loss = 0.09882086\n",
      "Iteration 2227, loss = 0.09873406\n",
      "Iteration 2228, loss = 0.09867427\n",
      "Iteration 2229, loss = 0.09855545\n",
      "Iteration 2230, loss = 0.09844613\n",
      "Iteration 2231, loss = 0.09833808\n",
      "Iteration 2232, loss = 0.09824894\n",
      "Iteration 2233, loss = 0.09817841\n",
      "Iteration 2234, loss = 0.09811539\n",
      "Iteration 2235, loss = 0.09805294\n",
      "Iteration 2236, loss = 0.09796865\n",
      "Iteration 2237, loss = 0.09787483\n",
      "Iteration 2238, loss = 0.09777066\n",
      "Iteration 2239, loss = 0.09767327\n",
      "Iteration 2240, loss = 0.09758739\n",
      "Iteration 2241, loss = 0.09750917\n",
      "Iteration 2242, loss = 0.09743006\n",
      "Iteration 2243, loss = 0.09734003\n",
      "Iteration 2244, loss = 0.09724271\n",
      "Iteration 2245, loss = 0.09714016\n",
      "Iteration 2246, loss = 0.09704030\n",
      "Iteration 2247, loss = 0.09694432\n",
      "Iteration 2248, loss = 0.09685025\n",
      "Iteration 2249, loss = 0.09675476\n",
      "Iteration 2250, loss = 0.09665451\n",
      "Iteration 2251, loss = 0.09655008\n",
      "Iteration 2252, loss = 0.09644171\n",
      "Iteration 2253, loss = 0.09633277\n",
      "Iteration 2254, loss = 0.09622390\n",
      "Iteration 2255, loss = 0.09611492\n",
      "Iteration 2256, loss = 0.09600459\n",
      "Iteration 2257, loss = 0.09589173\n",
      "Iteration 2258, loss = 0.09577629\n",
      "Iteration 2259, loss = 0.09565762\n",
      "Iteration 2260, loss = 0.09553698\n",
      "Iteration 2261, loss = 0.09541426\n",
      "Iteration 2262, loss = 0.09529032\n",
      "Iteration 2263, loss = 0.09516516\n",
      "Iteration 2264, loss = 0.09503854\n",
      "Iteration 2265, loss = 0.09491018\n",
      "Iteration 2266, loss = 0.09477964\n",
      "Iteration 2267, loss = 0.09464713\n",
      "Iteration 2268, loss = 0.09451237\n",
      "Iteration 2269, loss = 0.09437586\n",
      "Iteration 2270, loss = 0.09423780\n",
      "Iteration 2271, loss = 0.09409857\n",
      "Iteration 2272, loss = 0.09395833\n",
      "Iteration 2273, loss = 0.09381715\n",
      "Iteration 2274, loss = 0.09367501\n",
      "Iteration 2275, loss = 0.09353192\n",
      "Iteration 2276, loss = 0.09338799\n",
      "Iteration 2277, loss = 0.09324318\n",
      "Iteration 2278, loss = 0.09309778\n",
      "Iteration 2279, loss = 0.09295175\n",
      "Iteration 2280, loss = 0.09280547\n",
      "Iteration 2281, loss = 0.09265889\n",
      "Iteration 2282, loss = 0.09251236\n",
      "Iteration 2283, loss = 0.09236583\n",
      "Iteration 2284, loss = 0.09221973\n",
      "Iteration 2285, loss = 0.09207401\n",
      "Iteration 2286, loss = 0.09192960\n",
      "Iteration 2287, loss = 0.09178631\n",
      "Iteration 2288, loss = 0.09164722\n",
      "Iteration 2289, loss = 0.09151163\n",
      "Iteration 2290, loss = 0.09139180\n",
      "Iteration 2291, loss = 0.09128204\n",
      "Iteration 2292, loss = 0.09122519\n",
      "Iteration 2293, loss = 0.09117386\n",
      "Iteration 2294, loss = 0.09117124\n",
      "Iteration 2295, loss = 0.09112254\n",
      "Iteration 2296, loss = 0.09099326\n",
      "Iteration 2297, loss = 0.09087144\n",
      "Iteration 2298, loss = 0.09084088\n",
      "Iteration 2299, loss = 0.09091790\n",
      "Iteration 2300, loss = 0.09023897\n",
      "Iteration 2301, loss = 0.09015793\n",
      "Iteration 2302, loss = 0.09044613\n",
      "Iteration 2303, loss = 0.08977238\n",
      "Iteration 2304, loss = 0.09086829\n",
      "Iteration 2305, loss = 0.09144295\n",
      "Iteration 2306, loss = 0.09220313\n",
      "Iteration 2307, loss = 0.09021713\n",
      "Iteration 2308, loss = 0.09131943\n",
      "Iteration 2309, loss = 0.08959737\n",
      "Iteration 2310, loss = 0.09164586\n",
      "Iteration 2311, loss = 0.09462835\n",
      "Iteration 2312, loss = 0.09626922\n",
      "Iteration 2313, loss = 0.09148754\n",
      "Iteration 2314, loss = 0.09629504\n",
      "Iteration 2315, loss = 0.09165055\n",
      "Iteration 2316, loss = 0.09423078\n",
      "Iteration 2317, loss = 0.09247816\n",
      "Iteration 2318, loss = 0.09383766\n",
      "Iteration 2319, loss = 0.09213128\n",
      "Iteration 2320, loss = 0.09336448\n",
      "Iteration 2321, loss = 0.09135631\n",
      "Iteration 2322, loss = 0.09084541\n",
      "Iteration 2323, loss = 0.09186598\n",
      "Iteration 2324, loss = 0.08912331\n",
      "Iteration 2325, loss = 0.08980017\n",
      "Iteration 2326, loss = 0.08964765\n",
      "Iteration 2327, loss = 0.08881815\n",
      "Iteration 2328, loss = 0.08913784\n",
      "Iteration 2329, loss = 0.08755011\n",
      "Iteration 2330, loss = 0.08858392\n",
      "Iteration 2331, loss = 0.08776882\n",
      "Iteration 2332, loss = 0.08766102\n",
      "Iteration 2333, loss = 0.08781218\n",
      "Iteration 2334, loss = 0.08721260\n",
      "Iteration 2335, loss = 0.08699442\n",
      "Iteration 2336, loss = 0.08710040\n",
      "Iteration 2337, loss = 0.08664894\n",
      "Iteration 2338, loss = 0.08673052\n",
      "Iteration 2339, loss = 0.08640670\n",
      "Iteration 2340, loss = 0.08600120\n",
      "Iteration 2341, loss = 0.08622622\n",
      "Iteration 2342, loss = 0.08608211\n",
      "Iteration 2343, loss = 0.08591822\n",
      "Iteration 2344, loss = 0.08573111\n",
      "Iteration 2345, loss = 0.08551131\n",
      "Iteration 2346, loss = 0.08550827\n",
      "Iteration 2347, loss = 0.08536476\n",
      "Iteration 2348, loss = 0.08525963\n",
      "Iteration 2349, loss = 0.08505624\n",
      "Iteration 2350, loss = 0.08488340\n",
      "Iteration 2351, loss = 0.08482445\n",
      "Iteration 2352, loss = 0.08475176\n",
      "Iteration 2353, loss = 0.08459392\n",
      "Iteration 2354, loss = 0.08447709\n",
      "Iteration 2355, loss = 0.08427073\n",
      "Iteration 2356, loss = 0.08411860\n",
      "Iteration 2357, loss = 0.08401606\n",
      "Iteration 2358, loss = 0.08381387\n",
      "Iteration 2359, loss = 0.08361917\n",
      "Iteration 2360, loss = 0.08336833\n",
      "Iteration 2361, loss = 0.08314519\n",
      "Iteration 2362, loss = 0.08293175\n",
      "Iteration 2363, loss = 0.08280641\n",
      "Iteration 2364, loss = 0.08269589\n",
      "Iteration 2365, loss = 0.08252247\n",
      "Iteration 2366, loss = 0.08246288\n",
      "Iteration 2367, loss = 0.08235864\n",
      "Iteration 2368, loss = 0.08217409\n",
      "Iteration 2369, loss = 0.08197973\n",
      "Iteration 2370, loss = 0.08188144\n",
      "Iteration 2371, loss = 0.08179605\n",
      "Iteration 2372, loss = 0.08166346\n",
      "Iteration 2373, loss = 0.08157113\n",
      "Iteration 2374, loss = 0.08150265\n",
      "Iteration 2375, loss = 0.08138254\n",
      "Iteration 2376, loss = 0.08126090\n",
      "Iteration 2377, loss = 0.08116328\n",
      "Iteration 2378, loss = 0.08106853\n",
      "Iteration 2379, loss = 0.08095824\n",
      "Iteration 2380, loss = 0.08086052\n",
      "Iteration 2381, loss = 0.08077949\n",
      "Iteration 2382, loss = 0.08066407\n",
      "Iteration 2383, loss = 0.08056591\n",
      "Iteration 2384, loss = 0.08048567\n",
      "Iteration 2385, loss = 0.08039576\n",
      "Iteration 2386, loss = 0.08029872\n",
      "Iteration 2387, loss = 0.08020507\n",
      "Iteration 2388, loss = 0.08012126\n",
      "Iteration 2389, loss = 0.08002375\n",
      "Iteration 2390, loss = 0.07994176\n",
      "Iteration 2391, loss = 0.07986241\n",
      "Iteration 2392, loss = 0.07977465\n",
      "Iteration 2393, loss = 0.07968846\n",
      "Iteration 2394, loss = 0.07960694\n",
      "Iteration 2395, loss = 0.07952581\n",
      "Iteration 2396, loss = 0.07944451\n",
      "Iteration 2397, loss = 0.07937259\n",
      "Iteration 2398, loss = 0.07929667\n",
      "Iteration 2399, loss = 0.07922082\n",
      "Iteration 2400, loss = 0.07914557\n",
      "Iteration 2401, loss = 0.07907068\n",
      "Iteration 2402, loss = 0.07899376\n",
      "Iteration 2403, loss = 0.07892069\n",
      "Iteration 2404, loss = 0.07884750\n",
      "Iteration 2405, loss = 0.07877223\n",
      "Iteration 2406, loss = 0.07869694\n",
      "Iteration 2407, loss = 0.07862165\n",
      "Iteration 2408, loss = 0.07854529\n",
      "Iteration 2409, loss = 0.07846989\n",
      "Iteration 2410, loss = 0.07839509\n",
      "Iteration 2411, loss = 0.07831927\n",
      "Iteration 2412, loss = 0.07824361\n",
      "Iteration 2413, loss = 0.07816811\n",
      "Iteration 2414, loss = 0.07809254\n",
      "Iteration 2415, loss = 0.07801691\n",
      "Iteration 2416, loss = 0.07794147\n",
      "Iteration 2417, loss = 0.07786548\n",
      "Iteration 2418, loss = 0.07778947\n",
      "Iteration 2419, loss = 0.07771356\n",
      "Iteration 2420, loss = 0.07763751\n",
      "Iteration 2421, loss = 0.07756116\n",
      "Iteration 2422, loss = 0.07748444\n",
      "Iteration 2423, loss = 0.07740772\n",
      "Iteration 2424, loss = 0.07733066\n",
      "Iteration 2425, loss = 0.07725367\n",
      "Iteration 2426, loss = 0.07717659\n",
      "Iteration 2427, loss = 0.07709934\n",
      "Iteration 2428, loss = 0.07702176\n",
      "Iteration 2429, loss = 0.07694440\n",
      "Iteration 2430, loss = 0.07686702\n",
      "Iteration 2431, loss = 0.07678986\n",
      "Iteration 2432, loss = 0.07671312\n",
      "Iteration 2433, loss = 0.07663655\n",
      "Iteration 2434, loss = 0.07656041\n",
      "Iteration 2435, loss = 0.07648484\n",
      "Iteration 2436, loss = 0.07640987\n",
      "Iteration 2437, loss = 0.07633560\n",
      "Iteration 2438, loss = 0.07626225\n",
      "Iteration 2439, loss = 0.07618973\n",
      "Iteration 2440, loss = 0.07611824\n",
      "Iteration 2441, loss = 0.07604794\n",
      "Iteration 2442, loss = 0.07597879\n",
      "Iteration 2443, loss = 0.07591095\n",
      "Iteration 2444, loss = 0.07584452\n",
      "Iteration 2445, loss = 0.07577977\n",
      "Iteration 2446, loss = 0.07571773\n",
      "Iteration 2447, loss = 0.07566338\n",
      "Iteration 2448, loss = 0.07563279\n",
      "Iteration 2449, loss = 0.07575339\n",
      "Iteration 2450, loss = 0.07580808\n",
      "Iteration 2451, loss = 0.07680619\n",
      "Iteration 2452, loss = 0.07582163\n",
      "Iteration 2453, loss = 0.07821170\n",
      "Iteration 2454, loss = 0.08371496\n",
      "Iteration 2455, loss = 0.08310094\n",
      "Iteration 2456, loss = 0.08329049\n",
      "Iteration 2457, loss = 0.08233403\n",
      "Iteration 2458, loss = 0.08222219\n",
      "Iteration 2459, loss = 0.08135884\n",
      "Iteration 2460, loss = 0.08083341\n",
      "Iteration 2461, loss = 0.08019689\n",
      "Iteration 2462, loss = 0.08027955\n",
      "Iteration 2463, loss = 0.08083953\n",
      "Iteration 2464, loss = 0.07988292\n",
      "Iteration 2465, loss = 0.07962102\n",
      "Iteration 2466, loss = 0.07967246\n",
      "Iteration 2467, loss = 0.07893415\n",
      "Iteration 2468, loss = 0.07916715\n",
      "Iteration 2469, loss = 0.07854375\n",
      "Iteration 2470, loss = 0.07903406\n",
      "Iteration 2471, loss = 0.07899175\n",
      "Iteration 2472, loss = 0.07828371\n",
      "Iteration 2473, loss = 0.07811144\n",
      "Iteration 2474, loss = 0.07788610\n",
      "Iteration 2475, loss = 0.07797688\n",
      "Iteration 2476, loss = 0.07783869\n",
      "Iteration 2477, loss = 0.07775244\n",
      "Iteration 2478, loss = 0.07758756\n",
      "Iteration 2479, loss = 0.07741886\n",
      "Iteration 2480, loss = 0.07733931\n",
      "Iteration 2481, loss = 0.07717808\n",
      "Iteration 2482, loss = 0.07726175\n",
      "Iteration 2483, loss = 0.07718733\n",
      "Iteration 2484, loss = 0.07696228\n",
      "Iteration 2485, loss = 0.07681710\n",
      "Iteration 2486, loss = 0.07671842\n",
      "Iteration 2487, loss = 0.07671245\n",
      "Iteration 2488, loss = 0.07661381\n",
      "Iteration 2489, loss = 0.07650043\n",
      "Iteration 2490, loss = 0.07640706\n",
      "Iteration 2491, loss = 0.07630885\n",
      "Iteration 2492, loss = 0.07621903\n",
      "Iteration 2493, loss = 0.07612720\n",
      "Iteration 2494, loss = 0.07608375\n",
      "Iteration 2495, loss = 0.07597387\n",
      "Iteration 2496, loss = 0.07584358\n",
      "Iteration 2497, loss = 0.07574996\n",
      "Iteration 2498, loss = 0.07566520\n",
      "Iteration 2499, loss = 0.07558984\n",
      "Iteration 2500, loss = 0.07547076\n",
      "Iteration 2501, loss = 0.07534833\n",
      "Iteration 2502, loss = 0.07522706\n",
      "Iteration 2503, loss = 0.07509759\n",
      "Iteration 2504, loss = 0.07496306\n",
      "Iteration 2505, loss = 0.07481925\n",
      "Iteration 2506, loss = 0.07466634\n",
      "Iteration 2507, loss = 0.07446855\n",
      "Iteration 2508, loss = 0.07424558\n",
      "Iteration 2509, loss = 0.07400370\n",
      "Iteration 2510, loss = 0.07373644\n",
      "Iteration 2511, loss = 0.07345517\n",
      "Iteration 2512, loss = 0.07324695\n",
      "Iteration 2513, loss = 0.07330376\n",
      "Iteration 2514, loss = 0.07329742\n",
      "Iteration 2515, loss = 0.07325294\n",
      "Iteration 2516, loss = 0.07322934\n",
      "Iteration 2517, loss = 0.07301699\n",
      "Iteration 2518, loss = 0.07282348\n",
      "Iteration 2519, loss = 0.07270452\n",
      "Iteration 2520, loss = 0.07267997\n",
      "Iteration 2521, loss = 0.07261866\n",
      "Iteration 2522, loss = 0.07252135\n",
      "Iteration 2523, loss = 0.07249819\n",
      "Iteration 2524, loss = 0.07232732\n",
      "Iteration 2525, loss = 0.07227589\n",
      "Iteration 2526, loss = 0.07221133\n",
      "Iteration 2527, loss = 0.07215236\n",
      "Iteration 2528, loss = 0.07209005\n",
      "Iteration 2529, loss = 0.07201569\n",
      "Iteration 2530, loss = 0.07197390\n",
      "Iteration 2531, loss = 0.07189082\n",
      "Iteration 2532, loss = 0.07184734\n",
      "Iteration 2533, loss = 0.07180252\n",
      "Iteration 2534, loss = 0.07171843\n",
      "Iteration 2535, loss = 0.07168118\n",
      "Iteration 2536, loss = 0.07163135\n",
      "Iteration 2537, loss = 0.07158431\n",
      "Iteration 2538, loss = 0.07153358\n",
      "Iteration 2539, loss = 0.07148658\n",
      "Iteration 2540, loss = 0.07143026\n",
      "Iteration 2541, loss = 0.07136907\n",
      "Iteration 2542, loss = 0.07133484\n",
      "Iteration 2543, loss = 0.07128392\n",
      "Iteration 2544, loss = 0.07124008\n",
      "Iteration 2545, loss = 0.07119843\n",
      "Iteration 2546, loss = 0.07114461\n",
      "Iteration 2547, loss = 0.07109523\n",
      "Iteration 2548, loss = 0.07104737\n",
      "Iteration 2549, loss = 0.07100504\n",
      "Iteration 2550, loss = 0.07095894\n",
      "Iteration 2551, loss = 0.07091537\n",
      "Iteration 2552, loss = 0.07087047\n",
      "Iteration 2553, loss = 0.07082044\n",
      "Iteration 2554, loss = 0.07077471\n",
      "Iteration 2555, loss = 0.07072866\n",
      "Iteration 2556, loss = 0.07068459\n",
      "Iteration 2557, loss = 0.07063963\n",
      "Iteration 2558, loss = 0.07059442\n",
      "Iteration 2559, loss = 0.07054983\n",
      "Iteration 2560, loss = 0.07050246\n",
      "Iteration 2561, loss = 0.07045814\n",
      "Iteration 2562, loss = 0.07041410\n",
      "Iteration 2563, loss = 0.07036772\n",
      "Iteration 2564, loss = 0.07032348\n",
      "Iteration 2565, loss = 0.07027826\n",
      "Iteration 2566, loss = 0.07023190\n",
      "Iteration 2567, loss = 0.07018757\n",
      "Iteration 2568, loss = 0.07014227\n",
      "Iteration 2569, loss = 0.07009710\n",
      "Iteration 2570, loss = 0.07005231\n",
      "Iteration 2571, loss = 0.07000739\n",
      "Iteration 2572, loss = 0.06996299\n",
      "Iteration 2573, loss = 0.06991834\n",
      "Iteration 2574, loss = 0.06987456\n",
      "Iteration 2575, loss = 0.06983061\n",
      "Iteration 2576, loss = 0.06978679\n",
      "Iteration 2577, loss = 0.06974398\n",
      "Iteration 2578, loss = 0.06970075\n",
      "Iteration 2579, loss = 0.06965800\n",
      "Iteration 2580, loss = 0.06961535\n",
      "Iteration 2581, loss = 0.06957239\n",
      "Iteration 2582, loss = 0.06952987\n",
      "Iteration 2583, loss = 0.06948732\n",
      "Iteration 2584, loss = 0.06944489\n",
      "Iteration 2585, loss = 0.06940256\n",
      "Iteration 2586, loss = 0.06936006\n",
      "Iteration 2587, loss = 0.06931769\n",
      "Iteration 2588, loss = 0.06927523\n",
      "Iteration 2589, loss = 0.06923296\n",
      "Iteration 2590, loss = 0.06919077\n",
      "Iteration 2591, loss = 0.06914850\n",
      "Iteration 2592, loss = 0.06910642\n",
      "Iteration 2593, loss = 0.06906424\n",
      "Iteration 2594, loss = 0.06902217\n",
      "Iteration 2595, loss = 0.06898022\n",
      "Iteration 2596, loss = 0.06893829\n",
      "Iteration 2597, loss = 0.06889643\n",
      "Iteration 2598, loss = 0.06885462\n",
      "Iteration 2599, loss = 0.06881285\n",
      "Iteration 2600, loss = 0.06877111\n",
      "Iteration 2601, loss = 0.06872948\n",
      "Iteration 2602, loss = 0.06868787\n",
      "Iteration 2603, loss = 0.06864630\n",
      "Iteration 2604, loss = 0.06860480\n",
      "Iteration 2605, loss = 0.06856330\n",
      "Iteration 2606, loss = 0.06852187\n",
      "Iteration 2607, loss = 0.06848047\n",
      "Iteration 2608, loss = 0.06843910\n",
      "Iteration 2609, loss = 0.06839779\n",
      "Iteration 2610, loss = 0.06835651\n",
      "Iteration 2611, loss = 0.06831527\n",
      "Iteration 2612, loss = 0.06827407\n",
      "Iteration 2613, loss = 0.06823290\n",
      "Iteration 2614, loss = 0.06819178\n",
      "Iteration 2615, loss = 0.06815070\n",
      "Iteration 2616, loss = 0.06810966\n",
      "Iteration 2617, loss = 0.06806865\n",
      "Iteration 2618, loss = 0.06802769\n",
      "Iteration 2619, loss = 0.06798677\n",
      "Iteration 2620, loss = 0.06794588\n",
      "Iteration 2621, loss = 0.06790503\n",
      "Iteration 2622, loss = 0.06786422\n",
      "Iteration 2623, loss = 0.06782345\n",
      "Iteration 2624, loss = 0.06778271\n",
      "Iteration 2625, loss = 0.06774201\n",
      "Iteration 2626, loss = 0.06770134\n",
      "Iteration 2627, loss = 0.06766071\n",
      "Iteration 2628, loss = 0.06762012\n",
      "Iteration 2629, loss = 0.06757956\n",
      "Iteration 2630, loss = 0.06753904\n",
      "Iteration 2631, loss = 0.06749855\n",
      "Iteration 2632, loss = 0.06745809\n",
      "Iteration 2633, loss = 0.06741767\n",
      "Iteration 2634, loss = 0.06737728\n",
      "Iteration 2635, loss = 0.06733693\n",
      "Iteration 2636, loss = 0.06729661\n",
      "Iteration 2637, loss = 0.06725633\n",
      "Iteration 2638, loss = 0.06721608\n",
      "Iteration 2639, loss = 0.06717586\n",
      "Iteration 2640, loss = 0.06713568\n",
      "Iteration 2641, loss = 0.06709553\n",
      "Iteration 2642, loss = 0.06705541\n",
      "Iteration 2643, loss = 0.06701533\n",
      "Iteration 2644, loss = 0.06697528\n",
      "Iteration 2645, loss = 0.06693526\n",
      "Iteration 2646, loss = 0.06689527\n",
      "Iteration 2647, loss = 0.06685532\n",
      "Iteration 2648, loss = 0.06681540\n",
      "Iteration 2649, loss = 0.06677551\n",
      "Iteration 2650, loss = 0.06673566\n",
      "Iteration 2651, loss = 0.06669583\n",
      "Iteration 2652, loss = 0.06665604\n",
      "Iteration 2653, loss = 0.06661628\n",
      "Iteration 2654, loss = 0.06657655\n",
      "Iteration 2655, loss = 0.06653686\n",
      "Iteration 2656, loss = 0.06649719\n",
      "Iteration 2657, loss = 0.06645756\n",
      "Iteration 2658, loss = 0.06641795\n",
      "Iteration 2659, loss = 0.06637838\n",
      "Iteration 2660, loss = 0.06633884\n",
      "Iteration 2661, loss = 0.06629933\n",
      "Iteration 2662, loss = 0.06625986\n",
      "Iteration 2663, loss = 0.06622041\n",
      "Iteration 2664, loss = 0.06618099\n",
      "Iteration 2665, loss = 0.06614161\n",
      "Iteration 2666, loss = 0.06610225\n",
      "Iteration 2667, loss = 0.06606293\n",
      "Iteration 2668, loss = 0.06602363\n",
      "Iteration 2669, loss = 0.06598437\n",
      "Iteration 2670, loss = 0.06594513\n",
      "Iteration 2671, loss = 0.06590593\n",
      "Iteration 2672, loss = 0.06586675\n",
      "Iteration 2673, loss = 0.06582761\n",
      "Iteration 2674, loss = 0.06578849\n",
      "Iteration 2675, loss = 0.06574941\n",
      "Iteration 2676, loss = 0.06571035\n",
      "Iteration 2677, loss = 0.06567133\n",
      "Iteration 2678, loss = 0.06563233\n",
      "Iteration 2679, loss = 0.06559336\n",
      "Iteration 2680, loss = 0.06555443\n",
      "Iteration 2681, loss = 0.06551552\n",
      "Iteration 2682, loss = 0.06547664\n",
      "Iteration 2683, loss = 0.06543779\n",
      "Iteration 2684, loss = 0.06539896\n",
      "Iteration 2685, loss = 0.06536017\n",
      "Iteration 2686, loss = 0.06532141\n",
      "Iteration 2687, loss = 0.06528267\n",
      "Iteration 2688, loss = 0.06524396\n",
      "Iteration 2689, loss = 0.06520529\n",
      "Iteration 2690, loss = 0.06516664\n",
      "Iteration 2691, loss = 0.06512801\n",
      "Iteration 2692, loss = 0.06508942\n",
      "Iteration 2693, loss = 0.06505085\n",
      "Iteration 2694, loss = 0.06501231\n",
      "Iteration 2695, loss = 0.06497380\n",
      "Iteration 2696, loss = 0.06493532\n",
      "Iteration 2697, loss = 0.06489687\n",
      "Iteration 2698, loss = 0.06485844\n",
      "Iteration 2699, loss = 0.06482004\n",
      "Iteration 2700, loss = 0.06478167\n",
      "Iteration 2701, loss = 0.06474332\n",
      "Iteration 2702, loss = 0.06470501\n",
      "Iteration 2703, loss = 0.06466671\n",
      "Iteration 2704, loss = 0.06462845\n",
      "Iteration 2705, loss = 0.06459021\n",
      "Iteration 2706, loss = 0.06455200\n",
      "Iteration 2707, loss = 0.06451382\n",
      "Iteration 2708, loss = 0.06447566\n",
      "Iteration 2709, loss = 0.06443753\n",
      "Iteration 2710, loss = 0.06439943\n",
      "Iteration 2711, loss = 0.06436135\n",
      "Iteration 2712, loss = 0.06432330\n",
      "Iteration 2713, loss = 0.06428527\n",
      "Iteration 2714, loss = 0.06424727\n",
      "Iteration 2715, loss = 0.06420930\n",
      "Iteration 2716, loss = 0.06417135\n",
      "Iteration 2717, loss = 0.06413343\n",
      "Iteration 2718, loss = 0.06409553\n",
      "Iteration 2719, loss = 0.06405766\n",
      "Iteration 2720, loss = 0.06401981\n",
      "Iteration 2721, loss = 0.06398199\n",
      "Iteration 2722, loss = 0.06394419\n",
      "Iteration 2723, loss = 0.06390642\n",
      "Iteration 2724, loss = 0.06386867\n",
      "Iteration 2725, loss = 0.06383095\n",
      "Iteration 2726, loss = 0.06379325\n",
      "Iteration 2727, loss = 0.06375558\n",
      "Iteration 2728, loss = 0.06371793\n",
      "Iteration 2729, loss = 0.06368031\n",
      "Iteration 2730, loss = 0.06364270\n",
      "Iteration 2731, loss = 0.06360513\n",
      "Iteration 2732, loss = 0.06356757\n",
      "Iteration 2733, loss = 0.06353004\n",
      "Iteration 2734, loss = 0.06349254\n",
      "Iteration 2735, loss = 0.06345505\n",
      "Iteration 2736, loss = 0.06341759\n",
      "Iteration 2737, loss = 0.06338015\n",
      "Iteration 2738, loss = 0.06334274\n",
      "Iteration 2739, loss = 0.06330534\n",
      "Iteration 2740, loss = 0.06326798\n",
      "Iteration 2741, loss = 0.06323063\n",
      "Iteration 2742, loss = 0.06319330\n",
      "Iteration 2743, loss = 0.06315600\n",
      "Iteration 2744, loss = 0.06311872\n",
      "Iteration 2745, loss = 0.06308146\n",
      "Iteration 2746, loss = 0.06304422\n",
      "Iteration 2747, loss = 0.06300700\n",
      "Iteration 2748, loss = 0.06296981\n",
      "Iteration 2749, loss = 0.06293263\n",
      "Iteration 2750, loss = 0.06289548\n",
      "Iteration 2751, loss = 0.06285834\n",
      "Iteration 2752, loss = 0.06282123\n",
      "Iteration 2753, loss = 0.06278414\n",
      "Iteration 2754, loss = 0.06274706\n",
      "Iteration 2755, loss = 0.06271001\n",
      "Iteration 2756, loss = 0.06267298\n",
      "Iteration 2757, loss = 0.06263596\n",
      "Iteration 2758, loss = 0.06259897\n",
      "Iteration 2759, loss = 0.06256200\n",
      "Iteration 2760, loss = 0.06252504\n",
      "Iteration 2761, loss = 0.06248810\n",
      "Iteration 2762, loss = 0.06245118\n",
      "Iteration 2763, loss = 0.06241428\n",
      "Iteration 2764, loss = 0.06237740\n",
      "Iteration 2765, loss = 0.06234053\n",
      "Iteration 2766, loss = 0.06230369\n",
      "Iteration 2767, loss = 0.06226686\n",
      "Iteration 2768, loss = 0.06223004\n",
      "Iteration 2769, loss = 0.06219325\n",
      "Iteration 2770, loss = 0.06215647\n",
      "Iteration 2771, loss = 0.06211972\n",
      "Iteration 2772, loss = 0.06208300\n",
      "Iteration 2773, loss = 0.06204636\n",
      "Iteration 2774, loss = 0.06200986\n",
      "Iteration 2775, loss = 0.06197393\n",
      "Iteration 2776, loss = 0.06193864\n",
      "Iteration 2777, loss = 0.06190833\n",
      "Iteration 2778, loss = 0.06187164\n",
      "Iteration 2779, loss = 0.06184936\n",
      "Iteration 2780, loss = 0.06179700\n",
      "Iteration 2781, loss = 0.06175688\n",
      "Iteration 2782, loss = 0.06172448\n",
      "Iteration 2783, loss = 0.06169325\n",
      "Iteration 2784, loss = 0.06168388\n",
      "Iteration 2785, loss = 0.06162960\n",
      "Iteration 2786, loss = 0.06158402\n",
      "Iteration 2787, loss = 0.06158275\n",
      "Iteration 2788, loss = 0.06154218\n",
      "Iteration 2789, loss = 0.06160487\n",
      "Iteration 2790, loss = 0.06170588\n",
      "Iteration 2791, loss = 0.07363607\n",
      "Iteration 2792, loss = 0.09492106\n",
      "Iteration 2793, loss = 0.09433415\n",
      "Iteration 2794, loss = 0.08714614\n",
      "Iteration 2795, loss = 0.07432228\n",
      "Iteration 2796, loss = 0.06973391\n",
      "Iteration 2797, loss = 0.07218321\n",
      "Iteration 2798, loss = 0.07161379\n",
      "Iteration 2799, loss = 0.07151102\n",
      "Iteration 2800, loss = 0.07139573\n",
      "Iteration 2801, loss = 0.07113644\n",
      "Iteration 2802, loss = 0.07058907\n",
      "Iteration 2803, loss = 0.07034549\n",
      "Iteration 2804, loss = 0.06889066\n",
      "Iteration 2805, loss = 0.06728984\n",
      "Iteration 2806, loss = 0.06722853\n",
      "Iteration 2807, loss = 0.06724358\n",
      "Iteration 2808, loss = 0.06765619\n",
      "Iteration 2809, loss = 0.06720172\n",
      "Iteration 2810, loss = 0.06648206\n",
      "Iteration 2811, loss = 0.06630103\n",
      "Iteration 2812, loss = 0.06526566\n",
      "Iteration 2813, loss = 0.06446270\n",
      "Iteration 2814, loss = 0.06444797\n",
      "Iteration 2815, loss = 0.06421523\n",
      "Iteration 2816, loss = 0.06417378\n",
      "Iteration 2817, loss = 0.06407931\n",
      "Iteration 2818, loss = 0.06396253\n",
      "Iteration 2819, loss = 0.06380611\n",
      "Iteration 2820, loss = 0.06321142\n",
      "Iteration 2821, loss = 0.06311768\n",
      "Iteration 2822, loss = 0.06280149\n",
      "Iteration 2823, loss = 0.06281052\n",
      "Iteration 2824, loss = 0.06269429\n",
      "Iteration 2825, loss = 0.06251176\n",
      "Iteration 2826, loss = 0.06247417\n",
      "Iteration 2827, loss = 0.06231836\n",
      "Iteration 2828, loss = 0.06225046\n",
      "Iteration 2829, loss = 0.06201402\n",
      "Iteration 2830, loss = 0.06197828\n",
      "Iteration 2831, loss = 0.06182165\n",
      "Iteration 2832, loss = 0.06168097\n",
      "Iteration 2833, loss = 0.06162501\n",
      "Iteration 2834, loss = 0.06151575\n",
      "Iteration 2835, loss = 0.06150530\n",
      "Iteration 2836, loss = 0.06137454\n",
      "Iteration 2837, loss = 0.06131616\n",
      "Iteration 2838, loss = 0.06123675\n",
      "Iteration 2839, loss = 0.06116057\n",
      "Iteration 2840, loss = 0.06109046\n",
      "Iteration 2841, loss = 0.06100830\n",
      "Iteration 2842, loss = 0.06096042\n",
      "Iteration 2843, loss = 0.06086355\n",
      "Iteration 2844, loss = 0.06080061\n",
      "Iteration 2845, loss = 0.06073538\n",
      "Iteration 2846, loss = 0.06068659\n",
      "Iteration 2847, loss = 0.06063404\n",
      "Iteration 2848, loss = 0.06056906\n",
      "Iteration 2849, loss = 0.06052183\n",
      "Iteration 2850, loss = 0.06044559\n",
      "Iteration 2851, loss = 0.06039357\n",
      "Iteration 2852, loss = 0.06033573\n",
      "Iteration 2853, loss = 0.06028699\n",
      "Iteration 2854, loss = 0.06024245\n",
      "Iteration 2855, loss = 0.06019074\n",
      "Iteration 2856, loss = 0.06014665\n",
      "Iteration 2857, loss = 0.06009666\n",
      "Iteration 2858, loss = 0.06004922\n",
      "Iteration 2859, loss = 0.06000009\n",
      "Iteration 2860, loss = 0.05994901\n",
      "Iteration 2861, loss = 0.05990443\n",
      "Iteration 2862, loss = 0.05985369\n",
      "Iteration 2863, loss = 0.05981121\n",
      "Iteration 2864, loss = 0.05976658\n",
      "Iteration 2865, loss = 0.05972246\n",
      "Iteration 2866, loss = 0.05967857\n",
      "Iteration 2867, loss = 0.05963207\n",
      "Iteration 2868, loss = 0.05958865\n",
      "Iteration 2869, loss = 0.05954347\n",
      "Iteration 2870, loss = 0.05950032\n",
      "Iteration 2871, loss = 0.05945741\n",
      "Iteration 2872, loss = 0.05941377\n",
      "Iteration 2873, loss = 0.05937147\n",
      "Iteration 2874, loss = 0.05932803\n",
      "Iteration 2875, loss = 0.05928635\n",
      "Iteration 2876, loss = 0.05924468\n",
      "Iteration 2877, loss = 0.05920237\n",
      "Iteration 2878, loss = 0.05916106\n",
      "Iteration 2879, loss = 0.05911861\n",
      "Iteration 2880, loss = 0.05907701\n",
      "Iteration 2881, loss = 0.05903520\n",
      "Iteration 2882, loss = 0.05899407\n",
      "Iteration 2883, loss = 0.05895319\n",
      "Iteration 2884, loss = 0.05891208\n",
      "Iteration 2885, loss = 0.05887161\n",
      "Iteration 2886, loss = 0.05883088\n",
      "Iteration 2887, loss = 0.05879021\n",
      "Iteration 2888, loss = 0.05874980\n",
      "Iteration 2889, loss = 0.05870912\n",
      "Iteration 2890, loss = 0.05866877\n",
      "Iteration 2891, loss = 0.05862835\n",
      "Iteration 2892, loss = 0.05858819\n",
      "Iteration 2893, loss = 0.05854825\n",
      "Iteration 2894, loss = 0.05850820\n",
      "Iteration 2895, loss = 0.05846839\n",
      "Iteration 2896, loss = 0.05842843\n",
      "Iteration 2897, loss = 0.05838858\n",
      "Iteration 2898, loss = 0.05834878\n",
      "Iteration 2899, loss = 0.05830902\n",
      "Iteration 2900, loss = 0.05826941\n",
      "Iteration 2901, loss = 0.05822973\n",
      "Iteration 2902, loss = 0.05819015\n",
      "Iteration 2903, loss = 0.05815064\n",
      "Iteration 2904, loss = 0.05811109\n",
      "Iteration 2905, loss = 0.05807167\n",
      "Iteration 2906, loss = 0.05803225\n",
      "Iteration 2907, loss = 0.05799285\n",
      "Iteration 2908, loss = 0.05795348\n",
      "Iteration 2909, loss = 0.05791411\n",
      "Iteration 2910, loss = 0.05787480\n",
      "Iteration 2911, loss = 0.05783548\n",
      "Iteration 2912, loss = 0.05779620\n",
      "Iteration 2913, loss = 0.05775695\n",
      "Iteration 2914, loss = 0.05771769\n",
      "Iteration 2915, loss = 0.05767846\n",
      "Iteration 2916, loss = 0.05763923\n",
      "Iteration 2917, loss = 0.05760000\n",
      "Iteration 2918, loss = 0.05756078\n",
      "Iteration 2919, loss = 0.05752155\n",
      "Iteration 2920, loss = 0.05748233\n",
      "Iteration 2921, loss = 0.05744311\n",
      "Iteration 2922, loss = 0.05740388\n",
      "Iteration 2923, loss = 0.05736466\n",
      "Iteration 2924, loss = 0.05732542\n",
      "Iteration 2925, loss = 0.05728617\n",
      "Iteration 2926, loss = 0.05724690\n",
      "Iteration 2927, loss = 0.05720762\n",
      "Iteration 2928, loss = 0.05716833\n",
      "Iteration 2929, loss = 0.05712901\n",
      "Iteration 2930, loss = 0.05708968\n",
      "Iteration 2931, loss = 0.05705032\n",
      "Iteration 2932, loss = 0.05701093\n",
      "Iteration 2933, loss = 0.05697152\n",
      "Iteration 2934, loss = 0.05693208\n",
      "Iteration 2935, loss = 0.05689260\n",
      "Iteration 2936, loss = 0.05685310\n",
      "Iteration 2937, loss = 0.05681355\n",
      "Iteration 2938, loss = 0.05677397\n",
      "Iteration 2939, loss = 0.05673434\n",
      "Iteration 2940, loss = 0.05669467\n",
      "Iteration 2941, loss = 0.05665496\n",
      "Iteration 2942, loss = 0.05661520\n",
      "Iteration 2943, loss = 0.05657539\n",
      "Iteration 2944, loss = 0.05653553\n",
      "Iteration 2945, loss = 0.05649561\n",
      "Iteration 2946, loss = 0.05645564\n",
      "Iteration 2947, loss = 0.05641561\n",
      "Iteration 2948, loss = 0.05637551\n",
      "Iteration 2949, loss = 0.05633535\n",
      "Iteration 2950, loss = 0.05629513\n",
      "Iteration 2951, loss = 0.05625483\n",
      "Iteration 2952, loss = 0.05621447\n",
      "Iteration 2953, loss = 0.05617402\n",
      "Iteration 2954, loss = 0.05613351\n",
      "Iteration 2955, loss = 0.05609291\n",
      "Iteration 2956, loss = 0.05605223\n",
      "Iteration 2957, loss = 0.05601146\n",
      "Iteration 2958, loss = 0.05597060\n",
      "Iteration 2959, loss = 0.05592965\n",
      "Iteration 2960, loss = 0.05588861\n",
      "Iteration 2961, loss = 0.05584747\n",
      "Iteration 2962, loss = 0.05580623\n",
      "Iteration 2963, loss = 0.05576488\n",
      "Iteration 2964, loss = 0.05572343\n",
      "Iteration 2965, loss = 0.05568187\n",
      "Iteration 2966, loss = 0.05564018\n",
      "Iteration 2967, loss = 0.05559839\n",
      "Iteration 2968, loss = 0.05555646\n",
      "Iteration 2969, loss = 0.05551442\n",
      "Iteration 2970, loss = 0.05547224\n",
      "Iteration 2971, loss = 0.05542992\n",
      "Iteration 2972, loss = 0.05538747\n",
      "Iteration 2973, loss = 0.05534487\n",
      "Iteration 2974, loss = 0.05530213\n",
      "Iteration 2975, loss = 0.05525923\n",
      "Iteration 2976, loss = 0.05521617\n",
      "Iteration 2977, loss = 0.05517295\n",
      "Iteration 2978, loss = 0.05512956\n",
      "Iteration 2979, loss = 0.05508600\n",
      "Iteration 2980, loss = 0.05504225\n",
      "Iteration 2981, loss = 0.05499832\n",
      "Iteration 2982, loss = 0.05495419\n",
      "Iteration 2983, loss = 0.05490987\n",
      "Iteration 2984, loss = 0.05486534\n",
      "Iteration 2985, loss = 0.05482059\n",
      "Iteration 2986, loss = 0.05477562\n",
      "Iteration 2987, loss = 0.05473042\n",
      "Iteration 2988, loss = 0.05468498\n",
      "Iteration 2989, loss = 0.05463929\n",
      "Iteration 2990, loss = 0.05459334\n",
      "Iteration 2991, loss = 0.05454713\n",
      "Iteration 2992, loss = 0.05450063\n",
      "Iteration 2993, loss = 0.05445385\n",
      "Iteration 2994, loss = 0.05440677\n",
      "Iteration 2995, loss = 0.05435938\n",
      "Iteration 2996, loss = 0.05431166\n",
      "Iteration 2997, loss = 0.05426360\n",
      "Iteration 2998, loss = 0.05421518\n",
      "Iteration 2999, loss = 0.05416640\n",
      "Iteration 3000, loss = 0.05411722\n",
      "Iteration 3001, loss = 0.05406765\n",
      "Iteration 3002, loss = 0.05401765\n",
      "Iteration 3003, loss = 0.05396721\n",
      "Iteration 3004, loss = 0.05391630\n",
      "Iteration 3005, loss = 0.05386491\n",
      "Iteration 3006, loss = 0.05381301\n",
      "Iteration 3007, loss = 0.05376057\n",
      "Iteration 3008, loss = 0.05370757\n",
      "Iteration 3009, loss = 0.05365398\n",
      "Iteration 3010, loss = 0.05359976\n",
      "Iteration 3011, loss = 0.05354488\n",
      "Iteration 3012, loss = 0.05348931\n",
      "Iteration 3013, loss = 0.05343301\n",
      "Iteration 3014, loss = 0.05337593\n",
      "Iteration 3015, loss = 0.05331803\n",
      "Iteration 3016, loss = 0.05325927\n",
      "Iteration 3017, loss = 0.05319959\n",
      "Iteration 3018, loss = 0.05313893\n",
      "Iteration 3019, loss = 0.05307724\n",
      "Iteration 3020, loss = 0.05301445\n",
      "Iteration 3021, loss = 0.05295050\n",
      "Iteration 3022, loss = 0.05288531\n",
      "Iteration 3023, loss = 0.05281881\n",
      "Iteration 3024, loss = 0.05275091\n",
      "Iteration 3025, loss = 0.05268153\n",
      "Iteration 3026, loss = 0.05261058\n",
      "Iteration 3027, loss = 0.05253796\n",
      "Iteration 3028, loss = 0.05246357\n",
      "Iteration 3029, loss = 0.05238732\n",
      "Iteration 3030, loss = 0.05230911\n",
      "Iteration 3031, loss = 0.05222883\n",
      "Iteration 3032, loss = 0.05214639\n",
      "Iteration 3033, loss = 0.05206170\n",
      "Iteration 3034, loss = 0.05197470\n",
      "Iteration 3035, loss = 0.05188533\n",
      "Iteration 3036, loss = 0.05179355\n",
      "Iteration 3037, loss = 0.05169937\n",
      "Iteration 3038, loss = 0.05160283\n",
      "Iteration 3039, loss = 0.05150402\n",
      "Iteration 3040, loss = 0.05140309\n",
      "Iteration 3041, loss = 0.05130025\n",
      "Iteration 3042, loss = 0.05119578\n",
      "Iteration 3043, loss = 0.05109002\n",
      "Iteration 3044, loss = 0.05098340\n",
      "Iteration 3045, loss = 0.05087637\n",
      "Iteration 3046, loss = 0.05076943\n",
      "Iteration 3047, loss = 0.05066308\n",
      "Iteration 3048, loss = 0.05055779\n",
      "Iteration 3049, loss = 0.05045400\n",
      "Iteration 3050, loss = 0.05035203\n",
      "Iteration 3051, loss = 0.05025211\n",
      "Iteration 3052, loss = 0.05015438\n",
      "Iteration 3053, loss = 0.05005882\n",
      "Iteration 3054, loss = 0.04996534\n",
      "Iteration 3055, loss = 0.04987374\n",
      "Iteration 3056, loss = 0.04978379\n",
      "Iteration 3057, loss = 0.04969522\n",
      "Iteration 3058, loss = 0.04960777\n",
      "Iteration 3059, loss = 0.04952117\n",
      "Iteration 3060, loss = 0.04943520\n",
      "Iteration 3061, loss = 0.04934968\n",
      "Iteration 3062, loss = 0.04926445\n",
      "Iteration 3063, loss = 0.04917940\n",
      "Iteration 3064, loss = 0.04909446\n",
      "Iteration 3065, loss = 0.04900958\n",
      "Iteration 3066, loss = 0.04892473\n",
      "Iteration 3067, loss = 0.04883991\n",
      "Iteration 3068, loss = 0.04875514\n",
      "Iteration 3069, loss = 0.04867042\n",
      "Iteration 3070, loss = 0.04858579\n",
      "Iteration 3071, loss = 0.04850125\n",
      "Iteration 3072, loss = 0.04841682\n",
      "Iteration 3073, loss = 0.04833250\n",
      "Iteration 3074, loss = 0.04824829\n",
      "Iteration 3075, loss = 0.04816418\n",
      "Iteration 3076, loss = 0.04808015\n",
      "Iteration 3077, loss = 0.04799617\n",
      "Iteration 3078, loss = 0.04791223\n",
      "Iteration 3079, loss = 0.04782827\n",
      "Iteration 3080, loss = 0.04774426\n",
      "Iteration 3081, loss = 0.04766015\n",
      "Iteration 3082, loss = 0.04757590\n",
      "Iteration 3083, loss = 0.04749145\n",
      "Iteration 3084, loss = 0.04740677\n",
      "Iteration 3085, loss = 0.04732182\n",
      "Iteration 3086, loss = 0.04723655\n",
      "Iteration 3087, loss = 0.04715092\n",
      "Iteration 3088, loss = 0.04706492\n",
      "Iteration 3089, loss = 0.04697852\n",
      "Iteration 3090, loss = 0.04689169\n",
      "Iteration 3091, loss = 0.04680443\n",
      "Iteration 3092, loss = 0.04671670\n",
      "Iteration 3093, loss = 0.04662852\n",
      "Iteration 3094, loss = 0.04653985\n",
      "Iteration 3095, loss = 0.04645071\n",
      "Iteration 3096, loss = 0.04636107\n",
      "Iteration 3097, loss = 0.04627095\n",
      "Iteration 3098, loss = 0.04618033\n",
      "Iteration 3099, loss = 0.04608922\n",
      "Iteration 3100, loss = 0.04599761\n",
      "Iteration 3101, loss = 0.04590551\n",
      "Iteration 3102, loss = 0.04581292\n",
      "Iteration 3103, loss = 0.04571985\n",
      "Iteration 3104, loss = 0.04562631\n",
      "Iteration 3105, loss = 0.04553231\n",
      "Iteration 3106, loss = 0.04543786\n",
      "Iteration 3107, loss = 0.04534299\n",
      "Iteration 3108, loss = 0.04524770\n",
      "Iteration 3109, loss = 0.04515202\n",
      "Iteration 3110, loss = 0.04505598\n",
      "Iteration 3111, loss = 0.04495960\n",
      "Iteration 3112, loss = 0.04486290\n",
      "Iteration 3113, loss = 0.04476592\n",
      "Iteration 3114, loss = 0.04466870\n",
      "Iteration 3115, loss = 0.04457126\n",
      "Iteration 3116, loss = 0.04447363\n",
      "Iteration 3117, loss = 0.04437587\n",
      "Iteration 3118, loss = 0.04427800\n",
      "Iteration 3119, loss = 0.04418006\n",
      "Iteration 3120, loss = 0.04408210\n",
      "Iteration 3121, loss = 0.04398415\n",
      "Iteration 3122, loss = 0.04388626\n",
      "Iteration 3123, loss = 0.04378846\n",
      "Iteration 3124, loss = 0.04369080\n",
      "Iteration 3125, loss = 0.04359331\n",
      "Iteration 3126, loss = 0.04349604\n",
      "Iteration 3127, loss = 0.04339902\n",
      "Iteration 3128, loss = 0.04330230\n",
      "Iteration 3129, loss = 0.04320592\n",
      "Iteration 3130, loss = 0.04310990\n",
      "Iteration 3131, loss = 0.04301428\n",
      "Iteration 3132, loss = 0.04291910\n",
      "Iteration 3133, loss = 0.04282439\n",
      "Iteration 3134, loss = 0.04273017\n",
      "Iteration 3135, loss = 0.04263649\n",
      "Iteration 3136, loss = 0.04254335\n",
      "Iteration 3137, loss = 0.04245080\n",
      "Iteration 3138, loss = 0.04235884\n",
      "Iteration 3139, loss = 0.04226751\n",
      "Iteration 3140, loss = 0.04217681\n",
      "Iteration 3141, loss = 0.04208677\n",
      "Iteration 3142, loss = 0.04199739\n",
      "Iteration 3143, loss = 0.04190870\n",
      "Iteration 3144, loss = 0.04182070\n",
      "Iteration 3145, loss = 0.04173340\n",
      "Iteration 3146, loss = 0.04164681\n",
      "Iteration 3147, loss = 0.04156093\n",
      "Iteration 3148, loss = 0.04147577\n",
      "Iteration 3149, loss = 0.04139132\n",
      "Iteration 3150, loss = 0.04130758\n",
      "Iteration 3151, loss = 0.04122457\n",
      "Iteration 3152, loss = 0.04114226\n",
      "Iteration 3153, loss = 0.04106067\n",
      "Iteration 3154, loss = 0.04097978\n",
      "Iteration 3155, loss = 0.04089960\n",
      "Iteration 3156, loss = 0.04082010\n",
      "Iteration 3157, loss = 0.04074130\n",
      "Iteration 3158, loss = 0.04066317\n",
      "Iteration 3159, loss = 0.04058571\n",
      "Iteration 3160, loss = 0.04050891\n",
      "Iteration 3161, loss = 0.04043277\n",
      "Iteration 3162, loss = 0.04035727\n",
      "Iteration 3163, loss = 0.04028240\n",
      "Iteration 3164, loss = 0.04020815\n",
      "Iteration 3165, loss = 0.04013451\n",
      "Iteration 3166, loss = 0.04006148\n",
      "Iteration 3167, loss = 0.03998904\n",
      "Iteration 3168, loss = 0.03991717\n",
      "Iteration 3169, loss = 0.03984588\n",
      "Iteration 3170, loss = 0.03977514\n",
      "Iteration 3171, loss = 0.03970496\n",
      "Iteration 3172, loss = 0.03963530\n",
      "Iteration 3173, loss = 0.03956618\n",
      "Iteration 3174, loss = 0.03949757\n",
      "Iteration 3175, loss = 0.03942947\n",
      "Iteration 3176, loss = 0.03936187\n",
      "Iteration 3177, loss = 0.03929475\n",
      "Iteration 3178, loss = 0.03922811\n",
      "Iteration 3179, loss = 0.03916194\n",
      "Iteration 3180, loss = 0.03909622\n",
      "Iteration 3181, loss = 0.03903096\n",
      "Iteration 3182, loss = 0.03896613\n",
      "Iteration 3183, loss = 0.03890173\n",
      "Iteration 3184, loss = 0.03883776\n",
      "Iteration 3185, loss = 0.03877421\n",
      "Iteration 3186, loss = 0.03871105\n",
      "Iteration 3187, loss = 0.03864830\n",
      "Iteration 3188, loss = 0.03858594\n",
      "Iteration 3189, loss = 0.03852396\n",
      "Iteration 3190, loss = 0.03846236\n",
      "Iteration 3191, loss = 0.03840112\n",
      "Iteration 3192, loss = 0.03834024\n",
      "Iteration 3193, loss = 0.03827972\n",
      "Iteration 3194, loss = 0.03821955\n",
      "Iteration 3195, loss = 0.03815972\n",
      "Iteration 3196, loss = 0.03810022\n",
      "Iteration 3197, loss = 0.03804105\n",
      "Iteration 3198, loss = 0.03798221\n",
      "Iteration 3199, loss = 0.03792367\n",
      "Iteration 3200, loss = 0.03786545\n",
      "Iteration 3201, loss = 0.03780754\n",
      "Iteration 3202, loss = 0.03774992\n",
      "Iteration 3203, loss = 0.03769260\n",
      "Iteration 3204, loss = 0.03763556\n",
      "Iteration 3205, loss = 0.03757881\n",
      "Iteration 3206, loss = 0.03752234\n",
      "Iteration 3207, loss = 0.03746614\n",
      "Iteration 3208, loss = 0.03741021\n",
      "Iteration 3209, loss = 0.03735454\n",
      "Iteration 3210, loss = 0.03729913\n",
      "Iteration 3211, loss = 0.03724398\n",
      "Iteration 3212, loss = 0.03718907\n",
      "Iteration 3213, loss = 0.03713442\n",
      "Iteration 3214, loss = 0.03708000\n",
      "Iteration 3215, loss = 0.03702583\n",
      "Iteration 3216, loss = 0.03697189\n",
      "Iteration 3217, loss = 0.03691818\n",
      "Iteration 3218, loss = 0.03686470\n",
      "Iteration 3219, loss = 0.03681144\n",
      "Iteration 3220, loss = 0.03675841\n",
      "Iteration 3221, loss = 0.03670559\n",
      "Iteration 3222, loss = 0.03665298\n",
      "Iteration 3223, loss = 0.03660059\n",
      "Iteration 3224, loss = 0.03654840\n",
      "Iteration 3225, loss = 0.03649642\n",
      "Iteration 3226, loss = 0.03644464\n",
      "Iteration 3227, loss = 0.03639306\n",
      "Iteration 3228, loss = 0.03634168\n",
      "Iteration 3229, loss = 0.03629049\n",
      "Iteration 3230, loss = 0.03623950\n",
      "Iteration 3231, loss = 0.03618869\n",
      "Iteration 3232, loss = 0.03613807\n",
      "Iteration 3233, loss = 0.03608764\n",
      "Iteration 3234, loss = 0.03603739\n",
      "Iteration 3235, loss = 0.03598732\n",
      "Iteration 3236, loss = 0.03593743\n",
      "Iteration 3237, loss = 0.03588771\n",
      "Iteration 3238, loss = 0.03583817\n",
      "Iteration 3239, loss = 0.03578881\n",
      "Iteration 3240, loss = 0.03573961\n",
      "Iteration 3241, loss = 0.03569058\n",
      "Iteration 3242, loss = 0.03564171\n",
      "Iteration 3243, loss = 0.03559302\n",
      "Iteration 3244, loss = 0.03554448\n",
      "Iteration 3245, loss = 0.03549611\n",
      "Iteration 3246, loss = 0.03544789\n",
      "Iteration 3247, loss = 0.03539983\n",
      "Iteration 3248, loss = 0.03535193\n",
      "Iteration 3249, loss = 0.03530418\n",
      "Iteration 3250, loss = 0.03525657\n",
      "Iteration 3251, loss = 0.03520912\n",
      "Iteration 3252, loss = 0.03516182\n",
      "Iteration 3253, loss = 0.03511466\n",
      "Iteration 3254, loss = 0.03506764\n",
      "Iteration 3255, loss = 0.03502076\n",
      "Iteration 3256, loss = 0.03497402\n",
      "Iteration 3257, loss = 0.03492741\n",
      "Iteration 3258, loss = 0.03488094\n",
      "Iteration 3259, loss = 0.03483459\n",
      "Iteration 3260, loss = 0.03478837\n",
      "Iteration 3261, loss = 0.03474228\n",
      "Iteration 3262, loss = 0.03469631\n",
      "Iteration 3263, loss = 0.03465045\n",
      "Iteration 3264, loss = 0.03460471\n",
      "Iteration 3265, loss = 0.03455908\n",
      "Iteration 3266, loss = 0.03451356\n",
      "Iteration 3267, loss = 0.03446814\n",
      "Iteration 3268, loss = 0.03442282\n",
      "Iteration 3269, loss = 0.03437760\n",
      "Iteration 3270, loss = 0.03433248\n",
      "Iteration 3271, loss = 0.03428744\n",
      "Iteration 3272, loss = 0.03424248\n",
      "Iteration 3273, loss = 0.03419760\n",
      "Iteration 3274, loss = 0.03415280\n",
      "Iteration 3275, loss = 0.03410807\n",
      "Iteration 3276, loss = 0.03406340\n",
      "Iteration 3277, loss = 0.03401879\n",
      "Iteration 3278, loss = 0.03397424\n",
      "Iteration 3279, loss = 0.03392974\n",
      "Iteration 3280, loss = 0.03388528\n",
      "Iteration 3281, loss = 0.03384086\n",
      "Iteration 3282, loss = 0.03379648\n",
      "Iteration 3283, loss = 0.03375212\n",
      "Iteration 3284, loss = 0.03370778\n",
      "Iteration 3285, loss = 0.03366346\n",
      "Iteration 3286, loss = 0.03361915\n",
      "Iteration 3287, loss = 0.03357485\n",
      "Iteration 3288, loss = 0.03353054\n",
      "Iteration 3289, loss = 0.03348623\n",
      "Iteration 3290, loss = 0.03344191\n",
      "Iteration 3291, loss = 0.03339757\n",
      "Iteration 3292, loss = 0.03335322\n",
      "Iteration 3293, loss = 0.03330884\n",
      "Iteration 3294, loss = 0.03326443\n",
      "Iteration 3295, loss = 0.03321999\n",
      "Iteration 3296, loss = 0.03317552\n",
      "Iteration 3297, loss = 0.03313101\n",
      "Iteration 3298, loss = 0.03308647\n",
      "Iteration 3299, loss = 0.03304189\n",
      "Iteration 3300, loss = 0.03299728\n",
      "Iteration 3301, loss = 0.03295264\n",
      "Iteration 3302, loss = 0.03290797\n",
      "Iteration 3303, loss = 0.03286328\n",
      "Iteration 3304, loss = 0.03281858\n",
      "Iteration 3305, loss = 0.03277387\n",
      "Iteration 3306, loss = 0.03272917\n",
      "Iteration 3307, loss = 0.03268449\n",
      "Iteration 3308, loss = 0.03263984\n",
      "Iteration 3309, loss = 0.03259523\n",
      "Iteration 3310, loss = 0.03255069\n",
      "Iteration 3311, loss = 0.03250623\n",
      "Iteration 3312, loss = 0.03246187\n",
      "Iteration 3313, loss = 0.03241763\n",
      "Iteration 3314, loss = 0.03237354\n",
      "Iteration 3315, loss = 0.03232961\n",
      "Iteration 3316, loss = 0.03228585\n",
      "Iteration 3317, loss = 0.03224231\n",
      "Iteration 3318, loss = 0.03219898\n",
      "Iteration 3319, loss = 0.03215589\n",
      "Iteration 3320, loss = 0.03211306\n",
      "Iteration 3321, loss = 0.03207049\n",
      "Iteration 3322, loss = 0.03202821\n",
      "Iteration 3323, loss = 0.03198621\n",
      "Iteration 3324, loss = 0.03194451\n",
      "Iteration 3325, loss = 0.03190311\n",
      "Iteration 3326, loss = 0.03186202\n",
      "Iteration 3327, loss = 0.03182124\n",
      "Iteration 3328, loss = 0.03178076\n",
      "Iteration 3329, loss = 0.03174059\n",
      "Iteration 3330, loss = 0.03170073\n",
      "Iteration 3331, loss = 0.03166118\n",
      "Iteration 3332, loss = 0.03162192\n",
      "Iteration 3333, loss = 0.03158297\n",
      "Iteration 3334, loss = 0.03154431\n",
      "Iteration 3335, loss = 0.03150594\n",
      "Iteration 3336, loss = 0.03146786\n",
      "Iteration 3337, loss = 0.03143007\n",
      "Iteration 3338, loss = 0.03139256\n",
      "Iteration 3339, loss = 0.03135532\n",
      "Iteration 3340, loss = 0.03131836\n",
      "Iteration 3341, loss = 0.03128166\n",
      "Iteration 3342, loss = 0.03124522\n",
      "Iteration 3343, loss = 0.03120903\n",
      "Iteration 3344, loss = 0.03117310\n",
      "Iteration 3345, loss = 0.03113741\n",
      "Iteration 3346, loss = 0.03110196\n",
      "Iteration 3347, loss = 0.03106674\n",
      "Iteration 3348, loss = 0.03103175\n",
      "Iteration 3349, loss = 0.03099698\n",
      "Iteration 3350, loss = 0.03096243\n",
      "Iteration 3351, loss = 0.03092810\n",
      "Iteration 3352, loss = 0.03089397\n",
      "Iteration 3353, loss = 0.03086005\n",
      "Iteration 3354, loss = 0.03082633\n",
      "Iteration 3355, loss = 0.03079281\n",
      "Iteration 3356, loss = 0.03075947\n",
      "Iteration 3357, loss = 0.03072632\n",
      "Iteration 3358, loss = 0.03069336\n",
      "Iteration 3359, loss = 0.03066057\n",
      "Iteration 3360, loss = 0.03062796\n",
      "Iteration 3361, loss = 0.03059552\n",
      "Iteration 3362, loss = 0.03056324\n",
      "Iteration 3363, loss = 0.03053113\n",
      "Iteration 3364, loss = 0.03049918\n",
      "Iteration 3365, loss = 0.03046738\n",
      "Iteration 3366, loss = 0.03043573\n",
      "Iteration 3367, loss = 0.03040424\n",
      "Iteration 3368, loss = 0.03037289\n",
      "Iteration 3369, loss = 0.03034168\n",
      "Iteration 3370, loss = 0.03031061\n",
      "Iteration 3371, loss = 0.03027967\n",
      "Iteration 3372, loss = 0.03024888\n",
      "Iteration 3373, loss = 0.03021821\n",
      "Iteration 3374, loss = 0.03018767\n",
      "Iteration 3375, loss = 0.03015726\n",
      "Iteration 3376, loss = 0.03012697\n",
      "Iteration 3377, loss = 0.03009680\n",
      "Iteration 3378, loss = 0.03006676\n",
      "Iteration 3379, loss = 0.03003683\n",
      "Iteration 3380, loss = 0.03000702\n",
      "Iteration 3381, loss = 0.02997732\n",
      "Iteration 3382, loss = 0.02994774\n",
      "Iteration 3383, loss = 0.02991827\n",
      "Iteration 3384, loss = 0.02988892\n",
      "Iteration 3385, loss = 0.02985967\n",
      "Iteration 3386, loss = 0.02983053\n",
      "Iteration 3387, loss = 0.02980149\n",
      "Iteration 3388, loss = 0.02977257\n",
      "Iteration 3389, loss = 0.02974375\n",
      "Iteration 3390, loss = 0.02971503\n",
      "Iteration 3391, loss = 0.02968642\n",
      "Iteration 3392, loss = 0.02965791\n",
      "Iteration 3393, loss = 0.02962950\n",
      "Iteration 3394, loss = 0.02960119\n",
      "Iteration 3395, loss = 0.02957298\n",
      "Iteration 3396, loss = 0.02954488\n",
      "Iteration 3397, loss = 0.02951687\n",
      "Iteration 3398, loss = 0.02948896\n",
      "Iteration 3399, loss = 0.02946115\n",
      "Iteration 3400, loss = 0.02943343\n",
      "Iteration 3401, loss = 0.02940582\n",
      "Iteration 3402, loss = 0.02937830\n",
      "Iteration 3403, loss = 0.02935087\n",
      "Iteration 3404, loss = 0.02932354\n",
      "Iteration 3405, loss = 0.02929631\n",
      "Iteration 3406, loss = 0.02926917\n",
      "Iteration 3407, loss = 0.02924212\n",
      "Iteration 3408, loss = 0.02921517\n",
      "Iteration 3409, loss = 0.02918831\n",
      "Iteration 3410, loss = 0.02916155\n",
      "Iteration 3411, loss = 0.02913487\n",
      "Iteration 3412, loss = 0.02910829\n",
      "Iteration 3413, loss = 0.02908180\n",
      "Iteration 3414, loss = 0.02905539\n",
      "Iteration 3415, loss = 0.02902908\n",
      "Iteration 3416, loss = 0.02900286\n",
      "Iteration 3417, loss = 0.02897673\n",
      "Iteration 3418, loss = 0.02895068\n",
      "Iteration 3419, loss = 0.02892473\n",
      "Iteration 3420, loss = 0.02889886\n",
      "Iteration 3421, loss = 0.02887308\n",
      "Iteration 3422, loss = 0.02884738\n",
      "Iteration 3423, loss = 0.02882177\n",
      "Iteration 3424, loss = 0.02879624\n",
      "Iteration 3425, loss = 0.02877080\n",
      "Iteration 3426, loss = 0.02874544\n",
      "Iteration 3427, loss = 0.02872016\n",
      "Iteration 3428, loss = 0.02869497\n",
      "Iteration 3429, loss = 0.02866985\n",
      "Iteration 3430, loss = 0.02864482\n",
      "Iteration 3431, loss = 0.02861987\n",
      "Iteration 3432, loss = 0.02859499\n",
      "Iteration 3433, loss = 0.02857020\n",
      "Iteration 3434, loss = 0.02854548\n",
      "Iteration 3435, loss = 0.02852084\n",
      "Iteration 3436, loss = 0.02849627\n",
      "Iteration 3437, loss = 0.02847178\n",
      "Iteration 3438, loss = 0.02844736\n",
      "Iteration 3439, loss = 0.02842302\n",
      "Iteration 3440, loss = 0.02839875\n",
      "Iteration 3441, loss = 0.02837455\n",
      "Iteration 3442, loss = 0.02835043\n",
      "Iteration 3443, loss = 0.02832637\n",
      "Iteration 3444, loss = 0.02830238\n",
      "Iteration 3445, loss = 0.02827846\n",
      "Iteration 3446, loss = 0.02825461\n",
      "Iteration 3447, loss = 0.02823083\n",
      "Iteration 3448, loss = 0.02820711\n",
      "Iteration 3449, loss = 0.02818345\n",
      "Iteration 3450, loss = 0.02815987\n",
      "Iteration 3451, loss = 0.02813634\n",
      "Iteration 3452, loss = 0.02811288\n",
      "Iteration 3453, loss = 0.02808947\n",
      "Iteration 3454, loss = 0.02806613\n",
      "Iteration 3455, loss = 0.02804285\n",
      "Iteration 3456, loss = 0.02801963\n",
      "Iteration 3457, loss = 0.02799647\n",
      "Iteration 3458, loss = 0.02797336\n",
      "Iteration 3459, loss = 0.02795031\n",
      "Iteration 3460, loss = 0.02792732\n",
      "Iteration 3461, loss = 0.02790438\n",
      "Iteration 3462, loss = 0.02788150\n",
      "Iteration 3463, loss = 0.02785867\n",
      "Iteration 3464, loss = 0.02783589\n",
      "Iteration 3465, loss = 0.02781316\n",
      "Iteration 3466, loss = 0.02779049\n",
      "Iteration 3467, loss = 0.02776786\n",
      "Iteration 3468, loss = 0.02774529\n",
      "Iteration 3469, loss = 0.02772276\n",
      "Iteration 3470, loss = 0.02770028\n",
      "Iteration 3471, loss = 0.02767784\n",
      "Iteration 3472, loss = 0.02765545\n",
      "Iteration 3473, loss = 0.02763311\n",
      "Iteration 3474, loss = 0.02761081\n",
      "Iteration 3475, loss = 0.02758856\n",
      "Iteration 3476, loss = 0.02756635\n",
      "Iteration 3477, loss = 0.02754418\n",
      "Iteration 3478, loss = 0.02752205\n",
      "Iteration 3479, loss = 0.02749996\n",
      "Iteration 3480, loss = 0.02747791\n",
      "Iteration 3481, loss = 0.02745590\n",
      "Iteration 3482, loss = 0.02743392\n",
      "Iteration 3483, loss = 0.02741199\n",
      "Iteration 3484, loss = 0.02739009\n",
      "Iteration 3485, loss = 0.02736823\n",
      "Iteration 3486, loss = 0.02734640\n",
      "Iteration 3487, loss = 0.02732461\n",
      "Iteration 3488, loss = 0.02730284\n",
      "Iteration 3489, loss = 0.02728112\n",
      "Iteration 3490, loss = 0.02725942\n",
      "Iteration 3491, loss = 0.02723776\n",
      "Iteration 3492, loss = 0.02721612\n",
      "Iteration 3493, loss = 0.02719452\n",
      "Iteration 3494, loss = 0.02717294\n",
      "Iteration 3495, loss = 0.02715139\n",
      "Iteration 3496, loss = 0.02712987\n",
      "Iteration 3497, loss = 0.02710838\n",
      "Iteration 3498, loss = 0.02708691\n",
      "Iteration 3499, loss = 0.02706547\n",
      "Iteration 3500, loss = 0.02704405\n",
      "Iteration 3501, loss = 0.02702265\n",
      "Iteration 3502, loss = 0.02700128\n",
      "Iteration 3503, loss = 0.02697993\n",
      "Iteration 3504, loss = 0.02695860\n",
      "Iteration 3505, loss = 0.02693730\n",
      "Iteration 3506, loss = 0.02691601\n",
      "Iteration 3507, loss = 0.02689474\n",
      "Iteration 3508, loss = 0.02687349\n",
      "Iteration 3509, loss = 0.02685225\n",
      "Iteration 3510, loss = 0.02683104\n",
      "Iteration 3511, loss = 0.02680984\n",
      "Iteration 3512, loss = 0.02678865\n",
      "Iteration 3513, loss = 0.02676748\n",
      "Iteration 3514, loss = 0.02674633\n",
      "Iteration 3515, loss = 0.02672519\n",
      "Iteration 3516, loss = 0.02670406\n",
      "Iteration 3517, loss = 0.02668294\n",
      "Iteration 3518, loss = 0.02666183\n",
      "Iteration 3519, loss = 0.02664073\n",
      "Iteration 3520, loss = 0.02661965\n",
      "Iteration 3521, loss = 0.02659857\n",
      "Iteration 3522, loss = 0.02657750\n",
      "Iteration 3523, loss = 0.02655643\n",
      "Iteration 3524, loss = 0.02653537\n",
      "Iteration 3525, loss = 0.02651432\n",
      "Iteration 3526, loss = 0.02649328\n",
      "Iteration 3527, loss = 0.02647223\n",
      "Iteration 3528, loss = 0.02645119\n",
      "Iteration 3529, loss = 0.02643016\n",
      "Iteration 3530, loss = 0.02640912\n",
      "Iteration 3531, loss = 0.02638809\n",
      "Iteration 3532, loss = 0.02636706\n",
      "Iteration 3533, loss = 0.02634603\n",
      "Iteration 3534, loss = 0.02632499\n",
      "Iteration 3535, loss = 0.02630396\n",
      "Iteration 3536, loss = 0.02628292\n",
      "Iteration 3537, loss = 0.02626188\n",
      "Iteration 3538, loss = 0.02624083\n",
      "Iteration 3539, loss = 0.02621978\n",
      "Iteration 3540, loss = 0.02619873\n",
      "Iteration 3541, loss = 0.02617767\n",
      "Iteration 3542, loss = 0.02615660\n",
      "Iteration 3543, loss = 0.02613552\n",
      "Iteration 3544, loss = 0.02611444\n",
      "Iteration 3545, loss = 0.02609334\n",
      "Iteration 3546, loss = 0.02607224\n",
      "Iteration 3547, loss = 0.02605113\n",
      "Iteration 3548, loss = 0.02603000\n",
      "Iteration 3549, loss = 0.02600886\n",
      "Iteration 3550, loss = 0.02598771\n",
      "Iteration 3551, loss = 0.02596655\n",
      "Iteration 3552, loss = 0.02594537\n",
      "Iteration 3553, loss = 0.02592418\n",
      "Iteration 3554, loss = 0.02590297\n",
      "Iteration 3555, loss = 0.02588175\n",
      "Iteration 3556, loss = 0.02586051\n",
      "Iteration 3557, loss = 0.02583925\n",
      "Iteration 3558, loss = 0.02581798\n",
      "Iteration 3559, loss = 0.02579668\n",
      "Iteration 3560, loss = 0.02577537\n",
      "Iteration 3561, loss = 0.02575404\n",
      "Iteration 3562, loss = 0.02573268\n",
      "Iteration 3563, loss = 0.02571131\n",
      "Iteration 3564, loss = 0.02568991\n",
      "Iteration 3565, loss = 0.02566849\n",
      "Iteration 3566, loss = 0.02564705\n",
      "Iteration 3567, loss = 0.02562559\n",
      "Iteration 3568, loss = 0.02560410\n",
      "Iteration 3569, loss = 0.02558259\n",
      "Iteration 3570, loss = 0.02556105\n",
      "Iteration 3571, loss = 0.02553949\n",
      "Iteration 3572, loss = 0.02551790\n",
      "Iteration 3573, loss = 0.02549629\n",
      "Iteration 3574, loss = 0.02547465\n",
      "Iteration 3575, loss = 0.02545298\n",
      "Iteration 3576, loss = 0.02543129\n",
      "Iteration 3577, loss = 0.02540957\n",
      "Iteration 3578, loss = 0.02538782\n",
      "Iteration 3579, loss = 0.02536605\n",
      "Iteration 3580, loss = 0.02534424\n",
      "Iteration 3581, loss = 0.02532241\n",
      "Iteration 3582, loss = 0.02530055\n",
      "Iteration 3583, loss = 0.02527866\n",
      "Iteration 3584, loss = 0.02525675\n",
      "Iteration 3585, loss = 0.02523480\n",
      "Iteration 3586, loss = 0.02521283\n",
      "Iteration 3587, loss = 0.02519083\n",
      "Iteration 3588, loss = 0.02516880\n",
      "Iteration 3589, loss = 0.02514675\n",
      "Iteration 3590, loss = 0.02512466\n",
      "Iteration 3591, loss = 0.02510255\n",
      "Iteration 3592, loss = 0.02508042\n",
      "Iteration 3593, loss = 0.02505825\n",
      "Iteration 3594, loss = 0.02503606\n",
      "Iteration 3595, loss = 0.02501384\n",
      "Iteration 3596, loss = 0.02499160\n",
      "Iteration 3597, loss = 0.02496933\n",
      "Iteration 3598, loss = 0.02494704\n",
      "Iteration 3599, loss = 0.02492473\n",
      "Iteration 3600, loss = 0.02490239\n",
      "Iteration 3601, loss = 0.02488003\n",
      "Iteration 3602, loss = 0.02485765\n",
      "Iteration 3603, loss = 0.02483525\n",
      "Iteration 3604, loss = 0.02481283\n",
      "Iteration 3605, loss = 0.02479039\n",
      "Iteration 3606, loss = 0.02476793\n",
      "Iteration 3607, loss = 0.02474545\n",
      "Iteration 3608, loss = 0.02472296\n",
      "Iteration 3609, loss = 0.02470046\n",
      "Iteration 3610, loss = 0.02467794\n",
      "Iteration 3611, loss = 0.02465541\n",
      "Iteration 3612, loss = 0.02463287\n",
      "Iteration 3613, loss = 0.02461031\n",
      "Iteration 3614, loss = 0.02458775\n",
      "Iteration 3615, loss = 0.02456518\n",
      "Iteration 3616, loss = 0.02454261\n",
      "Iteration 3617, loss = 0.02452003\n",
      "Iteration 3618, loss = 0.02449745\n",
      "Iteration 3619, loss = 0.02447487\n",
      "Iteration 3620, loss = 0.02445228\n",
      "Iteration 3621, loss = 0.02442970\n",
      "Iteration 3622, loss = 0.02440712\n",
      "Iteration 3623, loss = 0.02438455\n",
      "Iteration 3624, loss = 0.02436198\n",
      "Iteration 3625, loss = 0.02433942\n",
      "Iteration 3626, loss = 0.02431686\n",
      "Iteration 3627, loss = 0.02429432\n",
      "Iteration 3628, loss = 0.02427179\n",
      "Iteration 3629, loss = 0.02424927\n",
      "Iteration 3630, loss = 0.02422677\n",
      "Iteration 3631, loss = 0.02420428\n",
      "Iteration 3632, loss = 0.02418182\n",
      "Iteration 3633, loss = 0.02415937\n",
      "Iteration 3634, loss = 0.02413694\n",
      "Iteration 3635, loss = 0.02411454\n",
      "Iteration 3636, loss = 0.02409216\n",
      "Iteration 3637, loss = 0.02406981\n",
      "Iteration 3638, loss = 0.02404748\n",
      "Iteration 3639, loss = 0.02402519\n",
      "Iteration 3640, loss = 0.02400295\n",
      "Iteration 3641, loss = 0.02398077\n",
      "Iteration 3642, loss = 0.02395872\n",
      "Iteration 3643, loss = 0.02393696\n",
      "Iteration 3644, loss = 0.02391590\n",
      "Iteration 3645, loss = 0.02389683\n",
      "Iteration 3646, loss = 0.02388246\n",
      "Iteration 3647, loss = 0.02388322\n",
      "Iteration 3648, loss = 0.02391179\n",
      "Iteration 3649, loss = 0.02404987\n",
      "Iteration 3650, loss = 0.02419874\n",
      "Iteration 3651, loss = 0.02470329\n",
      "Iteration 3652, loss = 0.02416114\n",
      "Iteration 3653, loss = 0.02385696\n",
      "Iteration 3654, loss = 0.02371263\n",
      "Iteration 3655, loss = 0.02380441\n",
      "Iteration 3656, loss = 0.02408052\n",
      "Iteration 3657, loss = 0.02404771\n",
      "Iteration 3658, loss = 0.02400147\n",
      "Iteration 3659, loss = 0.02366272\n",
      "Iteration 3660, loss = 0.02360876\n",
      "Iteration 3661, loss = 0.02377665\n",
      "Iteration 3662, loss = 0.02385827\n",
      "Iteration 3663, loss = 0.02387549\n",
      "Iteration 3664, loss = 0.02354525\n",
      "Iteration 3665, loss = 0.02350010\n",
      "Iteration 3666, loss = 0.02368532\n",
      "Iteration 3667, loss = 0.02367303\n",
      "Iteration 3668, loss = 0.02357783\n",
      "Iteration 3669, loss = 0.02339438\n",
      "Iteration 3670, loss = 0.02344478\n",
      "Iteration 3671, loss = 0.02361402\n",
      "Iteration 3672, loss = 0.02350040\n",
      "Iteration 3673, loss = 0.02336328\n",
      "Iteration 3674, loss = 0.02329235\n",
      "Iteration 3675, loss = 0.02336564\n",
      "Iteration 3676, loss = 0.02344128\n",
      "Iteration 3677, loss = 0.02329822\n",
      "Iteration 3678, loss = 0.02320942\n",
      "Iteration 3679, loss = 0.02322142\n",
      "Iteration 3680, loss = 0.02325323\n",
      "Iteration 3681, loss = 0.02322997\n",
      "Iteration 3682, loss = 0.02313629\n",
      "Iteration 3683, loss = 0.02311018\n",
      "Iteration 3684, loss = 0.02313196\n",
      "Iteration 3685, loss = 0.02311419\n",
      "Iteration 3686, loss = 0.02306530\n",
      "Iteration 3687, loss = 0.02302135\n",
      "Iteration 3688, loss = 0.02301923\n",
      "Iteration 3689, loss = 0.02302037\n",
      "Iteration 3690, loss = 0.02298519\n",
      "Iteration 3691, loss = 0.02294639\n",
      "Iteration 3692, loss = 0.02292843\n",
      "Iteration 3693, loss = 0.02292397\n",
      "Iteration 3694, loss = 0.02290742\n",
      "Iteration 3695, loss = 0.02287337\n",
      "Iteration 3696, loss = 0.02284817\n",
      "Iteration 3697, loss = 0.02283652\n",
      "Iteration 3698, loss = 0.02282354\n",
      "Iteration 3699, loss = 0.02280043\n",
      "Iteration 3700, loss = 0.02277342\n",
      "Iteration 3701, loss = 0.02275537\n",
      "Iteration 3702, loss = 0.02274261\n",
      "Iteration 3703, loss = 0.02272491\n",
      "Iteration 3704, loss = 0.02270167\n",
      "Iteration 3705, loss = 0.02267969\n",
      "Iteration 3706, loss = 0.02266377\n",
      "Iteration 3707, loss = 0.02264888\n",
      "Iteration 3708, loss = 0.02262962\n",
      "Iteration 3709, loss = 0.02260814\n",
      "Iteration 3710, loss = 0.02258897\n",
      "Iteration 3711, loss = 0.02257316\n",
      "Iteration 3712, loss = 0.02255682\n",
      "Iteration 3713, loss = 0.02253775\n",
      "Iteration 3714, loss = 0.02251800\n",
      "Iteration 3715, loss = 0.02250020\n",
      "Iteration 3716, loss = 0.02248408\n",
      "Iteration 3717, loss = 0.02246719\n",
      "Iteration 3718, loss = 0.02244878\n",
      "Iteration 3719, loss = 0.02243026\n",
      "Iteration 3720, loss = 0.02241307\n",
      "Iteration 3721, loss = 0.02239675\n",
      "Iteration 3722, loss = 0.02237985\n",
      "Iteration 3723, loss = 0.02236211\n",
      "Iteration 3724, loss = 0.02234441\n",
      "Iteration 3725, loss = 0.02232757\n",
      "Iteration 3726, loss = 0.02231123\n",
      "Iteration 3727, loss = 0.02229452\n",
      "Iteration 3728, loss = 0.02227738\n",
      "Iteration 3729, loss = 0.02226027\n",
      "Iteration 3730, loss = 0.02224370\n",
      "Iteration 3731, loss = 0.02222745\n",
      "Iteration 3732, loss = 0.02221104\n",
      "Iteration 3733, loss = 0.02219438\n",
      "Iteration 3734, loss = 0.02217773\n",
      "Iteration 3735, loss = 0.02216142\n",
      "Iteration 3736, loss = 0.02214534\n",
      "Iteration 3737, loss = 0.02212922\n",
      "Iteration 3738, loss = 0.02211296\n",
      "Iteration 3739, loss = 0.02209671\n",
      "Iteration 3740, loss = 0.02208067\n",
      "Iteration 3741, loss = 0.02206479\n",
      "Iteration 3742, loss = 0.02204895\n",
      "Iteration 3743, loss = 0.02203305\n",
      "Iteration 3744, loss = 0.02201714\n",
      "Iteration 3745, loss = 0.02200136\n",
      "Iteration 3746, loss = 0.02198571\n",
      "Iteration 3747, loss = 0.02197013\n",
      "Iteration 3748, loss = 0.02195454\n",
      "Iteration 3749, loss = 0.02193895\n",
      "Iteration 3750, loss = 0.02192344\n",
      "Iteration 3751, loss = 0.02190802\n",
      "Iteration 3752, loss = 0.02189268\n",
      "Iteration 3753, loss = 0.02187737\n",
      "Iteration 3754, loss = 0.02186207\n",
      "Iteration 3755, loss = 0.02184682\n",
      "Iteration 3756, loss = 0.02183164\n",
      "Iteration 3757, loss = 0.02181653\n",
      "Iteration 3758, loss = 0.02180146\n",
      "Iteration 3759, loss = 0.02178643\n",
      "Iteration 3760, loss = 0.02177143\n",
      "Iteration 3761, loss = 0.02175648\n",
      "Iteration 3762, loss = 0.02174159\n",
      "Iteration 3763, loss = 0.02172675\n",
      "Iteration 3764, loss = 0.02171196\n",
      "Iteration 3765, loss = 0.02169720\n",
      "Iteration 3766, loss = 0.02168247\n",
      "Iteration 3767, loss = 0.02166780\n",
      "Iteration 3768, loss = 0.02165317\n",
      "Iteration 3769, loss = 0.02163859\n",
      "Iteration 3770, loss = 0.02162405\n",
      "Iteration 3771, loss = 0.02160954\n",
      "Iteration 3772, loss = 0.02159507\n",
      "Iteration 3773, loss = 0.02158065\n",
      "Iteration 3774, loss = 0.02156626\n",
      "Iteration 3775, loss = 0.02155192\n",
      "Iteration 3776, loss = 0.02153761\n",
      "Iteration 3777, loss = 0.02152334\n",
      "Iteration 3778, loss = 0.02150910\n",
      "Iteration 3779, loss = 0.02149491\n",
      "Iteration 3780, loss = 0.02148075\n",
      "Iteration 3781, loss = 0.02146662\n",
      "Iteration 3782, loss = 0.02145253\n",
      "Iteration 3783, loss = 0.02143847\n",
      "Iteration 3784, loss = 0.02142445\n",
      "Iteration 3785, loss = 0.02141046\n",
      "Iteration 3786, loss = 0.02139650\n",
      "Iteration 3787, loss = 0.02138258\n",
      "Iteration 3788, loss = 0.02136869\n",
      "Iteration 3789, loss = 0.02135483\n",
      "Iteration 3790, loss = 0.02134100\n",
      "Iteration 3791, loss = 0.02132719\n",
      "Iteration 3792, loss = 0.02131342\n",
      "Iteration 3793, loss = 0.02129968\n",
      "Iteration 3794, loss = 0.02128597\n",
      "Iteration 3795, loss = 0.02127228\n",
      "Iteration 3796, loss = 0.02125862\n",
      "Iteration 3797, loss = 0.02124499\n",
      "Iteration 3798, loss = 0.02123138\n",
      "Iteration 3799, loss = 0.02121780\n",
      "Iteration 3800, loss = 0.02120425\n",
      "Iteration 3801, loss = 0.02119071\n",
      "Iteration 3802, loss = 0.02117721\n",
      "Iteration 3803, loss = 0.02116372\n",
      "Iteration 3804, loss = 0.02115026\n",
      "Iteration 3805, loss = 0.02113682\n",
      "Iteration 3806, loss = 0.02112340\n",
      "Iteration 3807, loss = 0.02111000\n",
      "Iteration 3808, loss = 0.02109662\n",
      "Iteration 3809, loss = 0.02108327\n",
      "Iteration 3810, loss = 0.02106993\n",
      "Iteration 3811, loss = 0.02105660\n",
      "Iteration 3812, loss = 0.02104330\n",
      "Iteration 3813, loss = 0.02103001\n",
      "Iteration 3814, loss = 0.02101674\n",
      "Iteration 3815, loss = 0.02100349\n",
      "Iteration 3816, loss = 0.02099025\n",
      "Iteration 3817, loss = 0.02097702\n",
      "Iteration 3818, loss = 0.02096381\n",
      "Iteration 3819, loss = 0.02095061\n",
      "Iteration 3820, loss = 0.02093742\n",
      "Iteration 3821, loss = 0.02092425\n",
      "Iteration 3822, loss = 0.02091108\n",
      "Iteration 3823, loss = 0.02089792\n",
      "Iteration 3824, loss = 0.02088478\n",
      "Iteration 3825, loss = 0.02087164\n",
      "Iteration 3826, loss = 0.02085851\n",
      "Iteration 3827, loss = 0.02084538\n",
      "Iteration 3828, loss = 0.02083227\n",
      "Iteration 3829, loss = 0.02081915\n",
      "Iteration 3830, loss = 0.02080604\n",
      "Iteration 3831, loss = 0.02079294\n",
      "Iteration 3832, loss = 0.02077984\n",
      "Iteration 3833, loss = 0.02076674\n",
      "Iteration 3834, loss = 0.02075364\n",
      "Iteration 3835, loss = 0.02074055\n",
      "Iteration 3836, loss = 0.02072745\n",
      "Iteration 3837, loss = 0.02071435\n",
      "Iteration 3838, loss = 0.02070126\n",
      "Iteration 3839, loss = 0.02068816\n",
      "Iteration 3840, loss = 0.02067506\n",
      "Iteration 3841, loss = 0.02066195\n",
      "Iteration 3842, loss = 0.02064885\n",
      "Iteration 3843, loss = 0.02063574\n",
      "Iteration 3844, loss = 0.02062263\n",
      "Iteration 3845, loss = 0.02060951\n",
      "Iteration 3846, loss = 0.02059640\n",
      "Iteration 3847, loss = 0.02058328\n",
      "Iteration 3848, loss = 0.02057015\n",
      "Iteration 3849, loss = 0.02055703\n",
      "Iteration 3850, loss = 0.02054390\n",
      "Iteration 3851, loss = 0.02053077\n",
      "Iteration 3852, loss = 0.02051765\n",
      "Iteration 3853, loss = 0.02050453\n",
      "Iteration 3854, loss = 0.02049141\n",
      "Iteration 3855, loss = 0.02047829\n",
      "Iteration 3856, loss = 0.02046518\n",
      "Iteration 3857, loss = 0.02045209\n",
      "Iteration 3858, loss = 0.02043900\n",
      "Iteration 3859, loss = 0.02042593\n",
      "Iteration 3860, loss = 0.02041288\n",
      "Iteration 3861, loss = 0.02039984\n",
      "Iteration 3862, loss = 0.02038683\n",
      "Iteration 3863, loss = 0.02037385\n",
      "Iteration 3864, loss = 0.02036090\n",
      "Iteration 3865, loss = 0.02034799\n",
      "Iteration 3866, loss = 0.02033511\n",
      "Iteration 3867, loss = 0.02032227\n",
      "Iteration 3868, loss = 0.02030948\n",
      "Iteration 3869, loss = 0.02029674\n",
      "Iteration 3870, loss = 0.02028405\n",
      "Iteration 3871, loss = 0.02027141\n",
      "Iteration 3872, loss = 0.02025884\n",
      "Iteration 3873, loss = 0.02024632\n",
      "Iteration 3874, loss = 0.02023386\n",
      "Iteration 3875, loss = 0.02022147\n",
      "Iteration 3876, loss = 0.02020914\n",
      "Iteration 3877, loss = 0.02019688\n",
      "Iteration 3878, loss = 0.02018468\n",
      "Iteration 3879, loss = 0.02017256\n",
      "Iteration 3880, loss = 0.02016049\n",
      "Iteration 3881, loss = 0.02014850\n",
      "Iteration 3882, loss = 0.02013656\n",
      "Iteration 3883, loss = 0.02012469\n",
      "Iteration 3884, loss = 0.02011288\n",
      "Iteration 3885, loss = 0.02010113\n",
      "Iteration 3886, loss = 0.02008944\n",
      "Iteration 3887, loss = 0.02007780\n",
      "Iteration 3888, loss = 0.02006621\n",
      "Iteration 3889, loss = 0.02005467\n",
      "Iteration 3890, loss = 0.02004317\n",
      "Iteration 3891, loss = 0.02003172\n",
      "Iteration 3892, loss = 0.02002030\n",
      "Iteration 3893, loss = 0.02000893\n",
      "Iteration 3894, loss = 0.01999759\n",
      "Iteration 3895, loss = 0.01998629\n",
      "Iteration 3896, loss = 0.01997504\n",
      "Iteration 3897, loss = 0.01996385\n",
      "Iteration 3898, loss = 0.01995278\n",
      "Iteration 3899, loss = 0.01994197\n",
      "Iteration 3900, loss = 0.01993181\n",
      "Iteration 3901, loss = 0.01992323\n",
      "Iteration 3902, loss = 0.01991963\n",
      "Iteration 3903, loss = 0.01992659\n",
      "Iteration 3904, loss = 0.01997625\n",
      "Iteration 3905, loss = 0.02006638\n",
      "Iteration 3906, loss = 0.02043296\n",
      "Iteration 3907, loss = 0.02037587\n",
      "Iteration 3908, loss = 0.02045264\n",
      "Iteration 3909, loss = 0.02006119\n",
      "Iteration 3910, loss = 0.01986824\n",
      "Iteration 3911, loss = 0.01985574\n",
      "Iteration 3912, loss = 0.01994665\n",
      "Iteration 3913, loss = 0.02010341\n",
      "Iteration 3914, loss = 0.02004634\n",
      "Iteration 3915, loss = 0.02009801\n",
      "Iteration 3916, loss = 0.01985180\n",
      "Iteration 3917, loss = 0.01982313\n",
      "Iteration 3918, loss = 0.02007870\n",
      "Iteration 3919, loss = 0.02010342\n",
      "Iteration 3920, loss = 0.02017158\n",
      "Iteration 3921, loss = 0.01984278\n",
      "Iteration 3922, loss = 0.01984912\n",
      "Iteration 3923, loss = 0.02024625\n",
      "Iteration 3924, loss = 0.02013939\n",
      "Iteration 3925, loss = 0.01981788\n",
      "Iteration 3926, loss = 0.01973943\n",
      "Iteration 3927, loss = 0.01991993\n",
      "Iteration 3928, loss = 0.02018296\n",
      "Iteration 3929, loss = 0.01987003\n",
      "Iteration 3930, loss = 0.01969156\n",
      "Iteration 3931, loss = 0.02003724\n",
      "Iteration 3932, loss = 0.01987198\n",
      "Iteration 3933, loss = 0.01985385\n",
      "Iteration 3934, loss = 0.01974644\n",
      "Iteration 3935, loss = 0.01981819\n",
      "Iteration 3936, loss = 0.01999872\n",
      "Iteration 3937, loss = 0.01962959\n",
      "Iteration 3938, loss = 0.01966328\n",
      "Iteration 3939, loss = 0.01980161\n",
      "Iteration 3940, loss = 0.01966989\n",
      "Iteration 3941, loss = 0.01955292\n",
      "Iteration 3942, loss = 0.01954377\n",
      "Iteration 3943, loss = 0.01956723\n",
      "Iteration 3944, loss = 0.01958966\n",
      "Iteration 3945, loss = 0.01950987\n",
      "Iteration 3946, loss = 0.01948891\n",
      "Iteration 3947, loss = 0.01948343\n",
      "Iteration 3948, loss = 0.01951242\n",
      "Iteration 3949, loss = 0.01953218\n",
      "Iteration 3950, loss = 0.01947598\n",
      "Iteration 3951, loss = 0.01942992\n",
      "Iteration 3952, loss = 0.01947492\n",
      "Iteration 3953, loss = 0.01946270\n",
      "Iteration 3954, loss = 0.01947072\n",
      "Iteration 3955, loss = 0.01939830\n",
      "Iteration 3956, loss = 0.01942735\n",
      "Iteration 3957, loss = 0.01950649\n",
      "Iteration 3958, loss = 0.01947013\n",
      "Iteration 3959, loss = 0.01936051\n",
      "Iteration 3960, loss = 0.01937292\n",
      "Iteration 3961, loss = 0.01940367\n",
      "Iteration 3962, loss = 0.01946720\n",
      "Iteration 3963, loss = 0.01935223\n",
      "Iteration 3964, loss = 0.01936428\n",
      "Iteration 3965, loss = 0.01951461\n",
      "Iteration 3966, loss = 0.01943434\n",
      "Iteration 3967, loss = 0.01930111\n",
      "Iteration 3968, loss = 0.01930844\n",
      "Iteration 3969, loss = 0.01941610\n",
      "Iteration 3970, loss = 0.01958456\n",
      "Iteration 3971, loss = 0.01941092\n",
      "Iteration 3972, loss = 0.01934102\n",
      "Iteration 3973, loss = 0.01957035\n",
      "Iteration 3974, loss = 0.01930023\n",
      "Iteration 3975, loss = 0.01931977\n",
      "Iteration 3976, loss = 0.01933427\n",
      "Iteration 3977, loss = 0.01933206\n",
      "Iteration 3978, loss = 0.01926123\n",
      "Iteration 3979, loss = 0.01921812\n",
      "Iteration 3980, loss = 0.01919617\n",
      "Iteration 3981, loss = 0.01925543\n",
      "Iteration 3982, loss = 0.01920694\n",
      "Iteration 3983, loss = 0.01921828\n",
      "Iteration 3984, loss = 0.01915650\n",
      "Iteration 3985, loss = 0.01920953\n",
      "Iteration 3986, loss = 0.01929945\n",
      "Iteration 3987, loss = 0.01918309\n",
      "Iteration 3988, loss = 0.01912661\n",
      "Iteration 3989, loss = 0.01921512\n",
      "Iteration 3990, loss = 0.01917886\n",
      "Iteration 3991, loss = 0.01919416\n",
      "Iteration 3992, loss = 0.01911332\n",
      "Iteration 3993, loss = 0.01913713\n",
      "Iteration 3994, loss = 0.01924978\n",
      "Iteration 3995, loss = 0.01911615\n",
      "Iteration 3996, loss = 0.01904034\n",
      "Iteration 3997, loss = 0.01910034\n",
      "Iteration 3998, loss = 0.01914810\n",
      "Iteration 3999, loss = 0.01918072\n",
      "Iteration 4000, loss = 0.01908971\n",
      "Iteration 4001, loss = 0.01908306\n",
      "Iteration 4002, loss = 0.01923164\n",
      "Iteration 4003, loss = 0.01900438\n",
      "Iteration 4004, loss = 0.01903108\n",
      "Iteration 4005, loss = 0.01906407\n",
      "Iteration 4006, loss = 0.01904283\n",
      "Iteration 4007, loss = 0.01897536\n",
      "Iteration 4008, loss = 0.01896096\n",
      "Iteration 4009, loss = 0.01894034\n",
      "Iteration 4010, loss = 0.01898817\n",
      "Iteration 4011, loss = 0.01891854\n",
      "Iteration 4012, loss = 0.01891547\n",
      "Iteration 4013, loss = 0.01889170\n",
      "Iteration 4014, loss = 0.01892985\n",
      "Iteration 4015, loss = 0.01893573\n",
      "Iteration 4016, loss = 0.01889607\n",
      "Iteration 4017, loss = 0.01885139\n",
      "Iteration 4018, loss = 0.01890874\n",
      "Iteration 4019, loss = 0.01887582\n",
      "Iteration 4020, loss = 0.01887189\n",
      "Iteration 4021, loss = 0.01881750\n",
      "Iteration 4022, loss = 0.01886811\n",
      "Iteration 4023, loss = 0.01892886\n",
      "Iteration 4024, loss = 0.01885591\n",
      "Iteration 4025, loss = 0.01878200\n",
      "Iteration 4026, loss = 0.01886048\n",
      "Iteration 4027, loss = 0.01884657\n",
      "Iteration 4028, loss = 0.01884894\n",
      "Iteration 4029, loss = 0.01876721\n",
      "Iteration 4030, loss = 0.01884654\n",
      "Iteration 4031, loss = 0.01898271\n",
      "Iteration 4032, loss = 0.01880645\n",
      "Iteration 4033, loss = 0.01876457\n",
      "Iteration 4034, loss = 0.01894213\n",
      "Iteration 4035, loss = 0.01879730\n",
      "Iteration 4036, loss = 0.01874493\n",
      "Iteration 4037, loss = 0.01872557\n",
      "Iteration 4038, loss = 0.01877549\n",
      "Iteration 4039, loss = 0.01877883\n",
      "Iteration 4040, loss = 0.01869069\n",
      "Iteration 4041, loss = 0.01867576\n",
      "Iteration 4042, loss = 0.01875565\n",
      "Iteration 4043, loss = 0.01869579\n",
      "Iteration 4044, loss = 0.01866827\n",
      "Iteration 4045, loss = 0.01863604\n",
      "Iteration 4046, loss = 0.01868039\n",
      "Iteration 4047, loss = 0.01872358\n",
      "Iteration 4048, loss = 0.01863298\n",
      "Iteration 4049, loss = 0.01860309\n",
      "Iteration 4050, loss = 0.01867675\n",
      "Iteration 4051, loss = 0.01863095\n",
      "Iteration 4052, loss = 0.01860431\n",
      "Iteration 4053, loss = 0.01856792\n",
      "Iteration 4054, loss = 0.01860546\n",
      "Iteration 4055, loss = 0.01864511\n",
      "Iteration 4056, loss = 0.01857099\n",
      "Iteration 4057, loss = 0.01852832\n",
      "Iteration 4058, loss = 0.01858003\n",
      "Iteration 4059, loss = 0.01857029\n",
      "Iteration 4060, loss = 0.01855418\n",
      "Iteration 4061, loss = 0.01850389\n",
      "Iteration 4062, loss = 0.01854331\n",
      "Iteration 4063, loss = 0.01861160\n",
      "Iteration 4064, loss = 0.01851611\n",
      "Iteration 4065, loss = 0.01846957\n",
      "Iteration 4066, loss = 0.01853087\n",
      "Iteration 4067, loss = 0.01851782\n",
      "Iteration 4068, loss = 0.01850041\n",
      "Iteration 4069, loss = 0.01844622\n",
      "Iteration 4070, loss = 0.01849083\n",
      "Iteration 4071, loss = 0.01857158\n",
      "Iteration 4072, loss = 0.01845943\n",
      "Iteration 4073, loss = 0.01841428\n",
      "Iteration 4074, loss = 0.01849039\n",
      "Iteration 4075, loss = 0.01846349\n",
      "Iteration 4076, loss = 0.01843528\n",
      "Iteration 4077, loss = 0.01838568\n",
      "Iteration 4078, loss = 0.01843552\n",
      "Iteration 4079, loss = 0.01850658\n",
      "Iteration 4080, loss = 0.01839457\n",
      "Iteration 4081, loss = 0.01835575\n",
      "Iteration 4082, loss = 0.01843784\n",
      "Iteration 4083, loss = 0.01839804\n",
      "Iteration 4084, loss = 0.01836363\n",
      "Iteration 4085, loss = 0.01832456\n",
      "Iteration 4086, loss = 0.01837303\n",
      "Iteration 4087, loss = 0.01842611\n",
      "Iteration 4088, loss = 0.01832710\n",
      "Iteration 4089, loss = 0.01829568\n",
      "Iteration 4090, loss = 0.01837164\n",
      "Iteration 4091, loss = 0.01832831\n",
      "Iteration 4092, loss = 0.01829408\n",
      "Iteration 4093, loss = 0.01826156\n",
      "Iteration 4094, loss = 0.01830380\n",
      "Iteration 4095, loss = 0.01834039\n",
      "Iteration 4096, loss = 0.01826197\n",
      "Iteration 4097, loss = 0.01823047\n",
      "Iteration 4098, loss = 0.01828973\n",
      "Iteration 4099, loss = 0.01826064\n",
      "Iteration 4100, loss = 0.01823252\n",
      "Iteration 4101, loss = 0.01819842\n",
      "Iteration 4102, loss = 0.01823554\n",
      "Iteration 4103, loss = 0.01827101\n",
      "Iteration 4104, loss = 0.01820278\n",
      "Iteration 4105, loss = 0.01816714\n",
      "Iteration 4106, loss = 0.01821467\n",
      "Iteration 4107, loss = 0.01819937\n",
      "Iteration 4108, loss = 0.01817823\n",
      "Iteration 4109, loss = 0.01813837\n",
      "Iteration 4110, loss = 0.01817181\n",
      "Iteration 4111, loss = 0.01821361\n",
      "Iteration 4112, loss = 0.01814836\n",
      "Iteration 4113, loss = 0.01810720\n",
      "Iteration 4114, loss = 0.01814784\n",
      "Iteration 4115, loss = 0.01814341\n",
      "Iteration 4116, loss = 0.01812842\n",
      "Iteration 4117, loss = 0.01808163\n",
      "Iteration 4118, loss = 0.01811269\n",
      "Iteration 4119, loss = 0.01816397\n",
      "Iteration 4120, loss = 0.01809672\n",
      "Iteration 4121, loss = 0.01804983\n",
      "Iteration 4122, loss = 0.01808704\n",
      "Iteration 4123, loss = 0.01809142\n",
      "Iteration 4124, loss = 0.01808161\n",
      "Iteration 4125, loss = 0.01802754\n",
      "Iteration 4126, loss = 0.01805842\n",
      "Iteration 4127, loss = 0.01812233\n",
      "Iteration 4128, loss = 0.01804615\n",
      "Iteration 4129, loss = 0.01799501\n",
      "Iteration 4130, loss = 0.01803472\n",
      "Iteration 4131, loss = 0.01804282\n",
      "Iteration 4132, loss = 0.01803378\n",
      "Iteration 4133, loss = 0.01797464\n",
      "Iteration 4134, loss = 0.01800906\n",
      "Iteration 4135, loss = 0.01808484\n",
      "Iteration 4136, loss = 0.01799416\n",
      "Iteration 4137, loss = 0.01794218\n",
      "Iteration 4138, loss = 0.01799178\n",
      "Iteration 4139, loss = 0.01799442\n",
      "Iteration 4140, loss = 0.01797836\n",
      "Iteration 4141, loss = 0.01792071\n",
      "Iteration 4142, loss = 0.01796131\n",
      "Iteration 4143, loss = 0.01803814\n",
      "Iteration 4144, loss = 0.01793721\n",
      "Iteration 4145, loss = 0.01789046\n",
      "Iteration 4146, loss = 0.01795145\n",
      "Iteration 4147, loss = 0.01793989\n",
      "Iteration 4148, loss = 0.01791278\n",
      "Iteration 4149, loss = 0.01786494\n",
      "Iteration 4150, loss = 0.01790777\n",
      "Iteration 4151, loss = 0.01796820\n",
      "Iteration 4152, loss = 0.01787481\n",
      "Iteration 4153, loss = 0.01783648\n",
      "Iteration 4154, loss = 0.01789631\n",
      "Iteration 4155, loss = 0.01787701\n",
      "Iteration 4156, loss = 0.01784598\n",
      "Iteration 4157, loss = 0.01780822\n",
      "Iteration 4158, loss = 0.01784576\n",
      "Iteration 4159, loss = 0.01788700\n",
      "Iteration 4160, loss = 0.01781220\n",
      "Iteration 4161, loss = 0.01777931\n",
      "Iteration 4162, loss = 0.01782574\n",
      "Iteration 4163, loss = 0.01781176\n",
      "Iteration 4164, loss = 0.01778476\n",
      "Iteration 4165, loss = 0.01775125\n",
      "Iteration 4166, loss = 0.01777987\n",
      "Iteration 4167, loss = 0.01781117\n",
      "Iteration 4168, loss = 0.01775367\n",
      "Iteration 4169, loss = 0.01772183\n",
      "Iteration 4170, loss = 0.01775229\n",
      "Iteration 4171, loss = 0.01774927\n",
      "Iteration 4172, loss = 0.01772952\n",
      "Iteration 4173, loss = 0.01769586\n",
      "Iteration 4174, loss = 0.01771448\n",
      "Iteration 4175, loss = 0.01774299\n",
      "Iteration 4176, loss = 0.01769919\n",
      "Iteration 4177, loss = 0.01766680\n",
      "Iteration 4178, loss = 0.01768223\n",
      "Iteration 4179, loss = 0.01768990\n",
      "Iteration 4180, loss = 0.01767865\n",
      "Iteration 4181, loss = 0.01764357\n",
      "Iteration 4182, loss = 0.01765071\n",
      "Iteration 4183, loss = 0.01767877\n",
      "Iteration 4184, loss = 0.01764716\n",
      "Iteration 4185, loss = 0.01761587\n",
      "Iteration 4186, loss = 0.01761659\n",
      "Iteration 4187, loss = 0.01763203\n",
      "Iteration 4188, loss = 0.01762968\n",
      "Iteration 4189, loss = 0.01759479\n",
      "Iteration 4190, loss = 0.01758856\n",
      "Iteration 4191, loss = 0.01761466\n",
      "Iteration 4192, loss = 0.01759537\n",
      "Iteration 4193, loss = 0.01756957\n",
      "Iteration 4194, loss = 0.01755570\n",
      "Iteration 4195, loss = 0.01757393\n",
      "Iteration 4196, loss = 0.01757941\n",
      "Iteration 4197, loss = 0.01754883\n",
      "Iteration 4198, loss = 0.01752906\n",
      "Iteration 4199, loss = 0.01754899\n",
      "Iteration 4200, loss = 0.01754146\n",
      "Iteration 4201, loss = 0.01752616\n",
      "Iteration 4202, loss = 0.01750047\n",
      "Iteration 4203, loss = 0.01751431\n",
      "Iteration 4204, loss = 0.01752402\n",
      "Iteration 4205, loss = 0.01750323\n",
      "Iteration 4206, loss = 0.01747507\n",
      "Iteration 4207, loss = 0.01748329\n",
      "Iteration 4208, loss = 0.01748392\n",
      "Iteration 4209, loss = 0.01748067\n",
      "Iteration 4210, loss = 0.01745128\n",
      "Iteration 4211, loss = 0.01745325\n",
      "Iteration 4212, loss = 0.01746092\n",
      "Iteration 4213, loss = 0.01745322\n",
      "Iteration 4214, loss = 0.01742864\n",
      "Iteration 4215, loss = 0.01742217\n",
      "Iteration 4216, loss = 0.01742351\n",
      "Iteration 4217, loss = 0.01742706\n",
      "Iteration 4218, loss = 0.01740589\n",
      "Iteration 4219, loss = 0.01739524\n",
      "Iteration 4220, loss = 0.01739487\n",
      "Iteration 4221, loss = 0.01739487\n",
      "Iteration 4222, loss = 0.01738373\n",
      "Iteration 4223, loss = 0.01736955\n",
      "Iteration 4224, loss = 0.01736435\n",
      "Iteration 4225, loss = 0.01736486\n",
      "Iteration 4226, loss = 0.01735735\n",
      "Iteration 4227, loss = 0.01734566\n",
      "Iteration 4228, loss = 0.01733750\n",
      "Iteration 4229, loss = 0.01733390\n",
      "Iteration 4230, loss = 0.01733056\n",
      "Iteration 4231, loss = 0.01732146\n",
      "Iteration 4232, loss = 0.01731278\n",
      "Iteration 4233, loss = 0.01730627\n",
      "Iteration 4234, loss = 0.01730222\n",
      "Iteration 4235, loss = 0.01729651\n",
      "Iteration 4236, loss = 0.01728902\n",
      "Iteration 4237, loss = 0.01728127\n",
      "Iteration 4238, loss = 0.01727548\n",
      "Iteration 4239, loss = 0.01727017\n",
      "Iteration 4240, loss = 0.01726454\n",
      "Iteration 4241, loss = 0.01725742\n",
      "Iteration 4242, loss = 0.01725061\n",
      "Iteration 4243, loss = 0.01724449\n",
      "Iteration 4244, loss = 0.01723912\n",
      "Iteration 4245, loss = 0.01723318\n",
      "Iteration 4246, loss = 0.01722666\n",
      "Iteration 4247, loss = 0.01721994\n",
      "Iteration 4248, loss = 0.01721396\n",
      "Iteration 4249, loss = 0.01720820\n",
      "Iteration 4250, loss = 0.01720238\n",
      "Iteration 4251, loss = 0.01719595\n",
      "Iteration 4252, loss = 0.01718961\n",
      "Iteration 4253, loss = 0.01718346\n",
      "Iteration 4254, loss = 0.01717767\n",
      "Iteration 4255, loss = 0.01717173\n",
      "Iteration 4256, loss = 0.01716562\n",
      "Iteration 4257, loss = 0.01715935\n",
      "Iteration 4258, loss = 0.01715327\n",
      "Iteration 4259, loss = 0.01714731\n",
      "Iteration 4260, loss = 0.01714143\n",
      "Iteration 4261, loss = 0.01713540\n",
      "Iteration 4262, loss = 0.01712931\n",
      "Iteration 4263, loss = 0.01712320\n",
      "Iteration 4264, loss = 0.01711723\n",
      "Iteration 4265, loss = 0.01711130\n",
      "Iteration 4266, loss = 0.01710537\n",
      "Iteration 4267, loss = 0.01709935\n",
      "Iteration 4268, loss = 0.01709332\n",
      "Iteration 4269, loss = 0.01708732\n",
      "Iteration 4270, loss = 0.01708139\n",
      "Iteration 4271, loss = 0.01707547\n",
      "Iteration 4272, loss = 0.01706953\n",
      "Iteration 4273, loss = 0.01706356\n",
      "Iteration 4274, loss = 0.01705759\n",
      "Iteration 4275, loss = 0.01705164\n",
      "Iteration 4276, loss = 0.01704574\n",
      "Iteration 4277, loss = 0.01703983\n",
      "Iteration 4278, loss = 0.01703392\n",
      "Iteration 4279, loss = 0.01702799\n",
      "Iteration 4280, loss = 0.01702206\n",
      "Iteration 4281, loss = 0.01701616\n",
      "Iteration 4282, loss = 0.01701027\n",
      "Iteration 4283, loss = 0.01700439\n",
      "Iteration 4284, loss = 0.01699851\n",
      "Iteration 4285, loss = 0.01699262\n",
      "Iteration 4286, loss = 0.01698673\n",
      "Iteration 4287, loss = 0.01698086\n",
      "Iteration 4288, loss = 0.01697499\n",
      "Iteration 4289, loss = 0.01696914\n",
      "Iteration 4290, loss = 0.01696328\n",
      "Iteration 4291, loss = 0.01695743\n",
      "Iteration 4292, loss = 0.01695158\n",
      "Iteration 4293, loss = 0.01694573\n",
      "Iteration 4294, loss = 0.01693990\n",
      "Iteration 4295, loss = 0.01693407\n",
      "Iteration 4296, loss = 0.01692824\n",
      "Iteration 4297, loss = 0.01692242\n",
      "Iteration 4298, loss = 0.01691660\n",
      "Iteration 4299, loss = 0.01691078\n",
      "Iteration 4300, loss = 0.01690497\n",
      "Iteration 4301, loss = 0.01689917\n",
      "Iteration 4302, loss = 0.01689337\n",
      "Iteration 4303, loss = 0.01688757\n",
      "Iteration 4304, loss = 0.01688178\n",
      "Iteration 4305, loss = 0.01687599\n",
      "Iteration 4306, loss = 0.01687021\n",
      "Iteration 4307, loss = 0.01686443\n",
      "Iteration 4308, loss = 0.01685866\n",
      "Iteration 4309, loss = 0.01685289\n",
      "Iteration 4310, loss = 0.01684712\n",
      "Iteration 4311, loss = 0.01684136\n",
      "Iteration 4312, loss = 0.01683560\n",
      "Iteration 4313, loss = 0.01682985\n",
      "Iteration 4314, loss = 0.01682410\n",
      "Iteration 4315, loss = 0.01681836\n",
      "Iteration 4316, loss = 0.01681262\n",
      "Iteration 4317, loss = 0.01680688\n",
      "Iteration 4318, loss = 0.01680115\n",
      "Iteration 4319, loss = 0.01679542\n",
      "Iteration 4320, loss = 0.01678970\n",
      "Iteration 4321, loss = 0.01678398\n",
      "Iteration 4322, loss = 0.01677826\n",
      "Iteration 4323, loss = 0.01677255\n",
      "Iteration 4324, loss = 0.01676684\n",
      "Iteration 4325, loss = 0.01676113\n",
      "Iteration 4326, loss = 0.01675543\n",
      "Iteration 4327, loss = 0.01674974\n",
      "Iteration 4328, loss = 0.01674404\n",
      "Iteration 4329, loss = 0.01673835\n",
      "Iteration 4330, loss = 0.01673267\n",
      "Iteration 4331, loss = 0.01672698\n",
      "Iteration 4332, loss = 0.01672130\n",
      "Iteration 4333, loss = 0.01671563\n",
      "Iteration 4334, loss = 0.01670996\n",
      "Iteration 4335, loss = 0.01670429\n",
      "Iteration 4336, loss = 0.01669862\n",
      "Iteration 4337, loss = 0.01669296\n",
      "Iteration 4338, loss = 0.01668730\n",
      "Iteration 4339, loss = 0.01668165\n",
      "Iteration 4340, loss = 0.01667600\n",
      "Iteration 4341, loss = 0.01667035\n",
      "Iteration 4342, loss = 0.01666470\n",
      "Iteration 4343, loss = 0.01665906\n",
      "Iteration 4344, loss = 0.01665342\n",
      "Iteration 4345, loss = 0.01664779\n",
      "Iteration 4346, loss = 0.01664215\n",
      "Iteration 4347, loss = 0.01663652\n",
      "Iteration 4348, loss = 0.01663090\n",
      "Iteration 4349, loss = 0.01662527\n",
      "Iteration 4350, loss = 0.01661965\n",
      "Iteration 4351, loss = 0.01661404\n",
      "Iteration 4352, loss = 0.01660842\n",
      "Iteration 4353, loss = 0.01660281\n",
      "Iteration 4354, loss = 0.01659720\n",
      "Iteration 4355, loss = 0.01659159\n",
      "Iteration 4356, loss = 0.01658599\n",
      "Iteration 4357, loss = 0.01658039\n",
      "Iteration 4358, loss = 0.01657479\n",
      "Iteration 4359, loss = 0.01656920\n",
      "Iteration 4360, loss = 0.01656360\n",
      "Iteration 4361, loss = 0.01655801\n",
      "Iteration 4362, loss = 0.01655242\n",
      "Iteration 4363, loss = 0.01654684\n",
      "Iteration 4364, loss = 0.01654125\n",
      "Iteration 4365, loss = 0.01653567\n",
      "Iteration 4366, loss = 0.01653009\n",
      "Iteration 4367, loss = 0.01652452\n",
      "Iteration 4368, loss = 0.01651894\n",
      "Iteration 4369, loss = 0.01651337\n",
      "Iteration 4370, loss = 0.01650780\n",
      "Iteration 4371, loss = 0.01650224\n",
      "Iteration 4372, loss = 0.01649667\n",
      "Iteration 4373, loss = 0.01649111\n",
      "Iteration 4374, loss = 0.01648554\n",
      "Iteration 4375, loss = 0.01647999\n",
      "Iteration 4376, loss = 0.01647443\n",
      "Iteration 4377, loss = 0.01646887\n",
      "Iteration 4378, loss = 0.01646332\n",
      "Iteration 4379, loss = 0.01645777\n",
      "Iteration 4380, loss = 0.01645221\n",
      "Iteration 4381, loss = 0.01644667\n",
      "Iteration 4382, loss = 0.01644112\n",
      "Iteration 4383, loss = 0.01643557\n",
      "Iteration 4384, loss = 0.01643003\n",
      "Iteration 4385, loss = 0.01642448\n",
      "Iteration 4386, loss = 0.01641894\n",
      "Iteration 4387, loss = 0.01641340\n",
      "Iteration 4388, loss = 0.01640786\n",
      "Iteration 4389, loss = 0.01640233\n",
      "Iteration 4390, loss = 0.01639679\n",
      "Iteration 4391, loss = 0.01639125\n",
      "Iteration 4392, loss = 0.01638572\n",
      "Iteration 4393, loss = 0.01638019\n",
      "Iteration 4394, loss = 0.01637465\n",
      "Iteration 4395, loss = 0.01636912\n",
      "Iteration 4396, loss = 0.01636359\n",
      "Iteration 4397, loss = 0.01635806\n",
      "Iteration 4398, loss = 0.01635253\n",
      "Iteration 4399, loss = 0.01634700\n",
      "Iteration 4400, loss = 0.01634148\n",
      "Iteration 4401, loss = 0.01633595\n",
      "Iteration 4402, loss = 0.01633042\n",
      "Iteration 4403, loss = 0.01632490\n",
      "Iteration 4404, loss = 0.01631937\n",
      "Iteration 4405, loss = 0.01631384\n",
      "Iteration 4406, loss = 0.01630832\n",
      "Iteration 4407, loss = 0.01630279\n",
      "Iteration 4408, loss = 0.01629727\n",
      "Iteration 4409, loss = 0.01629174\n",
      "Iteration 4410, loss = 0.01628622\n",
      "Iteration 4411, loss = 0.01628069\n",
      "Iteration 4412, loss = 0.01627516\n",
      "Iteration 4413, loss = 0.01626964\n",
      "Iteration 4414, loss = 0.01626411\n",
      "Iteration 4415, loss = 0.01625858\n",
      "Iteration 4416, loss = 0.01625306\n",
      "Iteration 4417, loss = 0.01624753\n",
      "Iteration 4418, loss = 0.01624200\n",
      "Iteration 4419, loss = 0.01623647\n",
      "Iteration 4420, loss = 0.01623094\n",
      "Iteration 4421, loss = 0.01622541\n",
      "Iteration 4422, loss = 0.01621987\n",
      "Iteration 4423, loss = 0.01621434\n",
      "Iteration 4424, loss = 0.01620881\n",
      "Iteration 4425, loss = 0.01620327\n",
      "Iteration 4426, loss = 0.01619773\n",
      "Iteration 4427, loss = 0.01619219\n",
      "Iteration 4428, loss = 0.01618666\n",
      "Iteration 4429, loss = 0.01618112\n",
      "Iteration 4430, loss = 0.01617558\n",
      "Iteration 4431, loss = 0.01617004\n",
      "Iteration 4432, loss = 0.01616450\n",
      "Iteration 4433, loss = 0.01615897\n",
      "Iteration 4434, loss = 0.01615345\n",
      "Iteration 4435, loss = 0.01614795\n",
      "Iteration 4436, loss = 0.01614249\n",
      "Iteration 4437, loss = 0.01613710\n",
      "Iteration 4438, loss = 0.01613181\n",
      "Iteration 4439, loss = 0.01612675\n",
      "Iteration 4440, loss = 0.01612200\n",
      "Iteration 4441, loss = 0.01611801\n",
      "Iteration 4442, loss = 0.01611480\n",
      "Iteration 4443, loss = 0.01611424\n",
      "Iteration 4444, loss = 0.01611525\n",
      "Iteration 4445, loss = 0.01612523\n",
      "Iteration 4446, loss = 0.01613465\n",
      "Iteration 4447, loss = 0.01617113\n",
      "Iteration 4448, loss = 0.01617996\n",
      "Iteration 4449, loss = 0.01624321\n",
      "Iteration 4450, loss = 0.01619885\n",
      "Iteration 4451, loss = 0.01620911\n",
      "Iteration 4452, loss = 0.01613024\n",
      "Iteration 4453, loss = 0.01609210\n",
      "Iteration 4454, loss = 0.01605470\n",
      "Iteration 4455, loss = 0.01603730\n",
      "Iteration 4456, loss = 0.01603310\n",
      "Iteration 4457, loss = 0.01603835\n",
      "Iteration 4458, loss = 0.01605336\n",
      "Iteration 4459, loss = 0.01606392\n",
      "Iteration 4460, loss = 0.01609043\n",
      "Iteration 4461, loss = 0.01607739\n",
      "Iteration 4462, loss = 0.01608133\n",
      "Iteration 4463, loss = 0.01604206\n",
      "Iteration 4464, loss = 0.01601774\n",
      "Iteration 4465, loss = 0.01599050\n",
      "Iteration 4466, loss = 0.01597538\n",
      "Iteration 4467, loss = 0.01596979\n",
      "Iteration 4468, loss = 0.01597121\n",
      "Iteration 4469, loss = 0.01597796\n",
      "Iteration 4470, loss = 0.01598151\n",
      "Iteration 4471, loss = 0.01599016\n",
      "Iteration 4472, loss = 0.01598125\n",
      "Iteration 4473, loss = 0.01597781\n",
      "Iteration 4474, loss = 0.01595730\n",
      "Iteration 4475, loss = 0.01594209\n",
      "Iteration 4476, loss = 0.01592478\n",
      "Iteration 4477, loss = 0.01591285\n",
      "Iteration 4478, loss = 0.01590547\n",
      "Iteration 4479, loss = 0.01590193\n",
      "Iteration 4480, loss = 0.01590095\n",
      "Iteration 4481, loss = 0.01590004\n",
      "Iteration 4482, loss = 0.01590066\n",
      "Iteration 4483, loss = 0.01589671\n",
      "Iteration 4484, loss = 0.01589465\n",
      "Iteration 4485, loss = 0.01588532\n",
      "Iteration 4486, loss = 0.01587785\n",
      "Iteration 4487, loss = 0.01586572\n",
      "Iteration 4488, loss = 0.01585557\n",
      "Iteration 4489, loss = 0.01584533\n",
      "Iteration 4490, loss = 0.01583688\n",
      "Iteration 4491, loss = 0.01582963\n",
      "Iteration 4492, loss = 0.01582341\n",
      "Iteration 4493, loss = 0.01581793\n",
      "Iteration 4494, loss = 0.01581297\n",
      "Iteration 4495, loss = 0.01580847\n",
      "Iteration 4496, loss = 0.01580391\n",
      "Iteration 4497, loss = 0.01579968\n",
      "Iteration 4498, loss = 0.01579461\n",
      "Iteration 4499, loss = 0.01579006\n",
      "Iteration 4500, loss = 0.01578433\n",
      "Iteration 4501, loss = 0.01577959\n",
      "Iteration 4502, loss = 0.01577355\n",
      "Iteration 4503, loss = 0.01576887\n",
      "Iteration 4504, loss = 0.01576278\n",
      "Iteration 4505, loss = 0.01575839\n",
      "Iteration 4506, loss = 0.01575235\n",
      "Iteration 4507, loss = 0.01574837\n",
      "Iteration 4508, loss = 0.01574235\n",
      "Iteration 4509, loss = 0.01573889\n",
      "Iteration 4510, loss = 0.01573299\n",
      "Iteration 4511, loss = 0.01573050\n",
      "Iteration 4512, loss = 0.01572510\n",
      "Iteration 4513, loss = 0.01572447\n",
      "Iteration 4514, loss = 0.01571986\n",
      "Iteration 4515, loss = 0.01572209\n",
      "Iteration 4516, loss = 0.01571794\n",
      "Iteration 4517, loss = 0.01572346\n",
      "Iteration 4518, loss = 0.01571796\n",
      "Iteration 4519, loss = 0.01572502\n",
      "Iteration 4520, loss = 0.01571482\n",
      "Iteration 4521, loss = 0.01571855\n",
      "Iteration 4522, loss = 0.01570120\n",
      "Iteration 4523, loss = 0.01569647\n",
      "Iteration 4524, loss = 0.01567409\n",
      "Iteration 4525, loss = 0.01566129\n",
      "Iteration 4526, loss = 0.01564008\n",
      "Iteration 4527, loss = 0.01562536\n",
      "Iteration 4528, loss = 0.01560969\n",
      "Iteration 4529, loss = 0.01559787\n",
      "Iteration 4530, loss = 0.01558749\n",
      "Iteration 4531, loss = 0.01557905\n",
      "Iteration 4532, loss = 0.01557177\n",
      "Iteration 4533, loss = 0.01556542\n",
      "Iteration 4534, loss = 0.01555985\n",
      "Iteration 4535, loss = 0.01555483\n",
      "Iteration 4536, loss = 0.01555088\n",
      "Iteration 4537, loss = 0.01554722\n",
      "Iteration 4538, loss = 0.01554595\n",
      "Iteration 4539, loss = 0.01554439\n",
      "Iteration 4540, loss = 0.01554868\n",
      "Iteration 4541, loss = 0.01555084\n",
      "Iteration 4542, loss = 0.01556691\n",
      "Iteration 4543, loss = 0.01557332\n",
      "Iteration 4544, loss = 0.01560818\n",
      "Iteration 4545, loss = 0.01560651\n",
      "Iteration 4546, loss = 0.01564292\n",
      "Iteration 4547, loss = 0.01560034\n",
      "Iteration 4548, loss = 0.01558420\n",
      "Iteration 4549, loss = 0.01551693\n",
      "Iteration 4550, loss = 0.01547489\n",
      "Iteration 4551, loss = 0.01544619\n",
      "Iteration 4552, loss = 0.01544023\n",
      "Iteration 4553, loss = 0.01544693\n",
      "Iteration 4554, loss = 0.01545138\n",
      "Iteration 4555, loss = 0.01545551\n",
      "Iteration 4556, loss = 0.01543868\n",
      "Iteration 4557, loss = 0.01542417\n",
      "Iteration 4558, loss = 0.01540417\n",
      "Iteration 4559, loss = 0.01539350\n",
      "Iteration 4560, loss = 0.01538530\n",
      "Iteration 4561, loss = 0.01538353\n",
      "Iteration 4562, loss = 0.01537751\n",
      "Iteration 4563, loss = 0.01537057\n",
      "Iteration 4564, loss = 0.01535899\n",
      "Iteration 4565, loss = 0.01534412\n",
      "Iteration 4566, loss = 0.01532947\n",
      "Iteration 4567, loss = 0.01531478\n",
      "Iteration 4568, loss = 0.01530299\n",
      "Iteration 4569, loss = 0.01529334\n",
      "Iteration 4570, loss = 0.01528623\n",
      "Iteration 4571, loss = 0.01527989\n",
      "Iteration 4572, loss = 0.01527496\n",
      "Iteration 4573, loss = 0.01526802\n",
      "Iteration 4574, loss = 0.01526244\n",
      "Iteration 4575, loss = 0.01525304\n",
      "Iteration 4576, loss = 0.01524577\n",
      "Iteration 4577, loss = 0.01523504\n",
      "Iteration 4578, loss = 0.01522807\n",
      "Iteration 4579, loss = 0.01522027\n",
      "Iteration 4580, loss = 0.01522153\n",
      "Iteration 4581, loss = 0.01522691\n",
      "Iteration 4582, loss = 0.01526459\n",
      "Iteration 4583, loss = 0.01530029\n",
      "Iteration 4584, loss = 0.01544078\n",
      "Iteration 4585, loss = 0.01542682\n",
      "Iteration 4586, loss = 0.01552137\n",
      "Iteration 4587, loss = 0.01530439\n",
      "Iteration 4588, loss = 0.01517198\n",
      "Iteration 4589, loss = 0.01512668\n",
      "Iteration 4590, loss = 0.01518369\n",
      "Iteration 4591, loss = 0.01525779\n",
      "Iteration 4592, loss = 0.01519985\n",
      "Iteration 4593, loss = 0.01511974\n",
      "Iteration 4594, loss = 0.01505287\n",
      "Iteration 4595, loss = 0.01506773\n",
      "Iteration 4596, loss = 0.01512319\n",
      "Iteration 4597, loss = 0.01510532\n",
      "Iteration 4598, loss = 0.01505826\n",
      "Iteration 4599, loss = 0.01500535\n",
      "Iteration 4600, loss = 0.01499909\n",
      "Iteration 4601, loss = 0.01502504\n",
      "Iteration 4602, loss = 0.01502327\n",
      "Iteration 4603, loss = 0.01499760\n",
      "Iteration 4604, loss = 0.01495461\n",
      "Iteration 4605, loss = 0.01493654\n",
      "Iteration 4606, loss = 0.01494025\n",
      "Iteration 4607, loss = 0.01494355\n",
      "Iteration 4608, loss = 0.01493460\n",
      "Iteration 4609, loss = 0.01490512\n",
      "Iteration 4610, loss = 0.01487871\n",
      "Iteration 4611, loss = 0.01486419\n",
      "Iteration 4612, loss = 0.01486014\n",
      "Iteration 4613, loss = 0.01486026\n",
      "Iteration 4614, loss = 0.01485041\n",
      "Iteration 4615, loss = 0.01483716\n",
      "Iteration 4616, loss = 0.01481626\n",
      "Iteration 4617, loss = 0.01479989\n",
      "Iteration 4618, loss = 0.01478789\n",
      "Iteration 4619, loss = 0.01478258\n",
      "Iteration 4620, loss = 0.01477742\n",
      "Iteration 4621, loss = 0.01477378\n",
      "Iteration 4622, loss = 0.01476458\n",
      "Iteration 4623, loss = 0.01475730\n",
      "Iteration 4624, loss = 0.01474384\n",
      "Iteration 4625, loss = 0.01473928\n",
      "Iteration 4626, loss = 0.01472742\n",
      "Iteration 4627, loss = 0.01473024\n",
      "Iteration 4628, loss = 0.01472224\n",
      "Iteration 4629, loss = 0.01473579\n",
      "Iteration 4630, loss = 0.01472717\n",
      "Iteration 4631, loss = 0.01475064\n",
      "Iteration 4632, loss = 0.01473232\n",
      "Iteration 4633, loss = 0.01475410\n",
      "Iteration 4634, loss = 0.01471101\n",
      "Iteration 4635, loss = 0.01470330\n",
      "Iteration 4636, loss = 0.01464603\n",
      "Iteration 4637, loss = 0.01461287\n",
      "Iteration 4638, loss = 0.01457260\n",
      "Iteration 4639, loss = 0.01454819\n",
      "Iteration 4640, loss = 0.01453137\n",
      "Iteration 4641, loss = 0.01452177\n",
      "Iteration 4642, loss = 0.01451542\n",
      "Iteration 4643, loss = 0.01450707\n",
      "Iteration 4644, loss = 0.01450021\n",
      "Iteration 4645, loss = 0.01448782\n",
      "Iteration 4646, loss = 0.01447852\n",
      "Iteration 4647, loss = 0.01446597\n",
      "Iteration 4648, loss = 0.01445852\n",
      "Iteration 4649, loss = 0.01444855\n",
      "Iteration 4650, loss = 0.01444479\n",
      "Iteration 4651, loss = 0.01443481\n",
      "Iteration 4652, loss = 0.01443182\n",
      "Iteration 4653, loss = 0.01441785\n",
      "Iteration 4654, loss = 0.01441142\n",
      "Iteration 4655, loss = 0.01439324\n",
      "Iteration 4656, loss = 0.01438233\n",
      "Iteration 4657, loss = 0.01436303\n",
      "Iteration 4658, loss = 0.01435064\n",
      "Iteration 4659, loss = 0.01433490\n",
      "Iteration 4660, loss = 0.01432679\n",
      "Iteration 4661, loss = 0.01431880\n",
      "Iteration 4662, loss = 0.01432381\n",
      "Iteration 4663, loss = 0.01432664\n",
      "Iteration 4664, loss = 0.01435810\n",
      "Iteration 4665, loss = 0.01436209\n",
      "Iteration 4666, loss = 0.01441529\n",
      "Iteration 4667, loss = 0.01437370\n",
      "Iteration 4668, loss = 0.01437317\n",
      "Iteration 4669, loss = 0.01428924\n",
      "Iteration 4670, loss = 0.01424171\n",
      "Iteration 4671, loss = 0.01420814\n",
      "Iteration 4672, loss = 0.01421042\n",
      "Iteration 4673, loss = 0.01422222\n",
      "Iteration 4674, loss = 0.01422473\n",
      "Iteration 4675, loss = 0.01421103\n",
      "Iteration 4676, loss = 0.01417140\n",
      "Iteration 4677, loss = 0.01413312\n",
      "Iteration 4678, loss = 0.01410946\n",
      "Iteration 4679, loss = 0.01410168\n",
      "Iteration 4680, loss = 0.01410443\n",
      "Iteration 4681, loss = 0.01409901\n",
      "Iteration 4682, loss = 0.01408864\n",
      "Iteration 4683, loss = 0.01406275\n",
      "Iteration 4684, loss = 0.01403951\n",
      "Iteration 4685, loss = 0.01402207\n",
      "Iteration 4686, loss = 0.01401299\n",
      "Iteration 4687, loss = 0.01400862\n",
      "Iteration 4688, loss = 0.01400256\n",
      "Iteration 4689, loss = 0.01399344\n",
      "Iteration 4690, loss = 0.01397773\n",
      "Iteration 4691, loss = 0.01396113\n",
      "Iteration 4692, loss = 0.01394492\n",
      "Iteration 4693, loss = 0.01393179\n",
      "Iteration 4694, loss = 0.01392173\n",
      "Iteration 4695, loss = 0.01391339\n",
      "Iteration 4696, loss = 0.01390523\n",
      "Iteration 4697, loss = 0.01389518\n",
      "Iteration 4698, loss = 0.01388385\n",
      "Iteration 4699, loss = 0.01387060\n",
      "Iteration 4700, loss = 0.01385725\n",
      "Iteration 4701, loss = 0.01384428\n",
      "Iteration 4702, loss = 0.01383241\n",
      "Iteration 4703, loss = 0.01382165\n",
      "Iteration 4704, loss = 0.01381163\n",
      "Iteration 4705, loss = 0.01380192\n",
      "Iteration 4706, loss = 0.01379190\n",
      "Iteration 4707, loss = 0.01378170\n",
      "Iteration 4708, loss = 0.01377093\n",
      "Iteration 4709, loss = 0.01376048\n",
      "Iteration 4710, loss = 0.01375030\n",
      "Iteration 4711, loss = 0.01374197\n",
      "Iteration 4712, loss = 0.01373553\n",
      "Iteration 4713, loss = 0.01373501\n",
      "Iteration 4714, loss = 0.01373851\n",
      "Iteration 4715, loss = 0.01376103\n",
      "Iteration 4716, loss = 0.01378358\n",
      "Iteration 4717, loss = 0.01386073\n",
      "Iteration 4718, loss = 0.01387989\n",
      "Iteration 4719, loss = 0.01399430\n",
      "Iteration 4720, loss = 0.01390068\n",
      "Iteration 4721, loss = 0.01388818\n",
      "Iteration 4722, loss = 0.01374383\n",
      "Iteration 4723, loss = 0.01366841\n",
      "Iteration 4724, loss = 0.01361270\n",
      "Iteration 4725, loss = 0.01359620\n",
      "Iteration 4726, loss = 0.01360515\n",
      "Iteration 4727, loss = 0.01362461\n",
      "Iteration 4728, loss = 0.01365958\n",
      "Iteration 4729, loss = 0.01365475\n",
      "Iteration 4730, loss = 0.01366453\n",
      "Iteration 4731, loss = 0.01361124\n",
      "Iteration 4732, loss = 0.01357551\n",
      "Iteration 4733, loss = 0.01353150\n",
      "Iteration 4734, loss = 0.01350765\n",
      "Iteration 4735, loss = 0.01349508\n",
      "Iteration 4736, loss = 0.01349262\n",
      "Iteration 4737, loss = 0.01349530\n",
      "Iteration 4738, loss = 0.01349126\n",
      "Iteration 4739, loss = 0.01349017\n",
      "Iteration 4740, loss = 0.01347266\n",
      "Iteration 4741, loss = 0.01345918\n",
      "Iteration 4742, loss = 0.01343538\n",
      "Iteration 4743, loss = 0.01341688\n",
      "Iteration 4744, loss = 0.01339801\n",
      "Iteration 4745, loss = 0.01338400\n",
      "Iteration 4746, loss = 0.01337273\n",
      "Iteration 4747, loss = 0.01336448\n",
      "Iteration 4748, loss = 0.01335914\n",
      "Iteration 4749, loss = 0.01335278\n",
      "Iteration 4750, loss = 0.01334758\n",
      "Iteration 4751, loss = 0.01333720\n",
      "Iteration 4752, loss = 0.01332713\n",
      "Iteration 4753, loss = 0.01331153\n",
      "Iteration 4754, loss = 0.01329660\n",
      "Iteration 4755, loss = 0.01328042\n",
      "Iteration 4756, loss = 0.01326622\n",
      "Iteration 4757, loss = 0.01325404\n",
      "Iteration 4758, loss = 0.01324402\n",
      "Iteration 4759, loss = 0.01323552\n",
      "Iteration 4760, loss = 0.01322770\n",
      "Iteration 4761, loss = 0.01322015\n",
      "Iteration 4762, loss = 0.01321188\n",
      "Iteration 4763, loss = 0.01320358\n",
      "Iteration 4764, loss = 0.01319407\n",
      "Iteration 4765, loss = 0.01318497\n",
      "Iteration 4766, loss = 0.01317515\n",
      "Iteration 4767, loss = 0.01316638\n",
      "Iteration 4768, loss = 0.01315708\n",
      "Iteration 4769, loss = 0.01314900\n",
      "Iteration 4770, loss = 0.01314008\n",
      "Iteration 4771, loss = 0.01313243\n",
      "Iteration 4772, loss = 0.01312361\n",
      "Iteration 4773, loss = 0.01311644\n",
      "Iteration 4774, loss = 0.01310777\n",
      "Iteration 4775, loss = 0.01310142\n",
      "Iteration 4776, loss = 0.01309327\n",
      "Iteration 4777, loss = 0.01308866\n",
      "Iteration 4778, loss = 0.01308136\n",
      "Iteration 4779, loss = 0.01307933\n",
      "Iteration 4780, loss = 0.01307301\n",
      "Iteration 4781, loss = 0.01307436\n",
      "Iteration 4782, loss = 0.01306793\n",
      "Iteration 4783, loss = 0.01307219\n",
      "Iteration 4784, loss = 0.01306320\n",
      "Iteration 4785, loss = 0.01306736\n",
      "Iteration 4786, loss = 0.01305248\n",
      "Iteration 4787, loss = 0.01305171\n",
      "Iteration 4788, loss = 0.01303020\n",
      "Iteration 4789, loss = 0.01302139\n",
      "Iteration 4790, loss = 0.01299677\n",
      "Iteration 4791, loss = 0.01298185\n",
      "Iteration 4792, loss = 0.01295892\n",
      "Iteration 4793, loss = 0.01294229\n",
      "Iteration 4794, loss = 0.01292383\n",
      "Iteration 4795, loss = 0.01290901\n",
      "Iteration 4796, loss = 0.01289505\n",
      "Iteration 4797, loss = 0.01288312\n",
      "Iteration 4798, loss = 0.01287243\n",
      "Iteration 4799, loss = 0.01286285\n",
      "Iteration 4800, loss = 0.01285410\n",
      "Iteration 4801, loss = 0.01284596\n",
      "Iteration 4802, loss = 0.01283830\n",
      "Iteration 4803, loss = 0.01283095\n",
      "Iteration 4804, loss = 0.01282406\n",
      "Iteration 4805, loss = 0.01281725\n",
      "Iteration 4806, loss = 0.01281123\n",
      "Iteration 4807, loss = 0.01280504\n",
      "Iteration 4808, loss = 0.01280041\n",
      "Iteration 4809, loss = 0.01279520\n",
      "Iteration 4810, loss = 0.01279297\n",
      "Iteration 4811, loss = 0.01278921\n",
      "Iteration 4812, loss = 0.01279089\n",
      "Iteration 4813, loss = 0.01278868\n",
      "Iteration 4814, loss = 0.01279576\n",
      "Iteration 4815, loss = 0.01279366\n",
      "Iteration 4816, loss = 0.01280570\n",
      "Iteration 4817, loss = 0.01279907\n",
      "Iteration 4818, loss = 0.01281041\n",
      "Iteration 4819, loss = 0.01279274\n",
      "Iteration 4820, loss = 0.01279321\n",
      "Iteration 4821, loss = 0.01276338\n",
      "Iteration 4822, loss = 0.01274836\n",
      "Iteration 4823, loss = 0.01271540\n",
      "Iteration 4824, loss = 0.01269286\n",
      "Iteration 4825, loss = 0.01266702\n",
      "Iteration 4826, loss = 0.01264791\n",
      "Iteration 4827, loss = 0.01263199\n",
      "Iteration 4828, loss = 0.01262026\n",
      "Iteration 4829, loss = 0.01261171\n",
      "Iteration 4830, loss = 0.01260555\n",
      "Iteration 4831, loss = 0.01260108\n",
      "Iteration 4832, loss = 0.01259725\n",
      "Iteration 4833, loss = 0.01259448\n",
      "Iteration 4834, loss = 0.01259040\n",
      "Iteration 4835, loss = 0.01258777\n",
      "Iteration 4836, loss = 0.01258201\n",
      "Iteration 4837, loss = 0.01257852\n",
      "Iteration 4838, loss = 0.01257061\n",
      "Iteration 4839, loss = 0.01256568\n",
      "Iteration 4840, loss = 0.01255588\n",
      "Iteration 4841, loss = 0.01254935\n",
      "Iteration 4842, loss = 0.01253834\n",
      "Iteration 4843, loss = 0.01253050\n",
      "Iteration 4844, loss = 0.01251910\n",
      "Iteration 4845, loss = 0.01251052\n",
      "Iteration 4846, loss = 0.01249943\n",
      "Iteration 4847, loss = 0.01249068\n",
      "Iteration 4848, loss = 0.01248030\n",
      "Iteration 4849, loss = 0.01247180\n",
      "Iteration 4850, loss = 0.01246226\n",
      "Iteration 4851, loss = 0.01245425\n",
      "Iteration 4852, loss = 0.01244559\n",
      "Iteration 4853, loss = 0.01243825\n",
      "Iteration 4854, loss = 0.01243052\n",
      "Iteration 4855, loss = 0.01242408\n",
      "Iteration 4856, loss = 0.01241747\n",
      "Iteration 4857, loss = 0.01241233\n",
      "Iteration 4858, loss = 0.01240733\n",
      "Iteration 4859, loss = 0.01240425\n",
      "Iteration 4860, loss = 0.01240182\n",
      "Iteration 4861, loss = 0.01240202\n",
      "Iteration 4862, loss = 0.01240367\n",
      "Iteration 4863, loss = 0.01240860\n",
      "Iteration 4864, loss = 0.01241576\n",
      "Iteration 4865, loss = 0.01242562\n",
      "Iteration 4866, loss = 0.01243674\n",
      "Iteration 4867, loss = 0.01244677\n",
      "Iteration 4868, loss = 0.01245191\n",
      "Iteration 4869, loss = 0.01245103\n",
      "Iteration 4870, loss = 0.01243480\n",
      "Iteration 4871, loss = 0.01241671\n",
      "Iteration 4872, loss = 0.01238086\n",
      "Iteration 4873, loss = 0.01235626\n",
      "Iteration 4874, loss = 0.01232435\n",
      "Iteration 4875, loss = 0.01230832\n",
      "Iteration 4876, loss = 0.01229228\n",
      "Iteration 4877, loss = 0.01228578\n",
      "Iteration 4878, loss = 0.01227451\n",
      "Iteration 4879, loss = 0.01226583\n",
      "Iteration 4880, loss = 0.01224953\n",
      "Iteration 4881, loss = 0.01223459\n",
      "Iteration 4882, loss = 0.01221932\n",
      "Iteration 4883, loss = 0.01220828\n",
      "Iteration 4884, loss = 0.01220199\n",
      "Iteration 4885, loss = 0.01219938\n",
      "Iteration 4886, loss = 0.01219941\n",
      "Iteration 4887, loss = 0.01219767\n",
      "Iteration 4888, loss = 0.01219613\n",
      "Iteration 4889, loss = 0.01218884\n",
      "Iteration 4890, loss = 0.01218224\n",
      "Iteration 4891, loss = 0.01217137\n",
      "Iteration 4892, loss = 0.01216305\n",
      "Iteration 4893, loss = 0.01215357\n",
      "Iteration 4894, loss = 0.01214741\n",
      "Iteration 4895, loss = 0.01214066\n",
      "Iteration 4896, loss = 0.01213625\n",
      "Iteration 4897, loss = 0.01213009\n",
      "Iteration 4898, loss = 0.01212497\n",
      "Iteration 4899, loss = 0.01211747\n",
      "Iteration 4900, loss = 0.01211091\n",
      "Iteration 4901, loss = 0.01210231\n",
      "Iteration 4902, loss = 0.01209557\n",
      "Iteration 4903, loss = 0.01208730\n",
      "Iteration 4904, loss = 0.01208164\n",
      "Iteration 4905, loss = 0.01207438\n",
      "Iteration 4906, loss = 0.01206994\n",
      "Iteration 4907, loss = 0.01206322\n",
      "Iteration 4908, loss = 0.01205937\n",
      "Iteration 4909, loss = 0.01205243\n",
      "Iteration 4910, loss = 0.01204859\n",
      "Iteration 4911, loss = 0.01204114\n",
      "Iteration 4912, loss = 0.01203717\n",
      "Iteration 4913, loss = 0.01202936\n",
      "Iteration 4914, loss = 0.01202541\n",
      "Iteration 4915, loss = 0.01201750\n",
      "Iteration 4916, loss = 0.01201369\n",
      "Iteration 4917, loss = 0.01200572\n",
      "Iteration 4918, loss = 0.01200187\n",
      "Iteration 4919, loss = 0.01199362\n",
      "Iteration 4920, loss = 0.01198932\n",
      "Iteration 4921, loss = 0.01198049\n",
      "Iteration 4922, loss = 0.01197528\n",
      "Iteration 4923, loss = 0.01196573\n",
      "Iteration 4924, loss = 0.01195937\n",
      "Iteration 4925, loss = 0.01194922\n",
      "Iteration 4926, loss = 0.01194179\n",
      "Iteration 4927, loss = 0.01193137\n",
      "Iteration 4928, loss = 0.01192319\n",
      "Iteration 4929, loss = 0.01191290\n",
      "Iteration 4930, loss = 0.01190443\n",
      "Iteration 4931, loss = 0.01189463\n",
      "Iteration 4932, loss = 0.01188625\n",
      "Iteration 4933, loss = 0.01187712\n",
      "Iteration 4934, loss = 0.01186910\n",
      "Iteration 4935, loss = 0.01186069\n",
      "Iteration 4936, loss = 0.01185312\n",
      "Iteration 4937, loss = 0.01184535\n",
      "Iteration 4938, loss = 0.01183824\n",
      "Iteration 4939, loss = 0.01183102\n",
      "Iteration 4940, loss = 0.01182433\n",
      "Iteration 4941, loss = 0.01181757\n",
      "Iteration 4942, loss = 0.01181131\n",
      "Iteration 4943, loss = 0.01180500\n",
      "Iteration 4944, loss = 0.01179926\n",
      "Iteration 4945, loss = 0.01179350\n",
      "Iteration 4946, loss = 0.01178850\n",
      "Iteration 4947, loss = 0.01178351\n",
      "Iteration 4948, loss = 0.01177973\n",
      "Iteration 4949, loss = 0.01177598\n",
      "Iteration 4950, loss = 0.01177428\n",
      "Iteration 4951, loss = 0.01177257\n",
      "Iteration 4952, loss = 0.01177449\n",
      "Iteration 4953, loss = 0.01177594\n",
      "Iteration 4954, loss = 0.01178380\n",
      "Iteration 4955, loss = 0.01178944\n",
      "Iteration 4956, loss = 0.01180559\n",
      "Iteration 4957, loss = 0.01181457\n",
      "Iteration 4958, loss = 0.01183806\n",
      "Iteration 4959, loss = 0.01184442\n",
      "Iteration 4960, loss = 0.01186472\n",
      "Iteration 4961, loss = 0.01185707\n",
      "Iteration 4962, loss = 0.01185423\n",
      "Iteration 4963, loss = 0.01182519\n",
      "Iteration 4964, loss = 0.01179029\n",
      "Iteration 4965, loss = 0.01174796\n",
      "Iteration 4966, loss = 0.01170357\n",
      "Iteration 4967, loss = 0.01167052\n",
      "Iteration 4968, loss = 0.01164886\n",
      "Iteration 4969, loss = 0.01164229\n",
      "Iteration 4970, loss = 0.01164623\n",
      "Iteration 4971, loss = 0.01165693\n",
      "Iteration 4972, loss = 0.01166394\n",
      "Iteration 4973, loss = 0.01166858\n",
      "Iteration 4974, loss = 0.01166056\n",
      "Iteration 4975, loss = 0.01164775\n",
      "Iteration 4976, loss = 0.01162683\n",
      "Iteration 4977, loss = 0.01160611\n",
      "Iteration 4978, loss = 0.01158722\n",
      "Iteration 4979, loss = 0.01157363\n",
      "Iteration 4980, loss = 0.01156566\n",
      "Iteration 4981, loss = 0.01156228\n",
      "Iteration 4982, loss = 0.01156153\n",
      "Iteration 4983, loss = 0.01156120\n",
      "Iteration 4984, loss = 0.01155988\n",
      "Iteration 4985, loss = 0.01155593\n",
      "Iteration 4986, loss = 0.01155007\n",
      "Iteration 4987, loss = 0.01154142\n",
      "Iteration 4988, loss = 0.01153207\n",
      "Iteration 4989, loss = 0.01152165\n",
      "Iteration 4990, loss = 0.01151199\n",
      "Iteration 4991, loss = 0.01150291\n",
      "Iteration 4992, loss = 0.01149511\n",
      "Iteration 4993, loss = 0.01148835\n",
      "Iteration 4994, loss = 0.01148252\n",
      "Iteration 4995, loss = 0.01147721\n",
      "Iteration 4996, loss = 0.01147225\n",
      "Iteration 4997, loss = 0.01146716\n",
      "Iteration 4998, loss = 0.01146202\n",
      "Iteration 4999, loss = 0.01145645\n",
      "Iteration 5000, loss = 0.01145071\n",
      "Iteration 5001, loss = 0.01144457\n",
      "Iteration 5002, loss = 0.01143829\n",
      "Iteration 5003, loss = 0.01143180\n",
      "Iteration 5004, loss = 0.01142527\n",
      "Iteration 5005, loss = 0.01141873\n",
      "Iteration 5006, loss = 0.01141223\n",
      "Iteration 5007, loss = 0.01140584\n",
      "Iteration 5008, loss = 0.01139952\n",
      "Iteration 5009, loss = 0.01139335\n",
      "Iteration 5010, loss = 0.01138727\n",
      "Iteration 5011, loss = 0.01138134\n",
      "Iteration 5012, loss = 0.01137549\n",
      "Iteration 5013, loss = 0.01136982\n",
      "Iteration 5014, loss = 0.01136423\n",
      "Iteration 5015, loss = 0.01135887\n",
      "Iteration 5016, loss = 0.01135363\n",
      "Iteration 5017, loss = 0.01134876\n",
      "Iteration 5018, loss = 0.01134407\n",
      "Iteration 5019, loss = 0.01134006\n",
      "Iteration 5020, loss = 0.01133635\n",
      "Iteration 5021, loss = 0.01133399\n",
      "Iteration 5022, loss = 0.01133205\n",
      "Iteration 5023, loss = 0.01133287\n",
      "Iteration 5024, loss = 0.01133401\n",
      "Iteration 5025, loss = 0.01134077\n",
      "Iteration 5026, loss = 0.01134660\n",
      "Iteration 5027, loss = 0.01136328\n",
      "Iteration 5028, loss = 0.01137370\n",
      "Iteration 5029, loss = 0.01140257\n",
      "Iteration 5030, loss = 0.01141013\n",
      "Iteration 5031, loss = 0.01144191\n",
      "Iteration 5032, loss = 0.01142766\n",
      "Iteration 5033, loss = 0.01143501\n",
      "Iteration 5034, loss = 0.01138800\n",
      "Iteration 5035, loss = 0.01135706\n",
      "Iteration 5036, loss = 0.01130351\n",
      "Iteration 5037, loss = 0.01126729\n",
      "Iteration 5038, loss = 0.01124078\n",
      "Iteration 5039, loss = 0.01123032\n",
      "Iteration 5040, loss = 0.01123144\n",
      "Iteration 5041, loss = 0.01123593\n",
      "Iteration 5042, loss = 0.01124227\n",
      "Iteration 5043, loss = 0.01123754\n",
      "Iteration 5044, loss = 0.01123049\n",
      "Iteration 5045, loss = 0.01121340\n",
      "Iteration 5046, loss = 0.01119791\n",
      "Iteration 5047, loss = 0.01118376\n",
      "Iteration 5048, loss = 0.01117555\n",
      "Iteration 5049, loss = 0.01117226\n",
      "Iteration 5050, loss = 0.01117215\n",
      "Iteration 5051, loss = 0.01117321\n",
      "Iteration 5052, loss = 0.01117066\n",
      "Iteration 5053, loss = 0.01116658\n",
      "Iteration 5054, loss = 0.01115670\n",
      "Iteration 5055, loss = 0.01114623\n",
      "Iteration 5056, loss = 0.01113436\n",
      "Iteration 5057, loss = 0.01112442\n",
      "Iteration 5058, loss = 0.01111689\n",
      "Iteration 5059, loss = 0.01111157\n",
      "Iteration 5060, loss = 0.01110795\n",
      "Iteration 5061, loss = 0.01110420\n",
      "Iteration 5062, loss = 0.01110020\n",
      "Iteration 5063, loss = 0.01109434\n",
      "Iteration 5064, loss = 0.01108790\n",
      "Iteration 5065, loss = 0.01108023\n",
      "Iteration 5066, loss = 0.01107289\n",
      "Iteration 5067, loss = 0.01106580\n",
      "Iteration 5068, loss = 0.01105975\n",
      "Iteration 5069, loss = 0.01105451\n",
      "Iteration 5070, loss = 0.01105018\n",
      "Iteration 5071, loss = 0.01104641\n",
      "Iteration 5072, loss = 0.01104298\n",
      "Iteration 5073, loss = 0.01103994\n",
      "Iteration 5074, loss = 0.01103702\n",
      "Iteration 5075, loss = 0.01103477\n",
      "Iteration 5076, loss = 0.01103324\n",
      "Iteration 5077, loss = 0.01103299\n",
      "Iteration 5078, loss = 0.01103504\n",
      "Iteration 5079, loss = 0.01103878\n",
      "Iteration 5080, loss = 0.01104777\n",
      "Iteration 5081, loss = 0.01105738\n",
      "Iteration 5082, loss = 0.01107680\n",
      "Iteration 5083, loss = 0.01109121\n",
      "Iteration 5084, loss = 0.01112017\n",
      "Iteration 5085, loss = 0.01112953\n",
      "Iteration 5086, loss = 0.01115318\n",
      "Iteration 5087, loss = 0.01113803\n",
      "Iteration 5088, loss = 0.01112936\n",
      "Iteration 5089, loss = 0.01108284\n",
      "Iteration 5090, loss = 0.01103943\n",
      "Iteration 5091, loss = 0.01098844\n",
      "Iteration 5092, loss = 0.01094926\n",
      "Iteration 5093, loss = 0.01092538\n",
      "Iteration 5094, loss = 0.01091758\n",
      "Iteration 5095, loss = 0.01092244\n",
      "Iteration 5096, loss = 0.01093282\n",
      "Iteration 5097, loss = 0.01094478\n",
      "Iteration 5098, loss = 0.01094849\n",
      "Iteration 5099, loss = 0.01094736\n",
      "Iteration 5100, loss = 0.01093488\n",
      "Iteration 5101, loss = 0.01091836\n",
      "Iteration 5102, loss = 0.01089831\n",
      "Iteration 5103, loss = 0.01087990\n",
      "Iteration 5104, loss = 0.01086573\n",
      "Iteration 5105, loss = 0.01085682\n",
      "Iteration 5106, loss = 0.01085281\n",
      "Iteration 5107, loss = 0.01085196\n",
      "Iteration 5108, loss = 0.01085245\n",
      "Iteration 5109, loss = 0.01085191\n",
      "Iteration 5110, loss = 0.01084986\n",
      "Iteration 5111, loss = 0.01084490\n",
      "Iteration 5112, loss = 0.01083794\n",
      "Iteration 5113, loss = 0.01082905\n",
      "Iteration 5114, loss = 0.01081945\n",
      "Iteration 5115, loss = 0.01080991\n",
      "Iteration 5116, loss = 0.01080119\n",
      "Iteration 5117, loss = 0.01079373\n",
      "Iteration 5118, loss = 0.01078760\n",
      "Iteration 5119, loss = 0.01078262\n",
      "Iteration 5120, loss = 0.01077844\n",
      "Iteration 5121, loss = 0.01077468\n",
      "Iteration 5122, loss = 0.01077096\n",
      "Iteration 5123, loss = 0.01076704\n",
      "Iteration 5124, loss = 0.01076272\n",
      "Iteration 5125, loss = 0.01075800\n",
      "Iteration 5126, loss = 0.01075282\n",
      "Iteration 5127, loss = 0.01074731\n",
      "Iteration 5128, loss = 0.01074151\n",
      "Iteration 5129, loss = 0.01073559\n",
      "Iteration 5130, loss = 0.01072958\n",
      "Iteration 5131, loss = 0.01072361\n",
      "Iteration 5132, loss = 0.01071769\n",
      "Iteration 5133, loss = 0.01071189\n",
      "Iteration 5134, loss = 0.01070620\n",
      "Iteration 5135, loss = 0.01070062\n",
      "Iteration 5136, loss = 0.01069515\n",
      "Iteration 5137, loss = 0.01068977\n",
      "Iteration 5138, loss = 0.01068447\n",
      "Iteration 5139, loss = 0.01067923\n",
      "Iteration 5140, loss = 0.01067404\n",
      "Iteration 5141, loss = 0.01066888\n",
      "Iteration 5142, loss = 0.01066376\n",
      "Iteration 5143, loss = 0.01065865\n",
      "Iteration 5144, loss = 0.01065356\n",
      "Iteration 5145, loss = 0.01064849\n",
      "Iteration 5146, loss = 0.01064343\n",
      "Iteration 5147, loss = 0.01063839\n",
      "Iteration 5148, loss = 0.01063336\n",
      "Iteration 5149, loss = 0.01062835\n",
      "Iteration 5150, loss = 0.01062337\n",
      "Iteration 5151, loss = 0.01061841\n",
      "Iteration 5152, loss = 0.01061349\n",
      "Iteration 5153, loss = 0.01060863\n",
      "Iteration 5154, loss = 0.01060385\n",
      "Iteration 5155, loss = 0.01059918\n",
      "Iteration 5156, loss = 0.01059469\n",
      "Iteration 5157, loss = 0.01059043\n",
      "Iteration 5158, loss = 0.01058655\n",
      "Iteration 5159, loss = 0.01058320\n",
      "Iteration 5160, loss = 0.01058073\n",
      "Iteration 5161, loss = 0.01057945\n",
      "Iteration 5162, loss = 0.01058022\n",
      "Iteration 5163, loss = 0.01058373\n",
      "Iteration 5164, loss = 0.01059199\n",
      "Iteration 5165, loss = 0.01060619\n",
      "Iteration 5166, loss = 0.01063074\n",
      "Iteration 5167, loss = 0.01066593\n",
      "Iteration 5168, loss = 0.01071939\n",
      "Iteration 5169, loss = 0.01078029\n",
      "Iteration 5170, loss = 0.01085677\n",
      "Iteration 5171, loss = 0.01089674\n",
      "Iteration 5172, loss = 0.01091993\n",
      "Iteration 5173, loss = 0.01083644\n",
      "Iteration 5174, loss = 0.01073955\n",
      "Iteration 5175, loss = 0.01060493\n",
      "Iteration 5176, loss = 0.01053006\n",
      "Iteration 5177, loss = 0.01051524\n",
      "Iteration 5178, loss = 0.01054794\n",
      "Iteration 5179, loss = 0.01058379\n",
      "Iteration 5180, loss = 0.01059711\n",
      "Iteration 5181, loss = 0.01058630\n",
      "Iteration 5182, loss = 0.01054853\n",
      "Iteration 5183, loss = 0.01052239\n",
      "Iteration 5184, loss = 0.01049987\n",
      "Iteration 5185, loss = 0.01049102\n",
      "Iteration 5186, loss = 0.01048052\n",
      "Iteration 5187, loss = 0.01047199\n",
      "Iteration 5188, loss = 0.01046499\n",
      "Iteration 5189, loss = 0.01046201\n",
      "Iteration 5190, loss = 0.01046461\n",
      "Iteration 5191, loss = 0.01046093\n",
      "Iteration 5192, loss = 0.01045288\n",
      "Iteration 5193, loss = 0.01043439\n",
      "Iteration 5194, loss = 0.01041656\n",
      "Iteration 5195, loss = 0.01040458\n",
      "Iteration 5196, loss = 0.01040222\n",
      "Iteration 5197, loss = 0.01040640\n",
      "Iteration 5198, loss = 0.01040915\n",
      "Iteration 5199, loss = 0.01040697\n",
      "Iteration 5200, loss = 0.01039595\n",
      "Iteration 5201, loss = 0.01038250\n",
      "Iteration 5202, loss = 0.01037026\n",
      "Iteration 5203, loss = 0.01036302\n",
      "Iteration 5204, loss = 0.01036040\n",
      "Iteration 5205, loss = 0.01035958\n",
      "Iteration 5206, loss = 0.01035785\n",
      "Iteration 5207, loss = 0.01035311\n",
      "Iteration 5208, loss = 0.01034659\n",
      "Iteration 5209, loss = 0.01033911\n",
      "Iteration 5210, loss = 0.01033252\n",
      "Iteration 5211, loss = 0.01032700\n",
      "Iteration 5212, loss = 0.01032227\n",
      "Iteration 5213, loss = 0.01031765\n",
      "Iteration 5214, loss = 0.01031269\n",
      "Iteration 5215, loss = 0.01030754\n",
      "Iteration 5216, loss = 0.01030244\n",
      "Iteration 5217, loss = 0.01029771\n",
      "Iteration 5218, loss = 0.01029329\n",
      "Iteration 5219, loss = 0.01028898\n",
      "Iteration 5220, loss = 0.01028436\n",
      "Iteration 5221, loss = 0.01027931\n",
      "Iteration 5222, loss = 0.01027387\n",
      "Iteration 5223, loss = 0.01026832\n",
      "Iteration 5224, loss = 0.01026297\n",
      "Iteration 5225, loss = 0.01025800\n",
      "Iteration 5226, loss = 0.01025343\n",
      "Iteration 5227, loss = 0.01024912\n",
      "Iteration 5228, loss = 0.01024483\n",
      "Iteration 5229, loss = 0.01024041\n",
      "Iteration 5230, loss = 0.01023576\n",
      "Iteration 5231, loss = 0.01023091\n",
      "Iteration 5232, loss = 0.01022597\n",
      "Iteration 5233, loss = 0.01022102\n",
      "Iteration 5234, loss = 0.01021615\n",
      "Iteration 5235, loss = 0.01021137\n",
      "Iteration 5236, loss = 0.01020666\n",
      "Iteration 5237, loss = 0.01020199\n",
      "Iteration 5238, loss = 0.01019732\n",
      "Iteration 5239, loss = 0.01019263\n",
      "Iteration 5240, loss = 0.01018792\n",
      "Iteration 5241, loss = 0.01018323\n",
      "Iteration 5242, loss = 0.01017855\n",
      "Iteration 5243, loss = 0.01017391\n",
      "Iteration 5244, loss = 0.01016930\n",
      "Iteration 5245, loss = 0.01016472\n",
      "Iteration 5246, loss = 0.01016014\n",
      "Iteration 5247, loss = 0.01015555\n",
      "Iteration 5248, loss = 0.01015095\n",
      "Iteration 5249, loss = 0.01014633\n",
      "Iteration 5250, loss = 0.01014169\n",
      "Iteration 5251, loss = 0.01013705\n",
      "Iteration 5252, loss = 0.01013241\n",
      "Iteration 5253, loss = 0.01012778\n",
      "Iteration 5254, loss = 0.01012316\n",
      "Iteration 5255, loss = 0.01011855\n",
      "Iteration 5256, loss = 0.01011396\n",
      "Iteration 5257, loss = 0.01010938\n",
      "Iteration 5258, loss = 0.01010481\n",
      "Iteration 5259, loss = 0.01010024\n",
      "Iteration 5260, loss = 0.01009568\n",
      "Iteration 5261, loss = 0.01009113\n",
      "Iteration 5262, loss = 0.01008659\n",
      "Iteration 5263, loss = 0.01008207\n",
      "Iteration 5264, loss = 0.01007757\n",
      "Iteration 5265, loss = 0.01007309\n",
      "Iteration 5266, loss = 0.01006865\n",
      "Iteration 5267, loss = 0.01006427\n",
      "Iteration 5268, loss = 0.01005994\n",
      "Iteration 5269, loss = 0.01005572\n",
      "Iteration 5270, loss = 0.01005161\n",
      "Iteration 5271, loss = 0.01004770\n",
      "Iteration 5272, loss = 0.01004400\n",
      "Iteration 5273, loss = 0.01004074\n",
      "Iteration 5274, loss = 0.01003787\n",
      "Iteration 5275, loss = 0.01003596\n",
      "Iteration 5276, loss = 0.01003479\n",
      "Iteration 5277, loss = 0.01003575\n",
      "Iteration 5278, loss = 0.01003804\n",
      "Iteration 5279, loss = 0.01004506\n",
      "Iteration 5280, loss = 0.01005388\n",
      "Iteration 5281, loss = 0.01007296\n",
      "Iteration 5282, loss = 0.01009183\n",
      "Iteration 5283, loss = 0.01013047\n",
      "Iteration 5284, loss = 0.01015610\n",
      "Iteration 5285, loss = 0.01021036\n",
      "Iteration 5286, loss = 0.01021566\n",
      "Iteration 5287, loss = 0.01024368\n",
      "Iteration 5288, loss = 0.01018998\n",
      "Iteration 5289, loss = 0.01014656\n",
      "Iteration 5290, loss = 0.01006553\n",
      "Iteration 5291, loss = 0.01000741\n",
      "Iteration 5292, loss = 0.00997536\n",
      "Iteration 5293, loss = 0.00997300\n",
      "Iteration 5294, loss = 0.00999335\n",
      "Iteration 5295, loss = 0.01001373\n",
      "Iteration 5296, loss = 0.01003185\n",
      "Iteration 5297, loss = 0.01001850\n",
      "Iteration 5298, loss = 0.00999789\n",
      "Iteration 5299, loss = 0.00996012\n",
      "Iteration 5300, loss = 0.00993030\n",
      "Iteration 5301, loss = 0.00991277\n",
      "Iteration 5302, loss = 0.00991062\n",
      "Iteration 5303, loss = 0.00991898\n",
      "Iteration 5304, loss = 0.00992834\n",
      "Iteration 5305, loss = 0.00993428\n",
      "Iteration 5306, loss = 0.00992779\n",
      "Iteration 5307, loss = 0.00991626\n",
      "Iteration 5308, loss = 0.00989887\n",
      "Iteration 5309, loss = 0.00988416\n",
      "Iteration 5310, loss = 0.00987412\n",
      "Iteration 5311, loss = 0.00986986\n",
      "Iteration 5312, loss = 0.00986957\n",
      "Iteration 5313, loss = 0.00987009\n",
      "Iteration 5314, loss = 0.00986946\n",
      "Iteration 5315, loss = 0.00986491\n",
      "Iteration 5316, loss = 0.00985812\n",
      "Iteration 5317, loss = 0.00984897\n",
      "Iteration 5318, loss = 0.00984019\n",
      "Iteration 5319, loss = 0.00983275\n",
      "Iteration 5320, loss = 0.00982743\n",
      "Iteration 5321, loss = 0.00982398\n",
      "Iteration 5322, loss = 0.00982157\n",
      "Iteration 5323, loss = 0.00981932\n",
      "Iteration 5324, loss = 0.00981624\n",
      "Iteration 5325, loss = 0.00981230\n",
      "Iteration 5326, loss = 0.00980715\n",
      "Iteration 5327, loss = 0.00980154\n",
      "Iteration 5328, loss = 0.00979570\n",
      "Iteration 5329, loss = 0.00979024\n",
      "Iteration 5330, loss = 0.00978536\n",
      "Iteration 5331, loss = 0.00978113\n",
      "Iteration 5332, loss = 0.00977749\n",
      "Iteration 5333, loss = 0.00977414\n",
      "Iteration 5334, loss = 0.00977100\n",
      "Iteration 5335, loss = 0.00976772\n",
      "Iteration 5336, loss = 0.00976451\n",
      "Iteration 5337, loss = 0.00976108\n",
      "Iteration 5338, loss = 0.00975801\n",
      "Iteration 5339, loss = 0.00975497\n",
      "Iteration 5340, loss = 0.00975287\n",
      "Iteration 5341, loss = 0.00975114\n",
      "Iteration 5342, loss = 0.00975114\n",
      "Iteration 5343, loss = 0.00975161\n",
      "Iteration 5344, loss = 0.00975476\n",
      "Iteration 5345, loss = 0.00975786\n",
      "Iteration 5346, loss = 0.00976444\n",
      "Iteration 5347, loss = 0.00976891\n",
      "Iteration 5348, loss = 0.00977643\n",
      "Iteration 5349, loss = 0.00977747\n",
      "Iteration 5350, loss = 0.00977870\n",
      "Iteration 5351, loss = 0.00976946\n",
      "Iteration 5352, loss = 0.00975747\n",
      "Iteration 5353, loss = 0.00973837\n",
      "Iteration 5354, loss = 0.00971886\n",
      "Iteration 5355, loss = 0.00970161\n",
      "Iteration 5356, loss = 0.00968849\n",
      "Iteration 5357, loss = 0.00968120\n",
      "Iteration 5358, loss = 0.00967706\n",
      "Iteration 5359, loss = 0.00967498\n",
      "Iteration 5360, loss = 0.00967163\n",
      "Iteration 5361, loss = 0.00966707\n",
      "Iteration 5362, loss = 0.00965995\n",
      "Iteration 5363, loss = 0.00965226\n",
      "Iteration 5364, loss = 0.00964455\n",
      "Iteration 5365, loss = 0.00963834\n",
      "Iteration 5366, loss = 0.00963402\n",
      "Iteration 5367, loss = 0.00963133\n",
      "Iteration 5368, loss = 0.00962980\n",
      "Iteration 5369, loss = 0.00962809\n",
      "Iteration 5370, loss = 0.00962612\n",
      "Iteration 5371, loss = 0.00962251\n",
      "Iteration 5372, loss = 0.00961831\n",
      "Iteration 5373, loss = 0.00961262\n",
      "Iteration 5374, loss = 0.00960693\n",
      "Iteration 5375, loss = 0.00960078\n",
      "Iteration 5376, loss = 0.00959528\n",
      "Iteration 5377, loss = 0.00959007\n",
      "Iteration 5378, loss = 0.00958562\n",
      "Iteration 5379, loss = 0.00958154\n",
      "Iteration 5380, loss = 0.00957790\n",
      "Iteration 5381, loss = 0.00957437\n",
      "Iteration 5382, loss = 0.00957093\n",
      "Iteration 5383, loss = 0.00956742\n",
      "Iteration 5384, loss = 0.00956382\n",
      "Iteration 5385, loss = 0.00956017\n",
      "Iteration 5386, loss = 0.00955652\n",
      "Iteration 5387, loss = 0.00955294\n",
      "Iteration 5388, loss = 0.00954965\n",
      "Iteration 5389, loss = 0.00954656\n",
      "Iteration 5390, loss = 0.00954419\n",
      "Iteration 5391, loss = 0.00954211\n",
      "Iteration 5392, loss = 0.00954132\n",
      "Iteration 5393, loss = 0.00954076\n",
      "Iteration 5394, loss = 0.00954239\n",
      "Iteration 5395, loss = 0.00954387\n",
      "Iteration 5396, loss = 0.00954896\n",
      "Iteration 5397, loss = 0.00955280\n",
      "Iteration 5398, loss = 0.00956220\n",
      "Iteration 5399, loss = 0.00956773\n",
      "Iteration 5400, loss = 0.00958083\n",
      "Iteration 5401, loss = 0.00958504\n",
      "Iteration 5402, loss = 0.00959750\n",
      "Iteration 5403, loss = 0.00959453\n",
      "Iteration 5404, loss = 0.00959777\n",
      "Iteration 5405, loss = 0.00958248\n",
      "Iteration 5406, loss = 0.00956999\n",
      "Iteration 5407, loss = 0.00954486\n",
      "Iteration 5408, loss = 0.00952142\n",
      "Iteration 5409, loss = 0.00949649\n",
      "Iteration 5410, loss = 0.00947521\n",
      "Iteration 5411, loss = 0.00945922\n",
      "Iteration 5412, loss = 0.00944839\n",
      "Iteration 5413, loss = 0.00944289\n",
      "Iteration 5414, loss = 0.00944100\n",
      "Iteration 5415, loss = 0.00944178\n",
      "Iteration 5416, loss = 0.00944260\n",
      "Iteration 5417, loss = 0.00944387\n",
      "Iteration 5418, loss = 0.00944237\n",
      "Iteration 5419, loss = 0.00944017\n",
      "Iteration 5420, loss = 0.00943478\n",
      "Iteration 5421, loss = 0.00942851\n",
      "Iteration 5422, loss = 0.00942053\n",
      "Iteration 5423, loss = 0.00941220\n",
      "Iteration 5424, loss = 0.00940388\n",
      "Iteration 5425, loss = 0.00939593\n",
      "Iteration 5426, loss = 0.00938888\n",
      "Iteration 5427, loss = 0.00938263\n",
      "Iteration 5428, loss = 0.00937735\n",
      "Iteration 5429, loss = 0.00937282\n",
      "Iteration 5430, loss = 0.00936897\n",
      "Iteration 5431, loss = 0.00936553\n",
      "Iteration 5432, loss = 0.00936243\n",
      "Iteration 5433, loss = 0.00935936\n",
      "Iteration 5434, loss = 0.00935640\n",
      "Iteration 5435, loss = 0.00935323\n",
      "Iteration 5436, loss = 0.00935006\n",
      "Iteration 5437, loss = 0.00934662\n",
      "Iteration 5438, loss = 0.00934315\n",
      "Iteration 5439, loss = 0.00933947\n",
      "Iteration 5440, loss = 0.00933576\n",
      "Iteration 5441, loss = 0.00933194\n",
      "Iteration 5442, loss = 0.00932812\n",
      "Iteration 5443, loss = 0.00932428\n",
      "Iteration 5444, loss = 0.00932046\n",
      "Iteration 5445, loss = 0.00931670\n",
      "Iteration 5446, loss = 0.00931297\n",
      "Iteration 5447, loss = 0.00930935\n",
      "Iteration 5448, loss = 0.00930579\n",
      "Iteration 5449, loss = 0.00930240\n",
      "Iteration 5450, loss = 0.00929910\n",
      "Iteration 5451, loss = 0.00929602\n",
      "Iteration 5452, loss = 0.00929307\n",
      "Iteration 5453, loss = 0.00929040\n",
      "Iteration 5454, loss = 0.00928796\n",
      "Iteration 5455, loss = 0.00928587\n",
      "Iteration 5456, loss = 0.00928415\n",
      "Iteration 5457, loss = 0.00928284\n",
      "Iteration 5458, loss = 0.00928217\n",
      "Iteration 5459, loss = 0.00928190\n",
      "Iteration 5460, loss = 0.00928269\n",
      "Iteration 5461, loss = 0.00928368\n",
      "Iteration 5462, loss = 0.00928642\n",
      "Iteration 5463, loss = 0.00928867\n",
      "Iteration 5464, loss = 0.00929369\n",
      "Iteration 5465, loss = 0.00929655\n",
      "Iteration 5466, loss = 0.00930345\n",
      "Iteration 5467, loss = 0.00930488\n",
      "Iteration 5468, loss = 0.00931146\n",
      "Iteration 5469, loss = 0.00930772\n",
      "Iteration 5470, loss = 0.00930939\n",
      "Iteration 5471, loss = 0.00929666\n",
      "Iteration 5472, loss = 0.00928868\n",
      "Iteration 5473, loss = 0.00926744\n",
      "Iteration 5474, loss = 0.00925060\n",
      "Iteration 5475, loss = 0.00922787\n",
      "Iteration 5476, loss = 0.00921010\n",
      "Iteration 5477, loss = 0.00919409\n",
      "Iteration 5478, loss = 0.00918320\n",
      "Iteration 5479, loss = 0.00917648\n",
      "Iteration 5480, loss = 0.00917353\n",
      "Iteration 5481, loss = 0.00917313\n",
      "Iteration 5482, loss = 0.00917419\n",
      "Iteration 5483, loss = 0.00917568\n",
      "Iteration 5484, loss = 0.00917641\n",
      "Iteration 5485, loss = 0.00917647\n",
      "Iteration 5486, loss = 0.00917439\n",
      "Iteration 5487, loss = 0.00917146\n",
      "Iteration 5488, loss = 0.00916621\n",
      "Iteration 5489, loss = 0.00916042\n",
      "Iteration 5490, loss = 0.00915317\n",
      "Iteration 5491, loss = 0.00914589\n",
      "Iteration 5492, loss = 0.00913835\n",
      "Iteration 5493, loss = 0.00913121\n",
      "Iteration 5494, loss = 0.00912461\n",
      "Iteration 5495, loss = 0.00911861\n",
      "Iteration 5496, loss = 0.00911338\n",
      "Iteration 5497, loss = 0.00910873\n",
      "Iteration 5498, loss = 0.00910475\n",
      "Iteration 5499, loss = 0.00910119\n",
      "Iteration 5500, loss = 0.00909814\n",
      "Iteration 5501, loss = 0.00909531\n",
      "Iteration 5502, loss = 0.00909294\n",
      "Iteration 5503, loss = 0.00909061\n",
      "Iteration 5504, loss = 0.00908884\n",
      "Iteration 5505, loss = 0.00908696\n",
      "Iteration 5506, loss = 0.00908591\n",
      "Iteration 5507, loss = 0.00908460\n",
      "Iteration 5508, loss = 0.00908457\n",
      "Iteration 5509, loss = 0.00908405\n",
      "Iteration 5510, loss = 0.00908546\n",
      "Iteration 5511, loss = 0.00908590\n",
      "Iteration 5512, loss = 0.00908901\n",
      "Iteration 5513, loss = 0.00909015\n",
      "Iteration 5514, loss = 0.00909454\n",
      "Iteration 5515, loss = 0.00909501\n",
      "Iteration 5516, loss = 0.00909863\n",
      "Iteration 5517, loss = 0.00909552\n",
      "Iteration 5518, loss = 0.00909446\n",
      "Iteration 5519, loss = 0.00908468\n",
      "Iteration 5520, loss = 0.00907581\n",
      "Iteration 5521, loss = 0.00905995\n",
      "Iteration 5522, loss = 0.00904567\n",
      "Iteration 5523, loss = 0.00902971\n",
      "Iteration 5524, loss = 0.00901728\n",
      "Iteration 5525, loss = 0.00900740\n",
      "Iteration 5526, loss = 0.00900149\n",
      "Iteration 5527, loss = 0.00899835\n",
      "Iteration 5528, loss = 0.00899731\n",
      "Iteration 5529, loss = 0.00899710\n",
      "Iteration 5530, loss = 0.00899651\n",
      "Iteration 5531, loss = 0.00899519\n",
      "Iteration 5532, loss = 0.00899204\n",
      "Iteration 5533, loss = 0.00898784\n",
      "Iteration 5534, loss = 0.00898207\n",
      "Iteration 5535, loss = 0.00897594\n",
      "Iteration 5536, loss = 0.00896958\n",
      "Iteration 5537, loss = 0.00896380\n",
      "Iteration 5538, loss = 0.00895903\n",
      "Iteration 5539, loss = 0.00895528\n",
      "Iteration 5540, loss = 0.00895305\n",
      "Iteration 5541, loss = 0.00895157\n",
      "Iteration 5542, loss = 0.00895179\n",
      "Iteration 5543, loss = 0.00895206\n",
      "Iteration 5544, loss = 0.00895455\n",
      "Iteration 5545, loss = 0.00895614\n",
      "Iteration 5546, loss = 0.00896122\n",
      "Iteration 5547, loss = 0.00896401\n",
      "Iteration 5548, loss = 0.00897227\n",
      "Iteration 5549, loss = 0.00897568\n",
      "Iteration 5550, loss = 0.00898677\n",
      "Iteration 5551, loss = 0.00898874\n",
      "Iteration 5552, loss = 0.00899972\n",
      "Iteration 5553, loss = 0.00899663\n",
      "Iteration 5554, loss = 0.00900195\n",
      "Iteration 5555, loss = 0.00899147\n",
      "Iteration 5556, loss = 0.00898721\n",
      "Iteration 5557, loss = 0.00897218\n",
      "Iteration 5558, loss = 0.00896079\n",
      "Iteration 5559, loss = 0.00894681\n",
      "Iteration 5560, loss = 0.00893352\n",
      "Iteration 5561, loss = 0.00892168\n",
      "Iteration 5562, loss = 0.00890782\n",
      "Iteration 5563, loss = 0.00889532\n",
      "Iteration 5564, loss = 0.00888138\n",
      "Iteration 5565, loss = 0.00886972\n",
      "Iteration 5566, loss = 0.00886001\n",
      "Iteration 5567, loss = 0.00885433\n",
      "Iteration 5568, loss = 0.00885160\n",
      "Iteration 5569, loss = 0.00885188\n",
      "Iteration 5570, loss = 0.00885266\n",
      "Iteration 5571, loss = 0.00885317\n",
      "Iteration 5572, loss = 0.00885162\n",
      "Iteration 5573, loss = 0.00884743\n",
      "Iteration 5574, loss = 0.00884082\n",
      "Iteration 5575, loss = 0.00883212\n",
      "Iteration 5576, loss = 0.00882282\n",
      "Iteration 5577, loss = 0.00881382\n",
      "Iteration 5578, loss = 0.00880619\n",
      "Iteration 5579, loss = 0.00880033\n",
      "Iteration 5580, loss = 0.00879618\n",
      "Iteration 5581, loss = 0.00879338\n",
      "Iteration 5582, loss = 0.00879133\n",
      "Iteration 5583, loss = 0.00878953\n",
      "Iteration 5584, loss = 0.00878748\n",
      "Iteration 5585, loss = 0.00878501\n",
      "Iteration 5586, loss = 0.00878193\n",
      "Iteration 5587, loss = 0.00877840\n",
      "Iteration 5588, loss = 0.00877448\n",
      "Iteration 5589, loss = 0.00877042\n",
      "Iteration 5590, loss = 0.00876639\n",
      "Iteration 5591, loss = 0.00876246\n",
      "Iteration 5592, loss = 0.00875885\n",
      "Iteration 5593, loss = 0.00875541\n",
      "Iteration 5594, loss = 0.00875236\n",
      "Iteration 5595, loss = 0.00874936\n",
      "Iteration 5596, loss = 0.00874677\n",
      "Iteration 5597, loss = 0.00874407\n",
      "Iteration 5598, loss = 0.00874184\n",
      "Iteration 5599, loss = 0.00873937\n",
      "Iteration 5600, loss = 0.00873753\n",
      "Iteration 5601, loss = 0.00873533\n",
      "Iteration 5602, loss = 0.00873407\n",
      "Iteration 5603, loss = 0.00873231\n",
      "Iteration 5604, loss = 0.00873194\n",
      "Iteration 5605, loss = 0.00873085\n",
      "Iteration 5606, loss = 0.00873181\n",
      "Iteration 5607, loss = 0.00873162\n",
      "Iteration 5608, loss = 0.00873438\n",
      "Iteration 5609, loss = 0.00873522\n",
      "Iteration 5610, loss = 0.00874016\n",
      "Iteration 5611, loss = 0.00874188\n",
      "Iteration 5612, loss = 0.00874893\n",
      "Iteration 5613, loss = 0.00875088\n",
      "Iteration 5614, loss = 0.00875909\n",
      "Iteration 5615, loss = 0.00876025\n",
      "Iteration 5616, loss = 0.00876765\n",
      "Iteration 5617, loss = 0.00876729\n",
      "Iteration 5618, loss = 0.00877150\n",
      "Iteration 5619, loss = 0.00876931\n",
      "Iteration 5620, loss = 0.00876746\n",
      "Iteration 5621, loss = 0.00876097\n",
      "Iteration 5622, loss = 0.00874841\n",
      "Iteration 5623, loss = 0.00873157\n",
      "Iteration 5624, loss = 0.00870665\n",
      "Iteration 5625, loss = 0.00868117\n",
      "Iteration 5626, loss = 0.00865671\n",
      "Iteration 5627, loss = 0.00864005\n",
      "Iteration 5628, loss = 0.00863286\n",
      "Iteration 5629, loss = 0.00863467\n",
      "Iteration 5630, loss = 0.00864144\n",
      "Iteration 5631, loss = 0.00864914\n",
      "Iteration 5632, loss = 0.00865286\n",
      "Iteration 5633, loss = 0.00865108\n",
      "Iteration 5634, loss = 0.00864303\n",
      "Iteration 5635, loss = 0.00863157\n",
      "Iteration 5636, loss = 0.00861878\n",
      "Iteration 5637, loss = 0.00860829\n",
      "Iteration 5638, loss = 0.00860073\n",
      "Iteration 5639, loss = 0.00859667\n",
      "Iteration 5640, loss = 0.00859453\n",
      "Iteration 5641, loss = 0.00859318\n",
      "Iteration 5642, loss = 0.00859095\n",
      "Iteration 5643, loss = 0.00858755\n",
      "Iteration 5644, loss = 0.00858256\n",
      "Iteration 5645, loss = 0.00857681\n",
      "Iteration 5646, loss = 0.00857080\n",
      "Iteration 5647, loss = 0.00856533\n",
      "Iteration 5648, loss = 0.00856081\n",
      "Iteration 5649, loss = 0.00855735\n",
      "Iteration 5650, loss = 0.00855478\n",
      "Iteration 5651, loss = 0.00855272\n",
      "Iteration 5652, loss = 0.00855087\n",
      "Iteration 5653, loss = 0.00854878\n",
      "Iteration 5654, loss = 0.00854647\n",
      "Iteration 5655, loss = 0.00854363\n",
      "Iteration 5656, loss = 0.00854060\n",
      "Iteration 5657, loss = 0.00853718\n",
      "Iteration 5658, loss = 0.00853390\n",
      "Iteration 5659, loss = 0.00853053\n",
      "Iteration 5660, loss = 0.00852765\n",
      "Iteration 5661, loss = 0.00852489\n",
      "Iteration 5662, loss = 0.00852291\n",
      "Iteration 5663, loss = 0.00852107\n",
      "Iteration 5664, loss = 0.00852035\n",
      "Iteration 5665, loss = 0.00851965\n",
      "Iteration 5666, loss = 0.00852062\n",
      "Iteration 5667, loss = 0.00852134\n",
      "Iteration 5668, loss = 0.00852467\n",
      "Iteration 5669, loss = 0.00852708\n",
      "Iteration 5670, loss = 0.00853359\n",
      "Iteration 5671, loss = 0.00853756\n",
      "Iteration 5672, loss = 0.00854754\n",
      "Iteration 5673, loss = 0.00855153\n",
      "Iteration 5674, loss = 0.00856319\n",
      "Iteration 5675, loss = 0.00856313\n",
      "Iteration 5676, loss = 0.00857090\n",
      "Iteration 5677, loss = 0.00856130\n",
      "Iteration 5678, loss = 0.00855779\n",
      "Iteration 5679, loss = 0.00853728\n",
      "Iteration 5680, loss = 0.00852129\n",
      "Iteration 5681, loss = 0.00849722\n",
      "Iteration 5682, loss = 0.00847810\n",
      "Iteration 5683, loss = 0.00846088\n",
      "Iteration 5684, loss = 0.00844938\n",
      "Iteration 5685, loss = 0.00844312\n",
      "Iteration 5686, loss = 0.00844143\n",
      "Iteration 5687, loss = 0.00844322\n",
      "Iteration 5688, loss = 0.00844661\n",
      "Iteration 5689, loss = 0.00845136\n",
      "Iteration 5690, loss = 0.00845436\n",
      "Iteration 5691, loss = 0.00845802\n",
      "Iteration 5692, loss = 0.00845778\n",
      "Iteration 5693, loss = 0.00845858\n",
      "Iteration 5694, loss = 0.00845543\n",
      "Iteration 5695, loss = 0.00845416\n",
      "Iteration 5696, loss = 0.00845030\n",
      "Iteration 5697, loss = 0.00844884\n",
      "Iteration 5698, loss = 0.00844559\n",
      "Iteration 5699, loss = 0.00844413\n",
      "Iteration 5700, loss = 0.00844009\n",
      "Iteration 5701, loss = 0.00843622\n",
      "Iteration 5702, loss = 0.00842905\n",
      "Iteration 5703, loss = 0.00842111\n",
      "Iteration 5704, loss = 0.00841150\n",
      "Iteration 5705, loss = 0.00840206\n",
      "Iteration 5706, loss = 0.00839411\n",
      "Iteration 5707, loss = 0.00838740\n",
      "Iteration 5708, loss = 0.00838355\n",
      "Iteration 5709, loss = 0.00838006\n",
      "Iteration 5710, loss = 0.00837827\n",
      "Iteration 5711, loss = 0.00837503\n",
      "Iteration 5712, loss = 0.00837192\n",
      "Iteration 5713, loss = 0.00836667\n",
      "Iteration 5714, loss = 0.00836107\n",
      "Iteration 5715, loss = 0.00835422\n",
      "Iteration 5716, loss = 0.00834754\n",
      "Iteration 5717, loss = 0.00834100\n",
      "Iteration 5718, loss = 0.00833527\n",
      "Iteration 5719, loss = 0.00833042\n",
      "Iteration 5720, loss = 0.00832653\n",
      "Iteration 5721, loss = 0.00832346\n",
      "Iteration 5722, loss = 0.00832101\n",
      "Iteration 5723, loss = 0.00831898\n",
      "Iteration 5724, loss = 0.00831710\n",
      "Iteration 5725, loss = 0.00831533\n",
      "Iteration 5726, loss = 0.00831335\n",
      "Iteration 5727, loss = 0.00831139\n",
      "Iteration 5728, loss = 0.00830905\n",
      "Iteration 5729, loss = 0.00830679\n",
      "Iteration 5730, loss = 0.00830412\n",
      "Iteration 5731, loss = 0.00830167\n",
      "Iteration 5732, loss = 0.00829883\n",
      "Iteration 5733, loss = 0.00829638\n",
      "Iteration 5734, loss = 0.00829357\n",
      "Iteration 5735, loss = 0.00829132\n",
      "Iteration 5736, loss = 0.00828873\n",
      "Iteration 5737, loss = 0.00828691\n",
      "Iteration 5738, loss = 0.00828475\n",
      "Iteration 5739, loss = 0.00828368\n",
      "Iteration 5740, loss = 0.00828222\n",
      "Iteration 5741, loss = 0.00828235\n",
      "Iteration 5742, loss = 0.00828200\n",
      "Iteration 5743, loss = 0.00828409\n",
      "Iteration 5744, loss = 0.00828554\n",
      "Iteration 5745, loss = 0.00829085\n",
      "Iteration 5746, loss = 0.00829528\n",
      "Iteration 5747, loss = 0.00830591\n",
      "Iteration 5748, loss = 0.00831537\n",
      "Iteration 5749, loss = 0.00833451\n",
      "Iteration 5750, loss = 0.00835228\n",
      "Iteration 5751, loss = 0.00838319\n",
      "Iteration 5752, loss = 0.00841143\n",
      "Iteration 5753, loss = 0.00844877\n",
      "Iteration 5754, loss = 0.00847304\n",
      "Iteration 5755, loss = 0.00847937\n",
      "Iteration 5756, loss = 0.00844843\n",
      "Iteration 5757, loss = 0.00837922\n",
      "Iteration 5758, loss = 0.00829375\n",
      "Iteration 5759, loss = 0.00822773\n",
      "Iteration 5760, loss = 0.00820612\n",
      "Iteration 5761, loss = 0.00822602\n",
      "Iteration 5762, loss = 0.00826031\n",
      "Iteration 5763, loss = 0.00827952\n",
      "Iteration 5764, loss = 0.00826894\n",
      "Iteration 5765, loss = 0.00823898\n",
      "Iteration 5766, loss = 0.00821398\n",
      "Iteration 5767, loss = 0.00820706\n",
      "Iteration 5768, loss = 0.00821756\n",
      "Iteration 5769, loss = 0.00822600\n",
      "Iteration 5770, loss = 0.00822227\n",
      "Iteration 5771, loss = 0.00820248\n",
      "Iteration 5772, loss = 0.00818034\n",
      "Iteration 5773, loss = 0.00816720\n",
      "Iteration 5774, loss = 0.00816699\n",
      "Iteration 5775, loss = 0.00817358\n",
      "Iteration 5776, loss = 0.00817752\n",
      "Iteration 5777, loss = 0.00817421\n",
      "Iteration 5778, loss = 0.00816481\n",
      "Iteration 5779, loss = 0.00815582\n",
      "Iteration 5780, loss = 0.00815138\n",
      "Iteration 5781, loss = 0.00815202\n",
      "Iteration 5782, loss = 0.00815384\n",
      "Iteration 5783, loss = 0.00815328\n",
      "Iteration 5784, loss = 0.00814842\n",
      "Iteration 5785, loss = 0.00814104\n",
      "Iteration 5786, loss = 0.00813370\n",
      "Iteration 5787, loss = 0.00812880\n",
      "Iteration 5788, loss = 0.00812631\n",
      "Iteration 5789, loss = 0.00812502\n",
      "Iteration 5790, loss = 0.00812300\n",
      "Iteration 5791, loss = 0.00811951\n",
      "Iteration 5792, loss = 0.00811467\n",
      "Iteration 5793, loss = 0.00810964\n",
      "Iteration 5794, loss = 0.00810538\n",
      "Iteration 5795, loss = 0.00810234\n",
      "Iteration 5796, loss = 0.00810020\n",
      "Iteration 5797, loss = 0.00809828\n",
      "Iteration 5798, loss = 0.00809597\n",
      "Iteration 5799, loss = 0.00809299\n",
      "Iteration 5800, loss = 0.00808950\n",
      "Iteration 5801, loss = 0.00808587\n",
      "Iteration 5802, loss = 0.00808250\n",
      "Iteration 5803, loss = 0.00807957\n",
      "Iteration 5804, loss = 0.00807704\n",
      "Iteration 5805, loss = 0.00807470\n",
      "Iteration 5806, loss = 0.00807234\n",
      "Iteration 5807, loss = 0.00806979\n",
      "Iteration 5808, loss = 0.00806703\n",
      "Iteration 5809, loss = 0.00806414\n",
      "Iteration 5810, loss = 0.00806128\n",
      "Iteration 5811, loss = 0.00805857\n",
      "Iteration 5812, loss = 0.00805614\n",
      "Iteration 5813, loss = 0.00805398\n",
      "Iteration 5814, loss = 0.00805220\n",
      "Iteration 5815, loss = 0.00805066\n",
      "Iteration 5816, loss = 0.00804967\n",
      "Iteration 5817, loss = 0.00804898\n",
      "Iteration 5818, loss = 0.00804941\n",
      "Iteration 5819, loss = 0.00805041\n",
      "Iteration 5820, loss = 0.00805387\n",
      "Iteration 5821, loss = 0.00805819\n",
      "Iteration 5822, loss = 0.00806768\n",
      "Iteration 5823, loss = 0.00807756\n",
      "Iteration 5824, loss = 0.00809761\n",
      "Iteration 5825, loss = 0.00811408\n",
      "Iteration 5826, loss = 0.00814797\n",
      "Iteration 5827, loss = 0.00816432\n",
      "Iteration 5828, loss = 0.00820301\n",
      "Iteration 5829, loss = 0.00819671\n",
      "Iteration 5830, loss = 0.00820761\n",
      "Iteration 5831, loss = 0.00815956\n",
      "Iteration 5832, loss = 0.00812224\n",
      "Iteration 5833, loss = 0.00806192\n",
      "Iteration 5834, loss = 0.00801955\n",
      "Iteration 5835, loss = 0.00799420\n",
      "Iteration 5836, loss = 0.00799004\n",
      "Iteration 5837, loss = 0.00800173\n",
      "Iteration 5838, loss = 0.00801926\n",
      "Iteration 5839, loss = 0.00803783\n",
      "Iteration 5840, loss = 0.00804157\n",
      "Iteration 5841, loss = 0.00804034\n",
      "Iteration 5842, loss = 0.00802140\n",
      "Iteration 5843, loss = 0.00800194\n",
      "Iteration 5844, loss = 0.00798080\n",
      "Iteration 5845, loss = 0.00796683\n",
      "Iteration 5846, loss = 0.00796102\n",
      "Iteration 5847, loss = 0.00796232\n",
      "Iteration 5848, loss = 0.00796764\n",
      "Iteration 5849, loss = 0.00797243\n",
      "Iteration 5850, loss = 0.00797553\n",
      "Iteration 5851, loss = 0.00797265\n",
      "Iteration 5852, loss = 0.00796744\n",
      "Iteration 5853, loss = 0.00795819\n",
      "Iteration 5854, loss = 0.00794924\n",
      "Iteration 5855, loss = 0.00794105\n",
      "Iteration 5856, loss = 0.00793531\n",
      "Iteration 5857, loss = 0.00793210\n",
      "Iteration 5858, loss = 0.00793095\n",
      "Iteration 5859, loss = 0.00793099\n",
      "Iteration 5860, loss = 0.00793109\n",
      "Iteration 5861, loss = 0.00793080\n",
      "Iteration 5862, loss = 0.00792919\n",
      "Iteration 5863, loss = 0.00792696\n",
      "Iteration 5864, loss = 0.00792375\n",
      "Iteration 5865, loss = 0.00792071\n",
      "Iteration 5866, loss = 0.00791810\n",
      "Iteration 5867, loss = 0.00791678\n",
      "Iteration 5868, loss = 0.00791738\n",
      "Iteration 5869, loss = 0.00792043\n",
      "Iteration 5870, loss = 0.00792698\n",
      "Iteration 5871, loss = 0.00793714\n",
      "Iteration 5872, loss = 0.00795286\n",
      "Iteration 5873, loss = 0.00797215\n",
      "Iteration 5874, loss = 0.00799716\n",
      "Iteration 5875, loss = 0.00801839\n",
      "Iteration 5876, loss = 0.00803516\n",
      "Iteration 5877, loss = 0.00802877\n",
      "Iteration 5878, loss = 0.00800231\n",
      "Iteration 5879, loss = 0.00795381\n",
      "Iteration 5880, loss = 0.00790534\n",
      "Iteration 5881, loss = 0.00787494\n",
      "Iteration 5882, loss = 0.00787184\n",
      "Iteration 5883, loss = 0.00788886\n",
      "Iteration 5884, loss = 0.00790800\n",
      "Iteration 5885, loss = 0.00791449\n",
      "Iteration 5886, loss = 0.00790123\n",
      "Iteration 5887, loss = 0.00787785\n",
      "Iteration 5888, loss = 0.00785727\n",
      "Iteration 5889, loss = 0.00784972\n",
      "Iteration 5890, loss = 0.00785454\n",
      "Iteration 5891, loss = 0.00786328\n",
      "Iteration 5892, loss = 0.00786692\n",
      "Iteration 5893, loss = 0.00786122\n",
      "Iteration 5894, loss = 0.00784943\n",
      "Iteration 5895, loss = 0.00783772\n",
      "Iteration 5896, loss = 0.00783135\n",
      "Iteration 5897, loss = 0.00783113\n",
      "Iteration 5898, loss = 0.00783400\n",
      "Iteration 5899, loss = 0.00783577\n",
      "Iteration 5900, loss = 0.00783369\n",
      "Iteration 5901, loss = 0.00782801\n",
      "Iteration 5902, loss = 0.00782089\n",
      "Iteration 5903, loss = 0.00781500\n",
      "Iteration 5904, loss = 0.00781175\n",
      "Iteration 5905, loss = 0.00781084\n",
      "Iteration 5906, loss = 0.00781084\n",
      "Iteration 5907, loss = 0.00781013\n",
      "Iteration 5908, loss = 0.00780794\n",
      "Iteration 5909, loss = 0.00780422\n",
      "Iteration 5910, loss = 0.00779991\n",
      "Iteration 5911, loss = 0.00779587\n",
      "Iteration 5912, loss = 0.00779282\n",
      "Iteration 5913, loss = 0.00779081\n",
      "Iteration 5914, loss = 0.00778958\n",
      "Iteration 5915, loss = 0.00778857\n",
      "Iteration 5916, loss = 0.00778744\n",
      "Iteration 5917, loss = 0.00778592\n",
      "Iteration 5918, loss = 0.00778426\n",
      "Iteration 5919, loss = 0.00778260\n",
      "Iteration 5920, loss = 0.00778168\n",
      "Iteration 5921, loss = 0.00778140\n",
      "Iteration 5922, loss = 0.00778296\n",
      "Iteration 5923, loss = 0.00778536\n",
      "Iteration 5924, loss = 0.00779109\n",
      "Iteration 5925, loss = 0.00779716\n",
      "Iteration 5926, loss = 0.00780929\n",
      "Iteration 5927, loss = 0.00781984\n",
      "Iteration 5928, loss = 0.00784121\n",
      "Iteration 5929, loss = 0.00785502\n",
      "Iteration 5930, loss = 0.00788524\n",
      "Iteration 5931, loss = 0.00789393\n",
      "Iteration 5932, loss = 0.00792076\n",
      "Iteration 5933, loss = 0.00790791\n",
      "Iteration 5934, loss = 0.00790782\n",
      "Iteration 5935, loss = 0.00786828\n",
      "Iteration 5936, loss = 0.00783736\n",
      "Iteration 5937, loss = 0.00779511\n",
      "Iteration 5938, loss = 0.00776460\n",
      "Iteration 5939, loss = 0.00774463\n",
      "Iteration 5940, loss = 0.00773683\n",
      "Iteration 5941, loss = 0.00773839\n",
      "Iteration 5942, loss = 0.00774416\n",
      "Iteration 5943, loss = 0.00775199\n",
      "Iteration 5944, loss = 0.00775429\n",
      "Iteration 5945, loss = 0.00775564\n",
      "Iteration 5946, loss = 0.00774888\n",
      "Iteration 5947, loss = 0.00774201\n",
      "Iteration 5948, loss = 0.00773212\n",
      "Iteration 5949, loss = 0.00772409\n",
      "Iteration 5950, loss = 0.00771769\n",
      "Iteration 5951, loss = 0.00771342\n",
      "Iteration 5952, loss = 0.00771071\n",
      "Iteration 5953, loss = 0.00770842\n",
      "Iteration 5954, loss = 0.00770609\n",
      "Iteration 5955, loss = 0.00770270\n",
      "Iteration 5956, loss = 0.00769898\n",
      "Iteration 5957, loss = 0.00769431\n",
      "Iteration 5958, loss = 0.00768998\n",
      "Iteration 5959, loss = 0.00768565\n",
      "Iteration 5960, loss = 0.00768218\n",
      "Iteration 5961, loss = 0.00767937\n",
      "Iteration 5962, loss = 0.00767738\n",
      "Iteration 5963, loss = 0.00767597\n",
      "Iteration 5964, loss = 0.00767490\n",
      "Iteration 5965, loss = 0.00767393\n",
      "Iteration 5966, loss = 0.00767275\n",
      "Iteration 5967, loss = 0.00767128\n",
      "Iteration 5968, loss = 0.00766932\n",
      "Iteration 5969, loss = 0.00766698\n",
      "Iteration 5970, loss = 0.00766419\n",
      "Iteration 5971, loss = 0.00766115\n",
      "Iteration 5972, loss = 0.00765787\n",
      "Iteration 5973, loss = 0.00765455\n",
      "Iteration 5974, loss = 0.00765122\n",
      "Iteration 5975, loss = 0.00764803\n",
      "Iteration 5976, loss = 0.00764500\n",
      "Iteration 5977, loss = 0.00764217\n",
      "Iteration 5978, loss = 0.00763954\n",
      "Iteration 5979, loss = 0.00763713\n",
      "Iteration 5980, loss = 0.00763491\n",
      "Iteration 5981, loss = 0.00763286\n",
      "Iteration 5982, loss = 0.00763101\n",
      "Iteration 5983, loss = 0.00762931\n",
      "Iteration 5984, loss = 0.00762785\n",
      "Iteration 5985, loss = 0.00762655\n",
      "Iteration 5986, loss = 0.00762563\n",
      "Iteration 5987, loss = 0.00762498\n",
      "Iteration 5988, loss = 0.00762500\n",
      "Iteration 5989, loss = 0.00762555\n",
      "Iteration 5990, loss = 0.00762736\n",
      "Iteration 5991, loss = 0.00763019\n",
      "Iteration 5992, loss = 0.00763539\n",
      "Iteration 5993, loss = 0.00764252\n",
      "Iteration 5994, loss = 0.00765391\n",
      "Iteration 5995, loss = 0.00766838\n",
      "Iteration 5996, loss = 0.00768953\n",
      "Iteration 5997, loss = 0.00771332\n",
      "Iteration 5998, loss = 0.00774382\n",
      "Iteration 5999, loss = 0.00776900\n",
      "Iteration 6000, loss = 0.00779224\n",
      "Iteration 6001, loss = 0.00779101\n",
      "Iteration 6002, loss = 0.00777726\n",
      "Iteration 6003, loss = 0.00773347\n",
      "Iteration 6004, loss = 0.00769332\n",
      "Iteration 6005, loss = 0.00765100\n",
      "Iteration 6006, loss = 0.00763392\n",
      "Iteration 6007, loss = 0.00762653\n",
      "Iteration 6008, loss = 0.00762959\n",
      "Iteration 6009, loss = 0.00762555\n",
      "Iteration 6010, loss = 0.00761273\n",
      "Iteration 6011, loss = 0.00759226\n",
      "Iteration 6012, loss = 0.00757363\n",
      "Iteration 6013, loss = 0.00756502\n",
      "Iteration 6014, loss = 0.00756844\n",
      "Iteration 6015, loss = 0.00757987\n",
      "Iteration 6016, loss = 0.00758993\n",
      "Iteration 6017, loss = 0.00759467\n",
      "Iteration 6018, loss = 0.00758872\n",
      "Iteration 6019, loss = 0.00757856\n",
      "Iteration 6020, loss = 0.00756574\n",
      "Iteration 6021, loss = 0.00755725\n",
      "Iteration 6022, loss = 0.00755245\n",
      "Iteration 6023, loss = 0.00755117\n",
      "Iteration 6024, loss = 0.00755000\n",
      "Iteration 6025, loss = 0.00754723\n",
      "Iteration 6026, loss = 0.00754215\n",
      "Iteration 6027, loss = 0.00753586\n",
      "Iteration 6028, loss = 0.00753003\n",
      "Iteration 6029, loss = 0.00752603\n",
      "Iteration 6030, loss = 0.00752424\n",
      "Iteration 6031, loss = 0.00752413\n",
      "Iteration 6032, loss = 0.00752464\n",
      "Iteration 6033, loss = 0.00752468\n",
      "Iteration 6034, loss = 0.00752375\n",
      "Iteration 6035, loss = 0.00752164\n",
      "Iteration 6036, loss = 0.00751892\n",
      "Iteration 6037, loss = 0.00751590\n",
      "Iteration 6038, loss = 0.00751336\n",
      "Iteration 6039, loss = 0.00751120\n",
      "Iteration 6040, loss = 0.00750984\n",
      "Iteration 6041, loss = 0.00750860\n",
      "Iteration 6042, loss = 0.00750779\n",
      "Iteration 6043, loss = 0.00750655\n",
      "Iteration 6044, loss = 0.00750548\n",
      "Iteration 6045, loss = 0.00750377\n",
      "Iteration 6046, loss = 0.00750239\n",
      "Iteration 6047, loss = 0.00750050\n",
      "Iteration 6048, loss = 0.00749935\n",
      "Iteration 6049, loss = 0.00749788\n",
      "Iteration 6050, loss = 0.00749752\n",
      "Iteration 6051, loss = 0.00749683\n",
      "Iteration 6052, loss = 0.00749761\n",
      "Iteration 6053, loss = 0.00749781\n",
      "Iteration 6054, loss = 0.00749993\n",
      "Iteration 6055, loss = 0.00750098\n",
      "Iteration 6056, loss = 0.00750466\n",
      "Iteration 6057, loss = 0.00750644\n",
      "Iteration 6058, loss = 0.00751184\n",
      "Iteration 6059, loss = 0.00751390\n",
      "Iteration 6060, loss = 0.00752072\n",
      "Iteration 6061, loss = 0.00752188\n",
      "Iteration 6062, loss = 0.00752871\n",
      "Iteration 6063, loss = 0.00752689\n",
      "Iteration 6064, loss = 0.00753088\n",
      "Iteration 6065, loss = 0.00752392\n",
      "Iteration 6066, loss = 0.00752201\n",
      "Iteration 6067, loss = 0.00750964\n",
      "Iteration 6068, loss = 0.00750126\n",
      "Iteration 6069, loss = 0.00748642\n",
      "Iteration 6070, loss = 0.00747490\n",
      "Iteration 6071, loss = 0.00746202\n",
      "Iteration 6072, loss = 0.00745218\n",
      "Iteration 6073, loss = 0.00744405\n",
      "Iteration 6074, loss = 0.00743853\n",
      "Iteration 6075, loss = 0.00743515\n",
      "Iteration 6076, loss = 0.00743360\n",
      "Iteration 6077, loss = 0.00743343\n",
      "Iteration 6078, loss = 0.00743415\n",
      "Iteration 6079, loss = 0.00743560\n",
      "Iteration 6080, loss = 0.00743711\n",
      "Iteration 6081, loss = 0.00743925\n",
      "Iteration 6082, loss = 0.00744091\n",
      "Iteration 6083, loss = 0.00744357\n",
      "Iteration 6084, loss = 0.00744560\n",
      "Iteration 6085, loss = 0.00744927\n",
      "Iteration 6086, loss = 0.00745257\n",
      "Iteration 6087, loss = 0.00745818\n",
      "Iteration 6088, loss = 0.00746376\n",
      "Iteration 6089, loss = 0.00747168\n",
      "Iteration 6090, loss = 0.00747901\n",
      "Iteration 6091, loss = 0.00748657\n",
      "Iteration 6092, loss = 0.00749042\n",
      "Iteration 6093, loss = 0.00748932\n",
      "Iteration 6094, loss = 0.00748019\n",
      "Iteration 6095, loss = 0.00746304\n",
      "Iteration 6096, loss = 0.00744039\n",
      "Iteration 6097, loss = 0.00741712\n",
      "Iteration 6098, loss = 0.00739919\n",
      "Iteration 6099, loss = 0.00739024\n",
      "Iteration 6100, loss = 0.00739074\n",
      "Iteration 6101, loss = 0.00739752\n",
      "Iteration 6102, loss = 0.00740668\n",
      "Iteration 6103, loss = 0.00741271\n",
      "Iteration 6104, loss = 0.00741484\n",
      "Iteration 6105, loss = 0.00741052\n",
      "Iteration 6106, loss = 0.00740439\n",
      "Iteration 6107, loss = 0.00739638\n",
      "Iteration 6108, loss = 0.00739216\n",
      "Iteration 6109, loss = 0.00739020\n",
      "Iteration 6110, loss = 0.00739343\n",
      "Iteration 6111, loss = 0.00739775\n",
      "Iteration 6112, loss = 0.00740492\n",
      "Iteration 6113, loss = 0.00740981\n",
      "Iteration 6114, loss = 0.00741597\n",
      "Iteration 6115, loss = 0.00741777\n",
      "Iteration 6116, loss = 0.00742202\n",
      "Iteration 6117, loss = 0.00742110\n",
      "Iteration 6118, loss = 0.00742536\n",
      "Iteration 6119, loss = 0.00742268\n",
      "Iteration 6120, loss = 0.00742683\n",
      "Iteration 6121, loss = 0.00742061\n",
      "Iteration 6122, loss = 0.00742051\n",
      "Iteration 6123, loss = 0.00740823\n",
      "Iteration 6124, loss = 0.00740038\n",
      "Iteration 6125, loss = 0.00738379\n",
      "Iteration 6126, loss = 0.00737083\n",
      "Iteration 6127, loss = 0.00735578\n",
      "Iteration 6128, loss = 0.00734444\n",
      "Iteration 6129, loss = 0.00733546\n",
      "Iteration 6130, loss = 0.00732984\n",
      "Iteration 6131, loss = 0.00732699\n",
      "Iteration 6132, loss = 0.00732628\n",
      "Iteration 6133, loss = 0.00732694\n",
      "Iteration 6134, loss = 0.00732810\n",
      "Iteration 6135, loss = 0.00732951\n",
      "Iteration 6136, loss = 0.00733009\n",
      "Iteration 6137, loss = 0.00733059\n",
      "Iteration 6138, loss = 0.00732959\n",
      "Iteration 6139, loss = 0.00732873\n",
      "Iteration 6140, loss = 0.00732638\n",
      "Iteration 6141, loss = 0.00732453\n",
      "Iteration 6142, loss = 0.00732167\n",
      "Iteration 6143, loss = 0.00731958\n",
      "Iteration 6144, loss = 0.00731707\n",
      "Iteration 6145, loss = 0.00731550\n",
      "Iteration 6146, loss = 0.00731402\n",
      "Iteration 6147, loss = 0.00731364\n",
      "Iteration 6148, loss = 0.00731378\n",
      "Iteration 6149, loss = 0.00731529\n",
      "Iteration 6150, loss = 0.00731775\n",
      "Iteration 6151, loss = 0.00732211\n",
      "Iteration 6152, loss = 0.00732784\n",
      "Iteration 6153, loss = 0.00733611\n",
      "Iteration 6154, loss = 0.00734566\n",
      "Iteration 6155, loss = 0.00735775\n",
      "Iteration 6156, loss = 0.00736904\n",
      "Iteration 6157, loss = 0.00738057\n",
      "Iteration 6158, loss = 0.00738590\n",
      "Iteration 6159, loss = 0.00738711\n",
      "Iteration 6160, loss = 0.00737745\n",
      "Iteration 6161, loss = 0.00736422\n",
      "Iteration 6162, loss = 0.00734492\n",
      "Iteration 6163, loss = 0.00733181\n",
      "Iteration 6164, loss = 0.00732122\n",
      "Iteration 6165, loss = 0.00732301\n",
      "Iteration 6166, loss = 0.00732432\n",
      "Iteration 6167, loss = 0.00733222\n",
      "Iteration 6168, loss = 0.00732980\n",
      "Iteration 6169, loss = 0.00732643\n",
      "Iteration 6170, loss = 0.00731155\n",
      "Iteration 6171, loss = 0.00729653\n",
      "Iteration 6172, loss = 0.00727907\n",
      "Iteration 6173, loss = 0.00726648\n",
      "Iteration 6174, loss = 0.00725828\n",
      "Iteration 6175, loss = 0.00725511\n",
      "Iteration 6176, loss = 0.00725499\n",
      "Iteration 6177, loss = 0.00725597\n",
      "Iteration 6178, loss = 0.00725628\n",
      "Iteration 6179, loss = 0.00725512\n",
      "Iteration 6180, loss = 0.00725243\n",
      "Iteration 6181, loss = 0.00724877\n",
      "Iteration 6182, loss = 0.00724513\n",
      "Iteration 6183, loss = 0.00724203\n",
      "Iteration 6184, loss = 0.00724024\n",
      "Iteration 6185, loss = 0.00723944\n",
      "Iteration 6186, loss = 0.00723982\n",
      "Iteration 6187, loss = 0.00724054\n",
      "Iteration 6188, loss = 0.00724175\n",
      "Iteration 6189, loss = 0.00724253\n",
      "Iteration 6190, loss = 0.00724352\n",
      "Iteration 6191, loss = 0.00724376\n",
      "Iteration 6192, loss = 0.00724452\n",
      "Iteration 6193, loss = 0.00724458\n",
      "Iteration 6194, loss = 0.00724593\n",
      "Iteration 6195, loss = 0.00724653\n",
      "Iteration 6196, loss = 0.00724945\n",
      "Iteration 6197, loss = 0.00725106\n",
      "Iteration 6198, loss = 0.00725614\n",
      "Iteration 6199, loss = 0.00725849\n",
      "Iteration 6200, loss = 0.00726549\n",
      "Iteration 6201, loss = 0.00726728\n",
      "Iteration 6202, loss = 0.00727464\n",
      "Iteration 6203, loss = 0.00727364\n",
      "Iteration 6204, loss = 0.00727842\n",
      "Iteration 6205, loss = 0.00727241\n",
      "Iteration 6206, loss = 0.00727155\n",
      "Iteration 6207, loss = 0.00726039\n",
      "Iteration 6208, loss = 0.00725338\n",
      "Iteration 6209, loss = 0.00724005\n",
      "Iteration 6210, loss = 0.00723023\n",
      "Iteration 6211, loss = 0.00721907\n",
      "Iteration 6212, loss = 0.00721110\n",
      "Iteration 6213, loss = 0.00720474\n",
      "Iteration 6214, loss = 0.00720107\n",
      "Iteration 6215, loss = 0.00719934\n",
      "Iteration 6216, loss = 0.00719944\n",
      "Iteration 6217, loss = 0.00720057\n",
      "Iteration 6218, loss = 0.00720248\n",
      "Iteration 6219, loss = 0.00720447\n",
      "Iteration 6220, loss = 0.00720622\n",
      "Iteration 6221, loss = 0.00720741\n",
      "Iteration 6222, loss = 0.00720757\n",
      "Iteration 6223, loss = 0.00720676\n",
      "Iteration 6224, loss = 0.00720452\n",
      "Iteration 6225, loss = 0.00720117\n",
      "Iteration 6226, loss = 0.00719652\n",
      "Iteration 6227, loss = 0.00719103\n",
      "Iteration 6228, loss = 0.00718486\n",
      "Iteration 6229, loss = 0.00717850\n",
      "Iteration 6230, loss = 0.00717230\n",
      "Iteration 6231, loss = 0.00716662\n",
      "Iteration 6232, loss = 0.00716170\n",
      "Iteration 6233, loss = 0.00715764\n",
      "Iteration 6234, loss = 0.00715447\n",
      "Iteration 6235, loss = 0.00715209\n",
      "Iteration 6236, loss = 0.00715037\n",
      "Iteration 6237, loss = 0.00714914\n",
      "Iteration 6238, loss = 0.00714827\n",
      "Iteration 6239, loss = 0.00714762\n",
      "Iteration 6240, loss = 0.00714714\n",
      "Iteration 6241, loss = 0.00714672\n",
      "Iteration 6242, loss = 0.00714644\n",
      "Iteration 6243, loss = 0.00714618\n",
      "Iteration 6244, loss = 0.00714615\n",
      "Iteration 6245, loss = 0.00714620\n",
      "Iteration 6246, loss = 0.00714669\n",
      "Iteration 6247, loss = 0.00714735\n",
      "Iteration 6248, loss = 0.00714882\n",
      "Iteration 6249, loss = 0.00715059\n",
      "Iteration 6250, loss = 0.00715379\n",
      "Iteration 6251, loss = 0.00715736\n",
      "Iteration 6252, loss = 0.00716337\n",
      "Iteration 6253, loss = 0.00716957\n",
      "Iteration 6254, loss = 0.00717976\n",
      "Iteration 6255, loss = 0.00718899\n",
      "Iteration 6256, loss = 0.00720424\n",
      "Iteration 6257, loss = 0.00721494\n",
      "Iteration 6258, loss = 0.00723347\n",
      "Iteration 6259, loss = 0.00723976\n",
      "Iteration 6260, loss = 0.00725422\n",
      "Iteration 6261, loss = 0.00724645\n",
      "Iteration 6262, loss = 0.00724596\n",
      "Iteration 6263, loss = 0.00722045\n",
      "Iteration 6264, loss = 0.00720319\n",
      "Iteration 6265, loss = 0.00717294\n",
      "Iteration 6266, loss = 0.00715424\n",
      "Iteration 6267, loss = 0.00713866\n",
      "Iteration 6268, loss = 0.00713537\n",
      "Iteration 6269, loss = 0.00714030\n",
      "Iteration 6270, loss = 0.00715332\n",
      "Iteration 6271, loss = 0.00716846\n",
      "Iteration 6272, loss = 0.00718412\n",
      "Iteration 6273, loss = 0.00719132\n",
      "Iteration 6274, loss = 0.00719157\n",
      "Iteration 6275, loss = 0.00717824\n",
      "Iteration 6276, loss = 0.00715945\n",
      "Iteration 6277, loss = 0.00713716\n",
      "Iteration 6278, loss = 0.00712131\n",
      "Iteration 6279, loss = 0.00711380\n",
      "Iteration 6280, loss = 0.00711717\n",
      "Iteration 6281, loss = 0.00712308\n",
      "Iteration 6282, loss = 0.00713164\n",
      "Iteration 6283, loss = 0.00713074\n",
      "Iteration 6284, loss = 0.00712687\n",
      "Iteration 6285, loss = 0.00711401\n",
      "Iteration 6286, loss = 0.00710213\n",
      "Iteration 6287, loss = 0.00709047\n",
      "Iteration 6288, loss = 0.00708405\n",
      "Iteration 6289, loss = 0.00708156\n",
      "Iteration 6290, loss = 0.00708226\n",
      "Iteration 6291, loss = 0.00708359\n",
      "Iteration 6292, loss = 0.00708403\n",
      "Iteration 6293, loss = 0.00708215\n",
      "Iteration 6294, loss = 0.00707839\n",
      "Iteration 6295, loss = 0.00707314\n",
      "Iteration 6296, loss = 0.00706784\n",
      "Iteration 6297, loss = 0.00706331\n",
      "Iteration 6298, loss = 0.00706024\n",
      "Iteration 6299, loss = 0.00705859\n",
      "Iteration 6300, loss = 0.00705807\n",
      "Iteration 6301, loss = 0.00705798\n",
      "Iteration 6302, loss = 0.00705794\n",
      "Iteration 6303, loss = 0.00705734\n",
      "Iteration 6304, loss = 0.00705629\n",
      "Iteration 6305, loss = 0.00705456\n",
      "Iteration 6306, loss = 0.00705266\n",
      "Iteration 6307, loss = 0.00705055\n",
      "Iteration 6308, loss = 0.00704881\n",
      "Iteration 6309, loss = 0.00704738\n",
      "Iteration 6310, loss = 0.00704673\n",
      "Iteration 6311, loss = 0.00704674\n",
      "Iteration 6312, loss = 0.00704794\n",
      "Iteration 6313, loss = 0.00705017\n",
      "Iteration 6314, loss = 0.00705446\n",
      "Iteration 6315, loss = 0.00706053\n",
      "Iteration 6316, loss = 0.00707080\n",
      "Iteration 6317, loss = 0.00708419\n",
      "Iteration 6318, loss = 0.00710657\n",
      "Iteration 6319, loss = 0.00713305\n",
      "Iteration 6320, loss = 0.00717754\n",
      "Iteration 6321, loss = 0.00722064\n",
      "Iteration 6322, loss = 0.00729256\n",
      "Iteration 6323, loss = 0.00733320\n",
      "Iteration 6324, loss = 0.00739942\n",
      "Iteration 6325, loss = 0.00737386\n",
      "Iteration 6326, loss = 0.00735011\n",
      "Iteration 6327, loss = 0.00723447\n",
      "Iteration 6328, loss = 0.00713417\n",
      "Iteration 6329, loss = 0.00704777\n",
      "Iteration 6330, loss = 0.00701525\n",
      "Iteration 6331, loss = 0.00703209\n",
      "Iteration 6332, loss = 0.00707576\n",
      "Iteration 6333, loss = 0.00712230\n",
      "Iteration 6334, loss = 0.00713565\n",
      "Iteration 6335, loss = 0.00712812\n",
      "Iteration 6336, loss = 0.00708444\n",
      "Iteration 6337, loss = 0.00704349\n",
      "Iteration 6338, loss = 0.00701327\n",
      "Iteration 6339, loss = 0.00700623\n",
      "Iteration 6340, loss = 0.00701758\n",
      "Iteration 6341, loss = 0.00703520\n",
      "Iteration 6342, loss = 0.00704909\n",
      "Iteration 6343, loss = 0.00704791\n",
      "Iteration 6344, loss = 0.00703820\n",
      "Iteration 6345, loss = 0.00701947\n",
      "Iteration 6346, loss = 0.00700405\n",
      "Iteration 6347, loss = 0.00699487\n",
      "Iteration 6348, loss = 0.00699380\n",
      "Iteration 6349, loss = 0.00699821\n",
      "Iteration 6350, loss = 0.00700370\n",
      "Iteration 6351, loss = 0.00700719\n",
      "Iteration 6352, loss = 0.00700551\n",
      "Iteration 6353, loss = 0.00700072\n",
      "Iteration 6354, loss = 0.00699304\n",
      "Iteration 6355, loss = 0.00698623\n",
      "Iteration 6356, loss = 0.00698136\n",
      "Iteration 6357, loss = 0.00697932\n",
      "Iteration 6358, loss = 0.00697953\n",
      "Iteration 6359, loss = 0.00698075\n",
      "Iteration 6360, loss = 0.00698176\n",
      "Iteration 6361, loss = 0.00698137\n",
      "Iteration 6362, loss = 0.00697966\n",
      "Iteration 6363, loss = 0.00697648\n",
      "Iteration 6364, loss = 0.00697293\n",
      "Iteration 6365, loss = 0.00696943\n",
      "Iteration 6366, loss = 0.00696671\n",
      "Iteration 6367, loss = 0.00696492\n",
      "Iteration 6368, loss = 0.00696398\n",
      "Iteration 6369, loss = 0.00696356\n",
      "Iteration 6370, loss = 0.00696326\n",
      "Iteration 6371, loss = 0.00696276\n",
      "Iteration 6372, loss = 0.00696178\n",
      "Iteration 6373, loss = 0.00696039\n",
      "Iteration 6374, loss = 0.00695854\n",
      "Iteration 6375, loss = 0.00695651\n",
      "Iteration 6376, loss = 0.00695441\n",
      "Iteration 6377, loss = 0.00695245\n",
      "Iteration 6378, loss = 0.00695068\n",
      "Iteration 6379, loss = 0.00694917\n",
      "Iteration 6380, loss = 0.00694787\n",
      "Iteration 6381, loss = 0.00694673\n",
      "Iteration 6382, loss = 0.00694568\n",
      "Iteration 6383, loss = 0.00694464\n",
      "Iteration 6384, loss = 0.00694357\n",
      "Iteration 6385, loss = 0.00694244\n",
      "Iteration 6386, loss = 0.00694125\n",
      "Iteration 6387, loss = 0.00693999\n",
      "Iteration 6388, loss = 0.00693869\n",
      "Iteration 6389, loss = 0.00693738\n",
      "Iteration 6390, loss = 0.00693607\n",
      "Iteration 6391, loss = 0.00693479\n",
      "Iteration 6392, loss = 0.00693357\n",
      "Iteration 6393, loss = 0.00693241\n",
      "Iteration 6394, loss = 0.00693135\n",
      "Iteration 6395, loss = 0.00693040\n",
      "Iteration 6396, loss = 0.00692963\n",
      "Iteration 6397, loss = 0.00692906\n",
      "Iteration 6398, loss = 0.00692883\n",
      "Iteration 6399, loss = 0.00692904\n",
      "Iteration 6400, loss = 0.00692998\n",
      "Iteration 6401, loss = 0.00693186\n",
      "Iteration 6402, loss = 0.00693543\n",
      "Iteration 6403, loss = 0.00694101\n",
      "Iteration 6404, loss = 0.00695046\n",
      "Iteration 6405, loss = 0.00696374\n",
      "Iteration 6406, loss = 0.00698498\n",
      "Iteration 6407, loss = 0.00701089\n",
      "Iteration 6408, loss = 0.00704853\n",
      "Iteration 6409, loss = 0.00708081\n",
      "Iteration 6410, loss = 0.00711529\n",
      "Iteration 6411, loss = 0.00711209\n",
      "Iteration 6412, loss = 0.00708664\n",
      "Iteration 6413, loss = 0.00702009\n",
      "Iteration 6414, loss = 0.00695379\n",
      "Iteration 6415, loss = 0.00691094\n",
      "Iteration 6416, loss = 0.00690809\n",
      "Iteration 6417, loss = 0.00693498\n",
      "Iteration 6418, loss = 0.00696418\n",
      "Iteration 6419, loss = 0.00697407\n",
      "Iteration 6420, loss = 0.00695404\n",
      "Iteration 6421, loss = 0.00692265\n",
      "Iteration 6422, loss = 0.00689979\n",
      "Iteration 6423, loss = 0.00689799\n",
      "Iteration 6424, loss = 0.00691180\n",
      "Iteration 6425, loss = 0.00692542\n",
      "Iteration 6426, loss = 0.00692751\n",
      "Iteration 6427, loss = 0.00691531\n",
      "Iteration 6428, loss = 0.00689927\n",
      "Iteration 6429, loss = 0.00688927\n",
      "Iteration 6430, loss = 0.00688969\n",
      "Iteration 6431, loss = 0.00689669\n",
      "Iteration 6432, loss = 0.00690273\n",
      "Iteration 6433, loss = 0.00690288\n",
      "Iteration 6434, loss = 0.00689693\n",
      "Iteration 6435, loss = 0.00688942\n",
      "Iteration 6436, loss = 0.00688529\n",
      "Iteration 6437, loss = 0.00688650\n",
      "Iteration 6438, loss = 0.00689243\n",
      "Iteration 6439, loss = 0.00689976\n",
      "Iteration 6440, loss = 0.00690811\n",
      "Iteration 6441, loss = 0.00691574\n",
      "Iteration 6442, loss = 0.00692980\n",
      "Iteration 6443, loss = 0.00694709\n",
      "Iteration 6444, loss = 0.00698486\n",
      "Iteration 6445, loss = 0.00702006\n",
      "Iteration 6446, loss = 0.00709207\n",
      "Iteration 6447, loss = 0.00712166\n",
      "Iteration 6448, loss = 0.00719255\n",
      "Iteration 6449, loss = 0.00715165\n",
      "Iteration 6450, loss = 0.00713421\n",
      "Iteration 6451, loss = 0.00702600\n",
      "Iteration 6452, loss = 0.00694721\n",
      "Iteration 6453, loss = 0.00688624\n",
      "Iteration 6454, loss = 0.00686918\n",
      "Iteration 6455, loss = 0.00688704\n",
      "Iteration 6456, loss = 0.00691909\n",
      "Iteration 6457, loss = 0.00695313\n",
      "Iteration 6458, loss = 0.00695511\n",
      "Iteration 6459, loss = 0.00694800\n",
      "Iteration 6460, loss = 0.00691451\n",
      "Iteration 6461, loss = 0.00688667\n",
      "Iteration 6462, loss = 0.00686767\n",
      "Iteration 6463, loss = 0.00686348\n",
      "Iteration 6464, loss = 0.00686922\n",
      "Iteration 6465, loss = 0.00687674\n",
      "Iteration 6466, loss = 0.00688218\n",
      "Iteration 6467, loss = 0.00687818\n",
      "Iteration 6468, loss = 0.00687193\n",
      "Iteration 6469, loss = 0.00686253\n",
      "Iteration 6470, loss = 0.00685604\n",
      "Iteration 6471, loss = 0.00685243\n",
      "Iteration 6472, loss = 0.00685150\n",
      "Iteration 6473, loss = 0.00685108\n",
      "Iteration 6474, loss = 0.00684981\n",
      "Iteration 6475, loss = 0.00684753\n",
      "Iteration 6476, loss = 0.00684387\n",
      "Iteration 6477, loss = 0.00684075\n",
      "Iteration 6478, loss = 0.00683829\n",
      "Iteration 6479, loss = 0.00683724\n",
      "Iteration 6480, loss = 0.00683693\n",
      "Iteration 6481, loss = 0.00683682\n",
      "Iteration 6482, loss = 0.00683593\n",
      "Iteration 6483, loss = 0.00683412\n",
      "Iteration 6484, loss = 0.00683132\n",
      "Iteration 6485, loss = 0.00682806\n",
      "Iteration 6486, loss = 0.00682500\n",
      "Iteration 6487, loss = 0.00682258\n",
      "Iteration 6488, loss = 0.00682111\n",
      "Iteration 6489, loss = 0.00682048\n",
      "Iteration 6490, loss = 0.00682039\n",
      "Iteration 6491, loss = 0.00682039\n",
      "Iteration 6492, loss = 0.00682017\n",
      "Iteration 6493, loss = 0.00681942\n",
      "Iteration 6494, loss = 0.00681818\n",
      "Iteration 6495, loss = 0.00681645\n",
      "Iteration 6496, loss = 0.00681449\n",
      "Iteration 6497, loss = 0.00681246\n",
      "Iteration 6498, loss = 0.00681056\n",
      "Iteration 6499, loss = 0.00680887\n",
      "Iteration 6500, loss = 0.00680744\n",
      "Iteration 6501, loss = 0.00680623\n",
      "Iteration 6502, loss = 0.00680519\n",
      "Iteration 6503, loss = 0.00680424\n",
      "Iteration 6504, loss = 0.00680331\n",
      "Iteration 6505, loss = 0.00680236\n",
      "Iteration 6506, loss = 0.00680137\n",
      "Iteration 6507, loss = 0.00680034\n",
      "Iteration 6508, loss = 0.00679929\n",
      "Iteration 6509, loss = 0.00679823\n",
      "Iteration 6510, loss = 0.00679719\n",
      "Iteration 6511, loss = 0.00679620\n",
      "Iteration 6512, loss = 0.00679526\n",
      "Iteration 6513, loss = 0.00679441\n",
      "Iteration 6514, loss = 0.00679365\n",
      "Iteration 6515, loss = 0.00679305\n",
      "Iteration 6516, loss = 0.00679259\n",
      "Iteration 6517, loss = 0.00679241\n",
      "Iteration 6518, loss = 0.00679252\n",
      "Iteration 6519, loss = 0.00679319\n",
      "Iteration 6520, loss = 0.00679445\n",
      "Iteration 6521, loss = 0.00679691\n",
      "Iteration 6522, loss = 0.00680062\n",
      "Iteration 6523, loss = 0.00680699\n",
      "Iteration 6524, loss = 0.00681579\n",
      "Iteration 6525, loss = 0.00683032\n",
      "Iteration 6526, loss = 0.00684860\n",
      "Iteration 6527, loss = 0.00687776\n",
      "Iteration 6528, loss = 0.00690822\n",
      "Iteration 6529, loss = 0.00695280\n",
      "Iteration 6530, loss = 0.00698084\n",
      "Iteration 6531, loss = 0.00701217\n",
      "Iteration 6532, loss = 0.00699655\n",
      "Iteration 6533, loss = 0.00697172\n",
      "Iteration 6534, loss = 0.00691548\n",
      "Iteration 6535, loss = 0.00687905\n",
      "Iteration 6536, loss = 0.00685617\n",
      "Iteration 6537, loss = 0.00686853\n",
      "Iteration 6538, loss = 0.00687710\n",
      "Iteration 6539, loss = 0.00688118\n",
      "Iteration 6540, loss = 0.00685582\n",
      "Iteration 6541, loss = 0.00681695\n",
      "Iteration 6542, loss = 0.00678025\n",
      "Iteration 6543, loss = 0.00676325\n",
      "Iteration 6544, loss = 0.00676823\n",
      "Iteration 6545, loss = 0.00678491\n",
      "Iteration 6546, loss = 0.00679871\n",
      "Iteration 6547, loss = 0.00680112\n",
      "Iteration 6548, loss = 0.00679374\n",
      "Iteration 6549, loss = 0.00678237\n",
      "Iteration 6550, loss = 0.00677717\n",
      "Iteration 6551, loss = 0.00677728\n",
      "Iteration 6552, loss = 0.00678171\n",
      "Iteration 6553, loss = 0.00678269\n",
      "Iteration 6554, loss = 0.00677887\n",
      "Iteration 6555, loss = 0.00676917\n",
      "Iteration 6556, loss = 0.00675876\n",
      "Iteration 6557, loss = 0.00675103\n",
      "Iteration 6558, loss = 0.00674812\n",
      "Iteration 6559, loss = 0.00674900\n",
      "Iteration 6560, loss = 0.00675110\n",
      "Iteration 6561, loss = 0.00675215\n",
      "Iteration 6562, loss = 0.00675091\n",
      "Iteration 6563, loss = 0.00674821\n",
      "Iteration 6564, loss = 0.00674522\n",
      "Iteration 6565, loss = 0.00674339\n",
      "Iteration 6566, loss = 0.00674301\n",
      "Iteration 6567, loss = 0.00674388\n",
      "Iteration 6568, loss = 0.00674482\n",
      "Iteration 6569, loss = 0.00674537\n",
      "Iteration 6570, loss = 0.00674464\n",
      "Iteration 6571, loss = 0.00674329\n",
      "Iteration 6572, loss = 0.00674118\n",
      "Iteration 6573, loss = 0.00673946\n",
      "Iteration 6574, loss = 0.00673793\n",
      "Iteration 6575, loss = 0.00673728\n",
      "Iteration 6576, loss = 0.00673687\n",
      "Iteration 6577, loss = 0.00673697\n",
      "Iteration 6578, loss = 0.00673681\n",
      "Iteration 6579, loss = 0.00673682\n",
      "Iteration 6580, loss = 0.00673637\n",
      "Iteration 6581, loss = 0.00673625\n",
      "Iteration 6582, loss = 0.00673586\n",
      "Iteration 6583, loss = 0.00673630\n",
      "Iteration 6584, loss = 0.00673664\n",
      "Iteration 6585, loss = 0.00673838\n",
      "Iteration 6586, loss = 0.00673984\n",
      "Iteration 6587, loss = 0.00674334\n",
      "Iteration 6588, loss = 0.00674602\n",
      "Iteration 6589, loss = 0.00675169\n",
      "Iteration 6590, loss = 0.00675554\n",
      "Iteration 6591, loss = 0.00676383\n",
      "Iteration 6592, loss = 0.00676853\n",
      "Iteration 6593, loss = 0.00677944\n",
      "Iteration 6594, loss = 0.00678377\n",
      "Iteration 6595, loss = 0.00679581\n",
      "Iteration 6596, loss = 0.00679712\n",
      "Iteration 6597, loss = 0.00680653\n",
      "Iteration 6598, loss = 0.00680164\n",
      "Iteration 6599, loss = 0.00680377\n",
      "Iteration 6600, loss = 0.00679181\n",
      "Iteration 6601, loss = 0.00678521\n",
      "Iteration 6602, loss = 0.00676962\n",
      "Iteration 6603, loss = 0.00675831\n",
      "Iteration 6604, loss = 0.00674441\n",
      "Iteration 6605, loss = 0.00673427\n",
      "Iteration 6606, loss = 0.00672498\n",
      "Iteration 6607, loss = 0.00671874\n",
      "Iteration 6608, loss = 0.00671369\n",
      "Iteration 6609, loss = 0.00671054\n",
      "Iteration 6610, loss = 0.00670797\n",
      "Iteration 6611, loss = 0.00670615\n",
      "Iteration 6612, loss = 0.00670468\n",
      "Iteration 6613, loss = 0.00670321\n",
      "Iteration 6614, loss = 0.00670219\n",
      "Iteration 6615, loss = 0.00670090\n",
      "Iteration 6616, loss = 0.00670020\n",
      "Iteration 6617, loss = 0.00669925\n",
      "Iteration 6618, loss = 0.00669894\n",
      "Iteration 6619, loss = 0.00669850\n",
      "Iteration 6620, loss = 0.00669868\n",
      "Iteration 6621, loss = 0.00669882\n",
      "Iteration 6622, loss = 0.00669964\n",
      "Iteration 6623, loss = 0.00670040\n",
      "Iteration 6624, loss = 0.00670203\n",
      "Iteration 6625, loss = 0.00670350\n",
      "Iteration 6626, loss = 0.00670613\n",
      "Iteration 6627, loss = 0.00670835\n",
      "Iteration 6628, loss = 0.00671212\n",
      "Iteration 6629, loss = 0.00671495\n",
      "Iteration 6630, loss = 0.00671968\n",
      "Iteration 6631, loss = 0.00672249\n",
      "Iteration 6632, loss = 0.00672730\n",
      "Iteration 6633, loss = 0.00672880\n",
      "Iteration 6634, loss = 0.00673202\n",
      "Iteration 6635, loss = 0.00673074\n",
      "Iteration 6636, loss = 0.00673084\n",
      "Iteration 6637, loss = 0.00672647\n",
      "Iteration 6638, loss = 0.00672393\n",
      "Iteration 6639, loss = 0.00671851\n",
      "Iteration 6640, loss = 0.00671644\n",
      "Iteration 6641, loss = 0.00671308\n",
      "Iteration 6642, loss = 0.00671482\n",
      "Iteration 6643, loss = 0.00671472\n",
      "Iteration 6644, loss = 0.00672061\n",
      "Iteration 6645, loss = 0.00672168\n",
      "Iteration 6646, loss = 0.00672843\n",
      "Iteration 6647, loss = 0.00672680\n",
      "Iteration 6648, loss = 0.00672973\n",
      "Iteration 6649, loss = 0.00672268\n",
      "Iteration 6650, loss = 0.00671899\n",
      "Iteration 6651, loss = 0.00670726\n",
      "Iteration 6652, loss = 0.00669830\n",
      "Iteration 6653, loss = 0.00668571\n",
      "Iteration 6654, loss = 0.00667584\n",
      "Iteration 6655, loss = 0.00666629\n",
      "Iteration 6656, loss = 0.00665937\n",
      "Iteration 6657, loss = 0.00665437\n",
      "Iteration 6658, loss = 0.00665146\n",
      "Iteration 6659, loss = 0.00665022\n",
      "Iteration 6660, loss = 0.00665022\n",
      "Iteration 6661, loss = 0.00665107\n",
      "Iteration 6662, loss = 0.00665237\n",
      "Iteration 6663, loss = 0.00665394\n",
      "Iteration 6664, loss = 0.00665534\n",
      "Iteration 6665, loss = 0.00665687\n",
      "Iteration 6666, loss = 0.00665784\n",
      "Iteration 6667, loss = 0.00665906\n",
      "Iteration 6668, loss = 0.00665955\n",
      "Iteration 6669, loss = 0.00666050\n",
      "Iteration 6670, loss = 0.00666074\n",
      "Iteration 6671, loss = 0.00666164\n",
      "Iteration 6672, loss = 0.00666202\n",
      "Iteration 6673, loss = 0.00666327\n",
      "Iteration 6674, loss = 0.00666440\n",
      "Iteration 6675, loss = 0.00666656\n",
      "Iteration 6676, loss = 0.00666936\n",
      "Iteration 6677, loss = 0.00667328\n",
      "Iteration 6678, loss = 0.00667917\n",
      "Iteration 6679, loss = 0.00668579\n",
      "Iteration 6680, loss = 0.00669636\n",
      "Iteration 6681, loss = 0.00670562\n",
      "Iteration 6682, loss = 0.00672050\n",
      "Iteration 6683, loss = 0.00672826\n",
      "Iteration 6684, loss = 0.00674063\n",
      "Iteration 6685, loss = 0.00673691\n",
      "Iteration 6686, loss = 0.00673385\n",
      "Iteration 6687, loss = 0.00671360\n",
      "Iteration 6688, loss = 0.00669524\n",
      "Iteration 6689, loss = 0.00667493\n",
      "Iteration 6690, loss = 0.00666618\n",
      "Iteration 6691, loss = 0.00666795\n",
      "Iteration 6692, loss = 0.00668595\n",
      "Iteration 6693, loss = 0.00670483\n",
      "Iteration 6694, loss = 0.00673424\n",
      "Iteration 6695, loss = 0.00674269\n",
      "Iteration 6696, loss = 0.00675626\n",
      "Iteration 6697, loss = 0.00673820\n",
      "Iteration 6698, loss = 0.00672810\n",
      "Iteration 6699, loss = 0.00669907\n",
      "Iteration 6700, loss = 0.00668254\n",
      "Iteration 6701, loss = 0.00666502\n",
      "Iteration 6702, loss = 0.00665697\n",
      "Iteration 6703, loss = 0.00665051\n",
      "Iteration 6704, loss = 0.00664635\n",
      "Iteration 6705, loss = 0.00663916\n",
      "Iteration 6706, loss = 0.00663139\n",
      "Iteration 6707, loss = 0.00662243\n",
      "Iteration 6708, loss = 0.00661550\n",
      "Iteration 6709, loss = 0.00661204\n",
      "Iteration 6710, loss = 0.00661239\n",
      "Iteration 6711, loss = 0.00661617\n",
      "Iteration 6712, loss = 0.00662125\n",
      "Iteration 6713, loss = 0.00662658\n",
      "Iteration 6714, loss = 0.00662947\n",
      "Iteration 6715, loss = 0.00663077\n",
      "Iteration 6716, loss = 0.00662854\n",
      "Iteration 6717, loss = 0.00662544\n",
      "Iteration 6718, loss = 0.00662060\n",
      "Iteration 6719, loss = 0.00661658\n",
      "Iteration 6720, loss = 0.00661268\n",
      "Iteration 6721, loss = 0.00661033\n",
      "Iteration 6722, loss = 0.00660823\n",
      "Iteration 6723, loss = 0.00660719\n",
      "Iteration 6724, loss = 0.00660563\n",
      "Iteration 6725, loss = 0.00660438\n",
      "Iteration 6726, loss = 0.00660224\n",
      "Iteration 6727, loss = 0.00660017\n",
      "Iteration 6728, loss = 0.00659755\n",
      "Iteration 6729, loss = 0.00659517\n",
      "Iteration 6730, loss = 0.00659278\n",
      "Iteration 6731, loss = 0.00659081\n",
      "Iteration 6732, loss = 0.00658915\n",
      "Iteration 6733, loss = 0.00658791\n",
      "Iteration 6734, loss = 0.00658697\n",
      "Iteration 6735, loss = 0.00658632\n",
      "Iteration 6736, loss = 0.00658584\n",
      "Iteration 6737, loss = 0.00658553\n",
      "Iteration 6738, loss = 0.00658528\n",
      "Iteration 6739, loss = 0.00658517\n",
      "Iteration 6740, loss = 0.00658512\n",
      "Iteration 6741, loss = 0.00658532\n",
      "Iteration 6742, loss = 0.00658569\n",
      "Iteration 6743, loss = 0.00658661\n",
      "Iteration 6744, loss = 0.00658795\n",
      "Iteration 6745, loss = 0.00659047\n",
      "Iteration 6746, loss = 0.00659388\n",
      "Iteration 6747, loss = 0.00659973\n",
      "Iteration 6748, loss = 0.00660732\n",
      "Iteration 6749, loss = 0.00662004\n",
      "Iteration 6750, loss = 0.00663552\n",
      "Iteration 6751, loss = 0.00666164\n",
      "Iteration 6752, loss = 0.00668997\n",
      "Iteration 6753, loss = 0.00673830\n",
      "Iteration 6754, loss = 0.00677935\n",
      "Iteration 6755, loss = 0.00684962\n",
      "Iteration 6756, loss = 0.00687829\n",
      "Iteration 6757, loss = 0.00693040\n",
      "Iteration 6758, loss = 0.00689001\n",
      "Iteration 6759, loss = 0.00685443\n",
      "Iteration 6760, loss = 0.00674558\n",
      "Iteration 6761, loss = 0.00665709\n",
      "Iteration 6762, loss = 0.00658712\n",
      "Iteration 6763, loss = 0.00656354\n",
      "Iteration 6764, loss = 0.00658021\n",
      "Iteration 6765, loss = 0.00661792\n",
      "Iteration 6766, loss = 0.00665824\n",
      "Iteration 6767, loss = 0.00667176\n",
      "Iteration 6768, loss = 0.00666854\n",
      "Iteration 6769, loss = 0.00663403\n",
      "Iteration 6770, loss = 0.00659955\n",
      "Iteration 6771, loss = 0.00657011\n",
      "Iteration 6772, loss = 0.00655874\n",
      "Iteration 6773, loss = 0.00656370\n",
      "Iteration 6774, loss = 0.00657728\n",
      "Iteration 6775, loss = 0.00659129\n",
      "Iteration 6776, loss = 0.00659588\n",
      "Iteration 6777, loss = 0.00659307\n",
      "Iteration 6778, loss = 0.00658049\n",
      "Iteration 6779, loss = 0.00656758\n",
      "Iteration 6780, loss = 0.00655647\n",
      "Iteration 6781, loss = 0.00655130\n",
      "Iteration 6782, loss = 0.00655155\n",
      "Iteration 6783, loss = 0.00655499\n",
      "Iteration 6784, loss = 0.00655909\n",
      "Iteration 6785, loss = 0.00656089\n",
      "Iteration 6786, loss = 0.00656024\n",
      "Iteration 6787, loss = 0.00655639\n",
      "Iteration 6788, loss = 0.00655164\n",
      "Iteration 6789, loss = 0.00654663\n",
      "Iteration 6790, loss = 0.00654307\n",
      "Iteration 6791, loss = 0.00654119\n",
      "Iteration 6792, loss = 0.00654089\n",
      "Iteration 6793, loss = 0.00654158\n",
      "Iteration 6794, loss = 0.00654236\n",
      "Iteration 6795, loss = 0.00654276\n",
      "Iteration 6796, loss = 0.00654215\n",
      "Iteration 6797, loss = 0.00654080\n",
      "Iteration 6798, loss = 0.00653866\n",
      "Iteration 6799, loss = 0.00653632\n",
      "Iteration 6800, loss = 0.00653401\n",
      "Iteration 6801, loss = 0.00653211\n",
      "Iteration 6802, loss = 0.00653073\n",
      "Iteration 6803, loss = 0.00652986\n",
      "Iteration 6804, loss = 0.00652942\n",
      "Iteration 6805, loss = 0.00652921\n",
      "Iteration 6806, loss = 0.00652908\n",
      "Iteration 6807, loss = 0.00652886\n",
      "Iteration 6808, loss = 0.00652851\n",
      "Iteration 6809, loss = 0.00652792\n",
      "Iteration 6810, loss = 0.00652716\n",
      "Iteration 6811, loss = 0.00652621\n",
      "Iteration 6812, loss = 0.00652518\n",
      "Iteration 6813, loss = 0.00652407\n",
      "Iteration 6814, loss = 0.00652299\n",
      "Iteration 6815, loss = 0.00652192\n",
      "Iteration 6816, loss = 0.00652094\n",
      "Iteration 6817, loss = 0.00652002\n",
      "Iteration 6818, loss = 0.00651920\n",
      "Iteration 6819, loss = 0.00651845\n",
      "Iteration 6820, loss = 0.00651781\n",
      "Iteration 6821, loss = 0.00651724\n",
      "Iteration 6822, loss = 0.00651680\n",
      "Iteration 6823, loss = 0.00651646\n",
      "Iteration 6824, loss = 0.00651631\n",
      "Iteration 6825, loss = 0.00651636\n",
      "Iteration 6826, loss = 0.00651676\n",
      "Iteration 6827, loss = 0.00651755\n",
      "Iteration 6828, loss = 0.00651908\n",
      "Iteration 6829, loss = 0.00652139\n",
      "Iteration 6830, loss = 0.00652536\n",
      "Iteration 6831, loss = 0.00653078\n",
      "Iteration 6832, loss = 0.00653987\n",
      "Iteration 6833, loss = 0.00655124\n",
      "Iteration 6834, loss = 0.00657023\n",
      "Iteration 6835, loss = 0.00659065\n",
      "Iteration 6836, loss = 0.00662402\n",
      "Iteration 6837, loss = 0.00664940\n",
      "Iteration 6838, loss = 0.00668726\n",
      "Iteration 6839, loss = 0.00669240\n",
      "Iteration 6840, loss = 0.00669504\n",
      "Iteration 6841, loss = 0.00665507\n",
      "Iteration 6842, loss = 0.00660863\n",
      "Iteration 6843, loss = 0.00656177\n",
      "Iteration 6844, loss = 0.00653309\n",
      "Iteration 6845, loss = 0.00653358\n",
      "Iteration 6846, loss = 0.00655217\n",
      "Iteration 6847, loss = 0.00657903\n",
      "Iteration 6848, loss = 0.00658999\n",
      "Iteration 6849, loss = 0.00658739\n",
      "Iteration 6850, loss = 0.00656361\n",
      "Iteration 6851, loss = 0.00654078\n",
      "Iteration 6852, loss = 0.00652449\n",
      "Iteration 6853, loss = 0.00652401\n",
      "Iteration 6854, loss = 0.00653042\n",
      "Iteration 6855, loss = 0.00653885\n",
      "Iteration 6856, loss = 0.00653833\n",
      "Iteration 6857, loss = 0.00652945\n",
      "Iteration 6858, loss = 0.00651476\n",
      "Iteration 6859, loss = 0.00650215\n",
      "Iteration 6860, loss = 0.00649506\n",
      "Iteration 6861, loss = 0.00649484\n",
      "Iteration 6862, loss = 0.00649783\n",
      "Iteration 6863, loss = 0.00650051\n",
      "Iteration 6864, loss = 0.00649934\n",
      "Iteration 6865, loss = 0.00649509\n",
      "Iteration 6866, loss = 0.00648884\n",
      "Iteration 6867, loss = 0.00648358\n",
      "Iteration 6868, loss = 0.00648079\n",
      "Iteration 6869, loss = 0.00648065\n",
      "Iteration 6870, loss = 0.00648205\n",
      "Iteration 6871, loss = 0.00648339\n",
      "Iteration 6872, loss = 0.00648364\n",
      "Iteration 6873, loss = 0.00648224\n",
      "Iteration 6874, loss = 0.00647987\n",
      "Iteration 6875, loss = 0.00647712\n",
      "Iteration 6876, loss = 0.00647491\n",
      "Iteration 6877, loss = 0.00647361\n",
      "Iteration 6878, loss = 0.00647320\n",
      "Iteration 6879, loss = 0.00647333\n",
      "Iteration 6880, loss = 0.00647353\n",
      "Iteration 6881, loss = 0.00647345\n",
      "Iteration 6882, loss = 0.00647287\n",
      "Iteration 6883, loss = 0.00647189\n",
      "Iteration 6884, loss = 0.00647066\n",
      "Iteration 6885, loss = 0.00646945\n",
      "Iteration 6886, loss = 0.00646849\n",
      "Iteration 6887, loss = 0.00646792\n",
      "Iteration 6888, loss = 0.00646788\n",
      "Iteration 6889, loss = 0.00646834\n",
      "Iteration 6890, loss = 0.00646949\n",
      "Iteration 6891, loss = 0.00647130\n",
      "Iteration 6892, loss = 0.00647446\n",
      "Iteration 6893, loss = 0.00647886\n",
      "Iteration 6894, loss = 0.00648659\n",
      "Iteration 6895, loss = 0.00649685\n",
      "Iteration 6896, loss = 0.00651550\n",
      "Iteration 6897, loss = 0.00653774\n",
      "Iteration 6898, loss = 0.00657965\n",
      "Iteration 6899, loss = 0.00661910\n",
      "Iteration 6900, loss = 0.00669615\n",
      "Iteration 6901, loss = 0.00673409\n",
      "Iteration 6902, loss = 0.00681606\n",
      "Iteration 6903, loss = 0.00678283\n",
      "Iteration 6904, loss = 0.00677090\n",
      "Iteration 6905, loss = 0.00665509\n",
      "Iteration 6906, loss = 0.00656742\n",
      "Iteration 6907, loss = 0.00649681\n",
      "Iteration 6908, loss = 0.00647717\n",
      "Iteration 6909, loss = 0.00649727\n",
      "Iteration 6910, loss = 0.00653441\n",
      "Iteration 6911, loss = 0.00656995\n",
      "Iteration 6912, loss = 0.00656402\n",
      "Iteration 6913, loss = 0.00654364\n",
      "Iteration 6914, loss = 0.00649702\n",
      "Iteration 6915, loss = 0.00646342\n",
      "Iteration 6916, loss = 0.00645077\n",
      "Iteration 6917, loss = 0.00646090\n",
      "Iteration 6918, loss = 0.00648261\n",
      "Iteration 6919, loss = 0.00649967\n",
      "Iteration 6920, loss = 0.00650544\n",
      "Iteration 6921, loss = 0.00649206\n",
      "Iteration 6922, loss = 0.00647240\n",
      "Iteration 6923, loss = 0.00645332\n",
      "Iteration 6924, loss = 0.00644337\n",
      "Iteration 6925, loss = 0.00644387\n",
      "Iteration 6926, loss = 0.00645070\n",
      "Iteration 6927, loss = 0.00645854\n",
      "Iteration 6928, loss = 0.00646061\n",
      "Iteration 6929, loss = 0.00645792\n",
      "Iteration 6930, loss = 0.00644966\n",
      "Iteration 6931, loss = 0.00644156\n",
      "Iteration 6932, loss = 0.00643606\n",
      "Iteration 6933, loss = 0.00643476\n",
      "Iteration 6934, loss = 0.00643693\n",
      "Iteration 6935, loss = 0.00644036\n",
      "Iteration 6936, loss = 0.00644314\n",
      "Iteration 6937, loss = 0.00644316\n",
      "Iteration 6938, loss = 0.00644126\n",
      "Iteration 6939, loss = 0.00643757\n",
      "Iteration 6940, loss = 0.00643416\n",
      "Iteration 6941, loss = 0.00643183\n",
      "Iteration 6942, loss = 0.00643126\n",
      "Iteration 6943, loss = 0.00643200\n",
      "Iteration 6944, loss = 0.00643362\n",
      "Iteration 6945, loss = 0.00643505\n",
      "Iteration 6946, loss = 0.00643623\n",
      "Iteration 6947, loss = 0.00643660\n",
      "Iteration 6948, loss = 0.00643718\n",
      "Iteration 6949, loss = 0.00643758\n",
      "Iteration 6950, loss = 0.00643969\n",
      "Iteration 6951, loss = 0.00644232\n",
      "Iteration 6952, loss = 0.00644802\n",
      "Iteration 6953, loss = 0.00645396\n",
      "Iteration 6954, loss = 0.00646423\n",
      "Iteration 6955, loss = 0.00647303\n",
      "Iteration 6956, loss = 0.00648773\n",
      "Iteration 6957, loss = 0.00649687\n",
      "Iteration 6958, loss = 0.00651283\n",
      "Iteration 6959, loss = 0.00651576\n",
      "Iteration 6960, loss = 0.00652361\n",
      "Iteration 6961, loss = 0.00651186\n",
      "Iteration 6962, loss = 0.00650157\n",
      "Iteration 6963, loss = 0.00647661\n",
      "Iteration 6964, loss = 0.00645440\n",
      "Iteration 6965, loss = 0.00643238\n",
      "Iteration 6966, loss = 0.00641807\n",
      "Iteration 6967, loss = 0.00641185\n",
      "Iteration 6968, loss = 0.00641294\n",
      "Iteration 6969, loss = 0.00641852\n",
      "Iteration 6970, loss = 0.00642479\n",
      "Iteration 6971, loss = 0.00642996\n",
      "Iteration 6972, loss = 0.00643074\n",
      "Iteration 6973, loss = 0.00642924\n",
      "Iteration 6974, loss = 0.00642435\n",
      "Iteration 6975, loss = 0.00641940\n",
      "Iteration 6976, loss = 0.00641459\n",
      "Iteration 6977, loss = 0.00641148\n",
      "Iteration 6978, loss = 0.00640960\n",
      "Iteration 6979, loss = 0.00640900\n",
      "Iteration 6980, loss = 0.00640854\n",
      "Iteration 6981, loss = 0.00640802\n",
      "Iteration 6982, loss = 0.00640691\n",
      "Iteration 6983, loss = 0.00640523\n",
      "Iteration 6984, loss = 0.00640328\n",
      "Iteration 6985, loss = 0.00640123\n",
      "Iteration 6986, loss = 0.00639951\n",
      "Iteration 6987, loss = 0.00639822\n",
      "Iteration 6988, loss = 0.00639748\n",
      "Iteration 6989, loss = 0.00639717\n",
      "Iteration 6990, loss = 0.00639721\n",
      "Iteration 6991, loss = 0.00639733\n",
      "Iteration 6992, loss = 0.00639751\n",
      "Iteration 6993, loss = 0.00639741\n",
      "Iteration 6994, loss = 0.00639724\n",
      "Iteration 6995, loss = 0.00639667\n",
      "Iteration 6996, loss = 0.00639605\n",
      "Iteration 6997, loss = 0.00639510\n",
      "Iteration 6998, loss = 0.00639421\n",
      "Iteration 6999, loss = 0.00639313\n",
      "Iteration 7000, loss = 0.00639221\n",
      "Iteration 7001, loss = 0.00639124\n",
      "Iteration 7002, loss = 0.00639051\n",
      "Iteration 7003, loss = 0.00638982\n",
      "Iteration 7004, loss = 0.00638943\n",
      "Iteration 7005, loss = 0.00638918\n",
      "Iteration 7006, loss = 0.00638934\n",
      "Iteration 7007, loss = 0.00638977\n",
      "Iteration 7008, loss = 0.00639087\n",
      "Iteration 7009, loss = 0.00639245\n",
      "Iteration 7010, loss = 0.00639531\n",
      "Iteration 7011, loss = 0.00639906\n",
      "Iteration 7012, loss = 0.00640541\n",
      "Iteration 7013, loss = 0.00641330\n",
      "Iteration 7014, loss = 0.00642657\n",
      "Iteration 7015, loss = 0.00644190\n",
      "Iteration 7016, loss = 0.00646801\n",
      "Iteration 7017, loss = 0.00649415\n",
      "Iteration 7018, loss = 0.00653908\n",
      "Iteration 7019, loss = 0.00657189\n",
      "Iteration 7020, loss = 0.00662814\n",
      "Iteration 7021, loss = 0.00664172\n",
      "Iteration 7022, loss = 0.00667021\n",
      "Iteration 7023, loss = 0.00663099\n",
      "Iteration 7024, loss = 0.00659971\n",
      "Iteration 7025, loss = 0.00652732\n",
      "Iteration 7026, loss = 0.00647871\n",
      "Iteration 7027, loss = 0.00643508\n",
      "Iteration 7028, loss = 0.00642030\n",
      "Iteration 7029, loss = 0.00641933\n",
      "Iteration 7030, loss = 0.00642154\n",
      "Iteration 7031, loss = 0.00642345\n",
      "Iteration 7032, loss = 0.00641511\n",
      "Iteration 7033, loss = 0.00640775\n",
      "Iteration 7034, loss = 0.00640231\n",
      "Iteration 7035, loss = 0.00640617\n",
      "Iteration 7036, loss = 0.00641097\n",
      "Iteration 7037, loss = 0.00641729\n",
      "Iteration 7038, loss = 0.00641416\n",
      "Iteration 7039, loss = 0.00640343\n",
      "Iteration 7040, loss = 0.00638740\n",
      "Iteration 7041, loss = 0.00637162\n",
      "Iteration 7042, loss = 0.00636205\n",
      "Iteration 7043, loss = 0.00636058\n",
      "Iteration 7044, loss = 0.00636572\n",
      "Iteration 7045, loss = 0.00637343\n",
      "Iteration 7046, loss = 0.00637972\n",
      "Iteration 7047, loss = 0.00638159\n",
      "Iteration 7048, loss = 0.00637923\n",
      "Iteration 7049, loss = 0.00637334\n",
      "Iteration 7050, loss = 0.00636723\n",
      "Iteration 7051, loss = 0.00636188\n",
      "Iteration 7052, loss = 0.00635887\n",
      "Iteration 7053, loss = 0.00635755\n",
      "Iteration 7054, loss = 0.00635729\n",
      "Iteration 7055, loss = 0.00635700\n",
      "Iteration 7056, loss = 0.00635633\n",
      "Iteration 7057, loss = 0.00635498\n",
      "Iteration 7058, loss = 0.00635348\n",
      "Iteration 7059, loss = 0.00635214\n",
      "Iteration 7060, loss = 0.00635127\n",
      "Iteration 7061, loss = 0.00635102\n",
      "Iteration 7062, loss = 0.00635108\n",
      "Iteration 7063, loss = 0.00635130\n",
      "Iteration 7064, loss = 0.00635121\n",
      "Iteration 7065, loss = 0.00635081\n",
      "Iteration 7066, loss = 0.00634981\n",
      "Iteration 7067, loss = 0.00634852\n",
      "Iteration 7068, loss = 0.00634690\n",
      "Iteration 7069, loss = 0.00634529\n",
      "Iteration 7070, loss = 0.00634375\n",
      "Iteration 7071, loss = 0.00634245\n",
      "Iteration 7072, loss = 0.00634138\n",
      "Iteration 7073, loss = 0.00634054\n",
      "Iteration 7074, loss = 0.00633986\n",
      "Iteration 7075, loss = 0.00633928\n",
      "Iteration 7076, loss = 0.00633874\n",
      "Iteration 7077, loss = 0.00633818\n",
      "Iteration 7078, loss = 0.00633760\n",
      "Iteration 7079, loss = 0.00633696\n",
      "Iteration 7080, loss = 0.00633628\n",
      "Iteration 7081, loss = 0.00633556\n",
      "Iteration 7082, loss = 0.00633484\n",
      "Iteration 7083, loss = 0.00633410\n",
      "Iteration 7084, loss = 0.00633337\n",
      "Iteration 7085, loss = 0.00633266\n",
      "Iteration 7086, loss = 0.00633197\n",
      "Iteration 7087, loss = 0.00633130\n",
      "Iteration 7088, loss = 0.00633066\n",
      "Iteration 7089, loss = 0.00633005\n",
      "Iteration 7090, loss = 0.00632948\n",
      "Iteration 7091, loss = 0.00632894\n",
      "Iteration 7092, loss = 0.00632846\n",
      "Iteration 7093, loss = 0.00632804\n",
      "Iteration 7094, loss = 0.00632772\n",
      "Iteration 7095, loss = 0.00632753\n",
      "Iteration 7096, loss = 0.00632755\n",
      "Iteration 7097, loss = 0.00632782\n",
      "Iteration 7098, loss = 0.00632859\n",
      "Iteration 7099, loss = 0.00632986\n",
      "Iteration 7100, loss = 0.00633229\n",
      "Iteration 7101, loss = 0.00633576\n",
      "Iteration 7102, loss = 0.00634197\n",
      "Iteration 7103, loss = 0.00635017\n",
      "Iteration 7104, loss = 0.00636488\n",
      "Iteration 7105, loss = 0.00638241\n",
      "Iteration 7106, loss = 0.00641478\n",
      "Iteration 7107, loss = 0.00644651\n",
      "Iteration 7108, loss = 0.00650717\n",
      "Iteration 7109, loss = 0.00654414\n",
      "Iteration 7110, loss = 0.00661957\n",
      "Iteration 7111, loss = 0.00661330\n",
      "Iteration 7112, loss = 0.00663144\n",
      "Iteration 7113, loss = 0.00654656\n",
      "Iteration 7114, loss = 0.00648208\n",
      "Iteration 7115, loss = 0.00640699\n",
      "Iteration 7116, loss = 0.00638056\n",
      "Iteration 7117, loss = 0.00639638\n",
      "Iteration 7118, loss = 0.00646308\n",
      "Iteration 7119, loss = 0.00652587\n",
      "Iteration 7120, loss = 0.00659258\n",
      "Iteration 7121, loss = 0.00657837\n",
      "Iteration 7122, loss = 0.00653276\n",
      "Iteration 7123, loss = 0.00643445\n",
      "Iteration 7124, loss = 0.00635973\n",
      "Iteration 7125, loss = 0.00632507\n",
      "Iteration 7126, loss = 0.00633657\n",
      "Iteration 7127, loss = 0.00636983\n",
      "Iteration 7128, loss = 0.00639995\n",
      "Iteration 7129, loss = 0.00639865\n",
      "Iteration 7130, loss = 0.00637342\n",
      "Iteration 7131, loss = 0.00633730\n",
      "Iteration 7132, loss = 0.00631332\n",
      "Iteration 7133, loss = 0.00631255\n",
      "Iteration 7134, loss = 0.00632882\n",
      "Iteration 7135, loss = 0.00634811\n",
      "Iteration 7136, loss = 0.00635082\n",
      "Iteration 7137, loss = 0.00633982\n",
      "Iteration 7138, loss = 0.00631895\n",
      "Iteration 7139, loss = 0.00630352\n",
      "Iteration 7140, loss = 0.00630004\n",
      "Iteration 7141, loss = 0.00630670\n",
      "Iteration 7142, loss = 0.00631661\n",
      "Iteration 7143, loss = 0.00632120\n",
      "Iteration 7144, loss = 0.00631917\n",
      "Iteration 7145, loss = 0.00631022\n",
      "Iteration 7146, loss = 0.00630080\n",
      "Iteration 7147, loss = 0.00629503\n",
      "Iteration 7148, loss = 0.00629482\n",
      "Iteration 7149, loss = 0.00629845\n",
      "Iteration 7150, loss = 0.00630228\n",
      "Iteration 7151, loss = 0.00630351\n",
      "Iteration 7152, loss = 0.00630099\n",
      "Iteration 7153, loss = 0.00629658\n",
      "Iteration 7154, loss = 0.00629227\n",
      "Iteration 7155, loss = 0.00628994\n",
      "Iteration 7156, loss = 0.00628979\n",
      "Iteration 7157, loss = 0.00629094\n",
      "Iteration 7158, loss = 0.00629207\n",
      "Iteration 7159, loss = 0.00629219\n",
      "Iteration 7160, loss = 0.00629100\n",
      "Iteration 7161, loss = 0.00628882\n",
      "Iteration 7162, loss = 0.00628653\n",
      "Iteration 7163, loss = 0.00628479\n",
      "Iteration 7164, loss = 0.00628398\n",
      "Iteration 7165, loss = 0.00628393\n",
      "Iteration 7166, loss = 0.00628419\n",
      "Iteration 7167, loss = 0.00628427\n",
      "Iteration 7168, loss = 0.00628387\n",
      "Iteration 7169, loss = 0.00628298\n",
      "Iteration 7170, loss = 0.00628180\n",
      "Iteration 7171, loss = 0.00628055\n",
      "Iteration 7172, loss = 0.00627947\n",
      "Iteration 7173, loss = 0.00627865\n",
      "Iteration 7174, loss = 0.00627809\n",
      "Iteration 7175, loss = 0.00627771\n",
      "Iteration 7176, loss = 0.00627737\n",
      "Iteration 7177, loss = 0.00627695\n",
      "Iteration 7178, loss = 0.00627638\n",
      "Iteration 7179, loss = 0.00627565\n",
      "Iteration 7180, loss = 0.00627483\n",
      "Iteration 7181, loss = 0.00627399\n",
      "Iteration 7182, loss = 0.00627320\n",
      "Iteration 7183, loss = 0.00627248\n",
      "Iteration 7184, loss = 0.00627184\n",
      "Iteration 7185, loss = 0.00627128\n",
      "Iteration 7186, loss = 0.00627076\n",
      "Iteration 7187, loss = 0.00627024\n",
      "Iteration 7188, loss = 0.00626972\n",
      "Iteration 7189, loss = 0.00626916\n",
      "Iteration 7190, loss = 0.00626856\n",
      "Iteration 7191, loss = 0.00626793\n",
      "Iteration 7192, loss = 0.00626727\n",
      "Iteration 7193, loss = 0.00626661\n",
      "Iteration 7194, loss = 0.00626595\n",
      "Iteration 7195, loss = 0.00626531\n",
      "Iteration 7196, loss = 0.00626468\n",
      "Iteration 7197, loss = 0.00626406\n",
      "Iteration 7198, loss = 0.00626346\n",
      "Iteration 7199, loss = 0.00626288\n",
      "Iteration 7200, loss = 0.00626230\n",
      "Iteration 7201, loss = 0.00626173\n",
      "Iteration 7202, loss = 0.00626117\n",
      "Iteration 7203, loss = 0.00626061\n",
      "Iteration 7204, loss = 0.00626006\n",
      "Iteration 7205, loss = 0.00625953\n",
      "Iteration 7206, loss = 0.00625903\n",
      "Iteration 7207, loss = 0.00625858\n",
      "Iteration 7208, loss = 0.00625823\n",
      "Iteration 7209, loss = 0.00625803\n",
      "Iteration 7210, loss = 0.00625813\n",
      "Iteration 7211, loss = 0.00625868\n",
      "Iteration 7212, loss = 0.00626014\n",
      "Iteration 7213, loss = 0.00626297\n",
      "Iteration 7214, loss = 0.00626872\n",
      "Iteration 7215, loss = 0.00627852\n",
      "Iteration 7216, loss = 0.00629800\n",
      "Iteration 7217, loss = 0.00632798\n",
      "Iteration 7218, loss = 0.00638905\n",
      "Iteration 7219, loss = 0.00646518\n",
      "Iteration 7220, loss = 0.00662036\n",
      "Iteration 7221, loss = 0.00672509\n",
      "Iteration 7222, loss = 0.00692035\n",
      "Iteration 7223, loss = 0.00686861\n",
      "Iteration 7224, loss = 0.00683339\n",
      "Iteration 7225, loss = 0.00663484\n",
      "Iteration 7226, loss = 0.00653512\n",
      "Iteration 7227, loss = 0.00645101\n",
      "Iteration 7228, loss = 0.00644096\n",
      "Iteration 7229, loss = 0.00642491\n",
      "Iteration 7230, loss = 0.00636626\n",
      "Iteration 7231, loss = 0.00633530\n",
      "Iteration 7232, loss = 0.00635965\n",
      "Iteration 7233, loss = 0.00643420\n",
      "Iteration 7234, loss = 0.00643977\n",
      "Iteration 7235, loss = 0.00637729\n",
      "Iteration 7236, loss = 0.00628016\n",
      "Iteration 7237, loss = 0.00624317\n",
      "Iteration 7238, loss = 0.00628284\n",
      "Iteration 7239, loss = 0.00633782\n",
      "Iteration 7240, loss = 0.00635164\n",
      "Iteration 7241, loss = 0.00630915\n",
      "Iteration 7242, loss = 0.00626865\n",
      "Iteration 7243, loss = 0.00625680\n",
      "Iteration 7244, loss = 0.00626890\n",
      "Iteration 7245, loss = 0.00627665\n",
      "Iteration 7246, loss = 0.00627164\n",
      "Iteration 7247, loss = 0.00626501\n",
      "Iteration 7248, loss = 0.00626432\n",
      "Iteration 7249, loss = 0.00626776\n",
      "Iteration 7250, loss = 0.00626148\n",
      "Iteration 7251, loss = 0.00624818\n",
      "Iteration 7252, loss = 0.00623736\n",
      "Iteration 7253, loss = 0.00623938\n",
      "Iteration 7254, loss = 0.00625017\n",
      "Iteration 7255, loss = 0.00625628\n",
      "Iteration 7256, loss = 0.00625199\n",
      "Iteration 7257, loss = 0.00623983\n",
      "Iteration 7258, loss = 0.00623147\n",
      "Iteration 7259, loss = 0.00623144\n",
      "Iteration 7260, loss = 0.00623587\n",
      "Iteration 7261, loss = 0.00623854\n",
      "Iteration 7262, loss = 0.00623657\n",
      "Iteration 7263, loss = 0.00623333\n",
      "Iteration 7264, loss = 0.00623131\n",
      "Iteration 7265, loss = 0.00623083\n",
      "Iteration 7266, loss = 0.00622991\n",
      "Iteration 7267, loss = 0.00622774\n",
      "Iteration 7268, loss = 0.00622561\n",
      "Iteration 7269, loss = 0.00622496\n",
      "Iteration 7270, loss = 0.00622592\n",
      "Iteration 7271, loss = 0.00622685\n",
      "Iteration 7272, loss = 0.00622628\n",
      "Iteration 7273, loss = 0.00622401\n",
      "Iteration 7274, loss = 0.00622147\n",
      "Iteration 7275, loss = 0.00622001\n",
      "Iteration 7276, loss = 0.00621993\n",
      "Iteration 7277, loss = 0.00622038\n",
      "Iteration 7278, loss = 0.00622038\n",
      "Iteration 7279, loss = 0.00621962\n",
      "Iteration 7280, loss = 0.00621846\n",
      "Iteration 7281, loss = 0.00621748\n",
      "Iteration 7282, loss = 0.00621683\n",
      "Iteration 7283, loss = 0.00621629\n",
      "Iteration 7284, loss = 0.00621561\n",
      "Iteration 7285, loss = 0.00621478\n",
      "Iteration 7286, loss = 0.00621399\n",
      "Iteration 7287, loss = 0.00621342\n",
      "Iteration 7288, loss = 0.00621307\n",
      "Iteration 7289, loss = 0.00621276\n",
      "Iteration 7290, loss = 0.00621230\n",
      "Iteration 7291, loss = 0.00621160\n",
      "Iteration 7292, loss = 0.00621076\n",
      "Iteration 7293, loss = 0.00620994\n",
      "Iteration 7294, loss = 0.00620926\n",
      "Iteration 7295, loss = 0.00620874\n",
      "Iteration 7296, loss = 0.00620828\n",
      "Iteration 7297, loss = 0.00620779\n",
      "Iteration 7298, loss = 0.00620723\n",
      "Iteration 7299, loss = 0.00620663\n",
      "Iteration 7300, loss = 0.00620604\n",
      "Iteration 7301, loss = 0.00620548\n",
      "Iteration 7302, loss = 0.00620494\n",
      "Iteration 7303, loss = 0.00620438\n",
      "Iteration 7304, loss = 0.00620380\n",
      "Iteration 7305, loss = 0.00620318\n",
      "Iteration 7306, loss = 0.00620256\n",
      "Iteration 7307, loss = 0.00620195\n",
      "Iteration 7308, loss = 0.00620138\n",
      "Iteration 7309, loss = 0.00620085\n",
      "Iteration 7310, loss = 0.00620032\n",
      "Iteration 7311, loss = 0.00619979\n",
      "Iteration 7312, loss = 0.00619924\n",
      "Iteration 7313, loss = 0.00619868\n",
      "Iteration 7314, loss = 0.00619811\n",
      "Iteration 7315, loss = 0.00619754\n",
      "Iteration 7316, loss = 0.00619698\n",
      "Iteration 7317, loss = 0.00619643\n",
      "Iteration 7318, loss = 0.00619587\n",
      "Iteration 7319, loss = 0.00619532\n",
      "Iteration 7320, loss = 0.00619476\n",
      "Iteration 7321, loss = 0.00619419\n",
      "Iteration 7322, loss = 0.00619362\n",
      "Iteration 7323, loss = 0.00619306\n",
      "Iteration 7324, loss = 0.00619249\n",
      "Iteration 7325, loss = 0.00619194\n",
      "Iteration 7326, loss = 0.00619138\n",
      "Iteration 7327, loss = 0.00619083\n",
      "Iteration 7328, loss = 0.00619028\n",
      "Iteration 7329, loss = 0.00618972\n",
      "Iteration 7330, loss = 0.00618917\n",
      "Iteration 7331, loss = 0.00618861\n",
      "Iteration 7332, loss = 0.00618806\n",
      "Iteration 7333, loss = 0.00618750\n",
      "Iteration 7334, loss = 0.00618695\n",
      "Iteration 7335, loss = 0.00618640\n",
      "Iteration 7336, loss = 0.00618585\n",
      "Iteration 7337, loss = 0.00618530\n",
      "Iteration 7338, loss = 0.00618475\n",
      "Iteration 7339, loss = 0.00618420\n",
      "Iteration 7340, loss = 0.00618365\n",
      "Iteration 7341, loss = 0.00618310\n",
      "Iteration 7342, loss = 0.00618255\n",
      "Iteration 7343, loss = 0.00618200\n",
      "Iteration 7344, loss = 0.00618145\n",
      "Iteration 7345, loss = 0.00618091\n",
      "Iteration 7346, loss = 0.00618037\n",
      "Iteration 7347, loss = 0.00617983\n",
      "Iteration 7348, loss = 0.00617930\n",
      "Iteration 7349, loss = 0.00617878\n",
      "Iteration 7350, loss = 0.00617827\n",
      "Iteration 7351, loss = 0.00617778\n",
      "Iteration 7352, loss = 0.00617732\n",
      "Iteration 7353, loss = 0.00617691\n",
      "Iteration 7354, loss = 0.00617657\n",
      "Iteration 7355, loss = 0.00617634\n",
      "Iteration 7356, loss = 0.00617627\n",
      "Iteration 7357, loss = 0.00617650\n",
      "Iteration 7358, loss = 0.00617713\n",
      "Iteration 7359, loss = 0.00617853\n",
      "Iteration 7360, loss = 0.00618094\n",
      "Iteration 7361, loss = 0.00618544\n",
      "Iteration 7362, loss = 0.00619242\n",
      "Iteration 7363, loss = 0.00620521\n",
      "Iteration 7364, loss = 0.00622368\n",
      "Iteration 7365, loss = 0.00625815\n",
      "Iteration 7366, loss = 0.00630226\n",
      "Iteration 7367, loss = 0.00638663\n",
      "Iteration 7368, loss = 0.00646811\n",
      "Iteration 7369, loss = 0.00662400\n",
      "Iteration 7370, loss = 0.00668274\n",
      "Iteration 7371, loss = 0.00680207\n",
      "Iteration 7372, loss = 0.00667837\n",
      "Iteration 7373, loss = 0.00657641\n",
      "Iteration 7374, loss = 0.00636805\n",
      "Iteration 7375, loss = 0.00624941\n",
      "Iteration 7376, loss = 0.00621377\n",
      "Iteration 7377, loss = 0.00625878\n",
      "Iteration 7378, loss = 0.00634255\n",
      "Iteration 7379, loss = 0.00636681\n",
      "Iteration 7380, loss = 0.00635602\n",
      "Iteration 7381, loss = 0.00627886\n",
      "Iteration 7382, loss = 0.00621712\n",
      "Iteration 7383, loss = 0.00619017\n",
      "Iteration 7384, loss = 0.00620699\n",
      "Iteration 7385, loss = 0.00624391\n",
      "Iteration 7386, loss = 0.00625617\n",
      "Iteration 7387, loss = 0.00624668\n",
      "Iteration 7388, loss = 0.00621048\n",
      "Iteration 7389, loss = 0.00618233\n",
      "Iteration 7390, loss = 0.00617332\n",
      "Iteration 7391, loss = 0.00618417\n",
      "Iteration 7392, loss = 0.00620124\n",
      "Iteration 7393, loss = 0.00620493\n",
      "Iteration 7394, loss = 0.00619715\n",
      "Iteration 7395, loss = 0.00617888\n",
      "Iteration 7396, loss = 0.00616560\n",
      "Iteration 7397, loss = 0.00616218\n",
      "Iteration 7398, loss = 0.00616788\n",
      "Iteration 7399, loss = 0.00617570\n",
      "Iteration 7400, loss = 0.00617701\n",
      "Iteration 7401, loss = 0.00617252\n",
      "Iteration 7402, loss = 0.00616339\n",
      "Iteration 7403, loss = 0.00615646\n",
      "Iteration 7404, loss = 0.00615431\n",
      "Iteration 7405, loss = 0.00615660\n",
      "Iteration 7406, loss = 0.00615996\n",
      "Iteration 7407, loss = 0.00616049\n",
      "Iteration 7408, loss = 0.00615826\n",
      "Iteration 7409, loss = 0.00615393\n",
      "Iteration 7410, loss = 0.00615029\n",
      "Iteration 7411, loss = 0.00614866\n",
      "Iteration 7412, loss = 0.00614908\n",
      "Iteration 7413, loss = 0.00615020\n",
      "Iteration 7414, loss = 0.00615038\n",
      "Iteration 7415, loss = 0.00614940\n",
      "Iteration 7416, loss = 0.00614736\n",
      "Iteration 7417, loss = 0.00614531\n",
      "Iteration 7418, loss = 0.00614395\n",
      "Iteration 7419, loss = 0.00614355\n",
      "Iteration 7420, loss = 0.00614366\n",
      "Iteration 7421, loss = 0.00614362\n",
      "Iteration 7422, loss = 0.00614312\n",
      "Iteration 7423, loss = 0.00614205\n",
      "Iteration 7424, loss = 0.00614079\n",
      "Iteration 7425, loss = 0.00613966\n",
      "Iteration 7426, loss = 0.00613897\n",
      "Iteration 7427, loss = 0.00613864\n",
      "Iteration 7428, loss = 0.00613844\n",
      "Iteration 7429, loss = 0.00613812\n",
      "Iteration 7430, loss = 0.00613754\n",
      "Iteration 7431, loss = 0.00613672\n",
      "Iteration 7432, loss = 0.00613579\n",
      "Iteration 7433, loss = 0.00613494\n",
      "Iteration 7434, loss = 0.00613428\n",
      "Iteration 7435, loss = 0.00613382\n",
      "Iteration 7436, loss = 0.00613348\n",
      "Iteration 7437, loss = 0.00613316\n",
      "Iteration 7438, loss = 0.00613274\n",
      "Iteration 7439, loss = 0.00613218\n",
      "Iteration 7440, loss = 0.00613149\n",
      "Iteration 7441, loss = 0.00613076\n",
      "Iteration 7442, loss = 0.00613004\n",
      "Iteration 7443, loss = 0.00612940\n",
      "Iteration 7444, loss = 0.00612885\n",
      "Iteration 7445, loss = 0.00612838\n",
      "Iteration 7446, loss = 0.00612795\n",
      "Iteration 7447, loss = 0.00612751\n",
      "Iteration 7448, loss = 0.00612705\n",
      "Iteration 7449, loss = 0.00612654\n",
      "Iteration 7450, loss = 0.00612599\n",
      "Iteration 7451, loss = 0.00612541\n",
      "Iteration 7452, loss = 0.00612483\n",
      "Iteration 7453, loss = 0.00612427\n",
      "Iteration 7454, loss = 0.00612372\n",
      "Iteration 7455, loss = 0.00612319\n",
      "Iteration 7456, loss = 0.00612268\n",
      "Iteration 7457, loss = 0.00612217\n",
      "Iteration 7458, loss = 0.00612167\n",
      "Iteration 7459, loss = 0.00612115\n",
      "Iteration 7460, loss = 0.00612063\n",
      "Iteration 7461, loss = 0.00612010\n",
      "Iteration 7462, loss = 0.00611956\n",
      "Iteration 7463, loss = 0.00611903\n",
      "Iteration 7464, loss = 0.00611850\n",
      "Iteration 7465, loss = 0.00611797\n",
      "Iteration 7466, loss = 0.00611746\n",
      "Iteration 7467, loss = 0.00611696\n",
      "Iteration 7468, loss = 0.00611647\n",
      "Iteration 7469, loss = 0.00611599\n",
      "Iteration 7470, loss = 0.00611554\n",
      "Iteration 7471, loss = 0.00611511\n",
      "Iteration 7472, loss = 0.00611474\n",
      "Iteration 7473, loss = 0.00611442\n",
      "Iteration 7474, loss = 0.00611421\n",
      "Iteration 7475, loss = 0.00611415\n",
      "Iteration 7476, loss = 0.00611439\n",
      "Iteration 7477, loss = 0.00611496\n",
      "Iteration 7478, loss = 0.00611637\n",
      "Iteration 7479, loss = 0.00611856\n",
      "Iteration 7480, loss = 0.00612310\n",
      "Iteration 7481, loss = 0.00612927\n",
      "Iteration 7482, loss = 0.00614220\n",
      "Iteration 7483, loss = 0.00615694\n",
      "Iteration 7484, loss = 0.00618987\n",
      "Iteration 7485, loss = 0.00621513\n",
      "Iteration 7486, loss = 0.00627832\n",
      "Iteration 7487, loss = 0.00628673\n",
      "Iteration 7488, loss = 0.00633712\n",
      "Iteration 7489, loss = 0.00627941\n",
      "Iteration 7490, loss = 0.00624431\n",
      "Iteration 7491, loss = 0.00617569\n",
      "Iteration 7492, loss = 0.00613362\n",
      "Iteration 7493, loss = 0.00611830\n",
      "Iteration 7494, loss = 0.00612555\n",
      "Iteration 7495, loss = 0.00614514\n",
      "Iteration 7496, loss = 0.00615498\n",
      "Iteration 7497, loss = 0.00616049\n",
      "Iteration 7498, loss = 0.00615241\n",
      "Iteration 7499, loss = 0.00613832\n",
      "Iteration 7500, loss = 0.00612158\n",
      "Iteration 7501, loss = 0.00610956\n",
      "Iteration 7502, loss = 0.00610816\n",
      "Iteration 7503, loss = 0.00611602\n",
      "Iteration 7504, loss = 0.00612663\n",
      "Iteration 7505, loss = 0.00612977\n",
      "Iteration 7506, loss = 0.00612317\n",
      "Iteration 7507, loss = 0.00611054\n",
      "Iteration 7508, loss = 0.00610171\n",
      "Iteration 7509, loss = 0.00610141\n",
      "Iteration 7510, loss = 0.00610770\n",
      "Iteration 7511, loss = 0.00611387\n",
      "Iteration 7512, loss = 0.00611573\n",
      "Iteration 7513, loss = 0.00611294\n",
      "Iteration 7514, loss = 0.00610989\n",
      "Iteration 7515, loss = 0.00610900\n",
      "Iteration 7516, loss = 0.00611210\n",
      "Iteration 7517, loss = 0.00611651\n",
      "Iteration 7518, loss = 0.00612319\n",
      "Iteration 7519, loss = 0.00612921\n",
      "Iteration 7520, loss = 0.00613929\n",
      "Iteration 7521, loss = 0.00614842\n",
      "Iteration 7522, loss = 0.00616420\n",
      "Iteration 7523, loss = 0.00617497\n",
      "Iteration 7524, loss = 0.00619545\n",
      "Iteration 7525, loss = 0.00620406\n",
      "Iteration 7526, loss = 0.00622513\n",
      "Iteration 7527, loss = 0.00622244\n",
      "Iteration 7528, loss = 0.00622898\n",
      "Iteration 7529, loss = 0.00620490\n",
      "Iteration 7530, loss = 0.00618636\n",
      "Iteration 7531, loss = 0.00615139\n",
      "Iteration 7532, loss = 0.00612474\n",
      "Iteration 7533, loss = 0.00610186\n",
      "Iteration 7534, loss = 0.00608926\n",
      "Iteration 7535, loss = 0.00608524\n",
      "Iteration 7536, loss = 0.00608801\n",
      "Iteration 7537, loss = 0.00609472\n",
      "Iteration 7538, loss = 0.00610220\n",
      "Iteration 7539, loss = 0.00610986\n",
      "Iteration 7540, loss = 0.00611317\n",
      "Iteration 7541, loss = 0.00611534\n",
      "Iteration 7542, loss = 0.00611191\n",
      "Iteration 7543, loss = 0.00610790\n",
      "Iteration 7544, loss = 0.00610084\n",
      "Iteration 7545, loss = 0.00609448\n",
      "Iteration 7546, loss = 0.00608802\n",
      "Iteration 7547, loss = 0.00608302\n",
      "Iteration 7548, loss = 0.00607925\n",
      "Iteration 7549, loss = 0.00607705\n",
      "Iteration 7550, loss = 0.00607618\n",
      "Iteration 7551, loss = 0.00607642\n",
      "Iteration 7552, loss = 0.00607742\n",
      "Iteration 7553, loss = 0.00607867\n",
      "Iteration 7554, loss = 0.00608004\n",
      "Iteration 7555, loss = 0.00608096\n",
      "Iteration 7556, loss = 0.00608172\n",
      "Iteration 7557, loss = 0.00608176\n",
      "Iteration 7558, loss = 0.00608168\n",
      "Iteration 7559, loss = 0.00608084\n",
      "Iteration 7560, loss = 0.00608000\n",
      "Iteration 7561, loss = 0.00607855\n",
      "Iteration 7562, loss = 0.00607724\n",
      "Iteration 7563, loss = 0.00607558\n",
      "Iteration 7564, loss = 0.00607415\n",
      "Iteration 7565, loss = 0.00607261\n",
      "Iteration 7566, loss = 0.00607131\n",
      "Iteration 7567, loss = 0.00607001\n",
      "Iteration 7568, loss = 0.00606891\n",
      "Iteration 7569, loss = 0.00606785\n",
      "Iteration 7570, loss = 0.00606696\n",
      "Iteration 7571, loss = 0.00606613\n",
      "Iteration 7572, loss = 0.00606542\n",
      "Iteration 7573, loss = 0.00606476\n",
      "Iteration 7574, loss = 0.00606420\n",
      "Iteration 7575, loss = 0.00606368\n",
      "Iteration 7576, loss = 0.00606324\n",
      "Iteration 7577, loss = 0.00606286\n",
      "Iteration 7578, loss = 0.00606261\n",
      "Iteration 7579, loss = 0.00606244\n",
      "Iteration 7580, loss = 0.00606248\n",
      "Iteration 7581, loss = 0.00606269\n",
      "Iteration 7582, loss = 0.00606330\n",
      "Iteration 7583, loss = 0.00606424\n",
      "Iteration 7584, loss = 0.00606603\n",
      "Iteration 7585, loss = 0.00606851\n",
      "Iteration 7586, loss = 0.00607291\n",
      "Iteration 7587, loss = 0.00607863\n",
      "Iteration 7588, loss = 0.00608887\n",
      "Iteration 7589, loss = 0.00610116\n",
      "Iteration 7590, loss = 0.00612412\n",
      "Iteration 7591, loss = 0.00614793\n",
      "Iteration 7592, loss = 0.00619491\n",
      "Iteration 7593, loss = 0.00623062\n",
      "Iteration 7594, loss = 0.00630592\n",
      "Iteration 7595, loss = 0.00633074\n",
      "Iteration 7596, loss = 0.00639711\n",
      "Iteration 7597, loss = 0.00637006\n",
      "Iteration 7598, loss = 0.00636699\n",
      "Iteration 7599, loss = 0.00628784\n",
      "Iteration 7600, loss = 0.00622979\n",
      "Iteration 7601, loss = 0.00615354\n",
      "Iteration 7602, loss = 0.00611075\n",
      "Iteration 7603, loss = 0.00609155\n",
      "Iteration 7604, loss = 0.00609062\n",
      "Iteration 7605, loss = 0.00610516\n",
      "Iteration 7606, loss = 0.00611505\n",
      "Iteration 7607, loss = 0.00612328\n",
      "Iteration 7608, loss = 0.00611925\n",
      "Iteration 7609, loss = 0.00611590\n",
      "Iteration 7610, loss = 0.00610375\n",
      "Iteration 7611, loss = 0.00609559\n",
      "Iteration 7612, loss = 0.00608423\n",
      "Iteration 7613, loss = 0.00607286\n",
      "Iteration 7614, loss = 0.00606243\n",
      "Iteration 7615, loss = 0.00605514\n",
      "Iteration 7616, loss = 0.00605437\n",
      "Iteration 7617, loss = 0.00605934\n",
      "Iteration 7618, loss = 0.00606794\n",
      "Iteration 7619, loss = 0.00607272\n",
      "Iteration 7620, loss = 0.00607283\n",
      "Iteration 7621, loss = 0.00606519\n",
      "Iteration 7622, loss = 0.00605479\n",
      "Iteration 7623, loss = 0.00604507\n",
      "Iteration 7624, loss = 0.00603937\n",
      "Iteration 7625, loss = 0.00603846\n",
      "Iteration 7626, loss = 0.00604089\n",
      "Iteration 7627, loss = 0.00604453\n",
      "Iteration 7628, loss = 0.00604727\n",
      "Iteration 7629, loss = 0.00604840\n",
      "Iteration 7630, loss = 0.00604728\n",
      "Iteration 7631, loss = 0.00604465\n",
      "Iteration 7632, loss = 0.00604098\n",
      "Iteration 7633, loss = 0.00603735\n",
      "Iteration 7634, loss = 0.00603443\n",
      "Iteration 7635, loss = 0.00603280\n",
      "Iteration 7636, loss = 0.00603245\n",
      "Iteration 7637, loss = 0.00603299\n",
      "Iteration 7638, loss = 0.00603369\n",
      "Iteration 7639, loss = 0.00603414\n",
      "Iteration 7640, loss = 0.00603392\n",
      "Iteration 7641, loss = 0.00603317\n",
      "Iteration 7642, loss = 0.00603209\n",
      "Iteration 7643, loss = 0.00603089\n",
      "Iteration 7644, loss = 0.00602980\n",
      "Iteration 7645, loss = 0.00602880\n",
      "Iteration 7646, loss = 0.00602795\n",
      "Iteration 7647, loss = 0.00602718\n",
      "Iteration 7648, loss = 0.00602652\n",
      "Iteration 7649, loss = 0.00602591\n",
      "Iteration 7650, loss = 0.00602539\n",
      "Iteration 7651, loss = 0.00602488\n",
      "Iteration 7652, loss = 0.00602439\n",
      "Iteration 7653, loss = 0.00602384\n",
      "Iteration 7654, loss = 0.00602328\n",
      "Iteration 7655, loss = 0.00602270\n",
      "Iteration 7656, loss = 0.00602212\n",
      "Iteration 7657, loss = 0.00602157\n",
      "Iteration 7658, loss = 0.00602104\n",
      "Iteration 7659, loss = 0.00602053\n",
      "Iteration 7660, loss = 0.00602002\n",
      "Iteration 7661, loss = 0.00601951\n",
      "Iteration 7662, loss = 0.00601900\n",
      "Iteration 7663, loss = 0.00601850\n",
      "Iteration 7664, loss = 0.00601801\n",
      "Iteration 7665, loss = 0.00601756\n",
      "Iteration 7666, loss = 0.00601713\n",
      "Iteration 7667, loss = 0.00601675\n",
      "Iteration 7668, loss = 0.00601639\n",
      "Iteration 7669, loss = 0.00601612\n",
      "Iteration 7670, loss = 0.00601591\n",
      "Iteration 7671, loss = 0.00601586\n",
      "Iteration 7672, loss = 0.00601594\n",
      "Iteration 7673, loss = 0.00601637\n",
      "Iteration 7674, loss = 0.00601710\n",
      "Iteration 7675, loss = 0.00601862\n",
      "Iteration 7676, loss = 0.00602077\n",
      "Iteration 7677, loss = 0.00602488\n",
      "Iteration 7678, loss = 0.00603016\n",
      "Iteration 7679, loss = 0.00604053\n",
      "Iteration 7680, loss = 0.00605232\n",
      "Iteration 7681, loss = 0.00607709\n",
      "Iteration 7682, loss = 0.00609866\n",
      "Iteration 7683, loss = 0.00614895\n",
      "Iteration 7684, loss = 0.00617031\n",
      "Iteration 7685, loss = 0.00623478\n",
      "Iteration 7686, loss = 0.00622061\n",
      "Iteration 7687, loss = 0.00624296\n",
      "Iteration 7688, loss = 0.00620212\n",
      "Iteration 7689, loss = 0.00619392\n",
      "Iteration 7690, loss = 0.00617569\n",
      "Iteration 7691, loss = 0.00619254\n",
      "Iteration 7692, loss = 0.00618540\n",
      "Iteration 7693, loss = 0.00618727\n",
      "Iteration 7694, loss = 0.00615343\n",
      "Iteration 7695, loss = 0.00611456\n",
      "Iteration 7696, loss = 0.00607075\n",
      "Iteration 7697, loss = 0.00604066\n",
      "Iteration 7698, loss = 0.00602427\n",
      "Iteration 7699, loss = 0.00601979\n",
      "Iteration 7700, loss = 0.00602435\n",
      "Iteration 7701, loss = 0.00603581\n",
      "Iteration 7702, loss = 0.00605212\n",
      "Iteration 7703, loss = 0.00606169\n",
      "Iteration 7704, loss = 0.00606493\n",
      "Iteration 7705, loss = 0.00605059\n",
      "Iteration 7706, loss = 0.00603107\n",
      "Iteration 7707, loss = 0.00601106\n",
      "Iteration 7708, loss = 0.00600080\n",
      "Iteration 7709, loss = 0.00600059\n",
      "Iteration 7710, loss = 0.00600597\n",
      "Iteration 7711, loss = 0.00601081\n",
      "Iteration 7712, loss = 0.00601194\n",
      "Iteration 7713, loss = 0.00601050\n",
      "Iteration 7714, loss = 0.00600811\n",
      "Iteration 7715, loss = 0.00600770\n",
      "Iteration 7716, loss = 0.00600739\n",
      "Iteration 7717, loss = 0.00600677\n",
      "Iteration 7718, loss = 0.00600363\n",
      "Iteration 7719, loss = 0.00599926\n",
      "Iteration 7720, loss = 0.00599470\n",
      "Iteration 7721, loss = 0.00599168\n",
      "Iteration 7722, loss = 0.00599044\n",
      "Iteration 7723, loss = 0.00599050\n",
      "Iteration 7724, loss = 0.00599074\n",
      "Iteration 7725, loss = 0.00599074\n",
      "Iteration 7726, loss = 0.00599055\n",
      "Iteration 7727, loss = 0.00599041\n",
      "Iteration 7728, loss = 0.00599060\n",
      "Iteration 7729, loss = 0.00599073\n",
      "Iteration 7730, loss = 0.00599065\n",
      "Iteration 7731, loss = 0.00598967\n",
      "Iteration 7732, loss = 0.00598822\n",
      "Iteration 7733, loss = 0.00598631\n",
      "Iteration 7734, loss = 0.00598460\n",
      "Iteration 7735, loss = 0.00598326\n",
      "Iteration 7736, loss = 0.00598240\n",
      "Iteration 7737, loss = 0.00598183\n",
      "Iteration 7738, loss = 0.00598135\n",
      "Iteration 7739, loss = 0.00598081\n",
      "Iteration 7740, loss = 0.00598018\n",
      "Iteration 7741, loss = 0.00597957\n",
      "Iteration 7742, loss = 0.00597901\n",
      "Iteration 7743, loss = 0.00597856\n",
      "Iteration 7744, loss = 0.00597817\n",
      "Iteration 7745, loss = 0.00597781\n",
      "Iteration 7746, loss = 0.00597742\n",
      "Iteration 7747, loss = 0.00597701\n",
      "Iteration 7748, loss = 0.00597660\n",
      "Iteration 7749, loss = 0.00597625\n",
      "Iteration 7750, loss = 0.00597597\n",
      "Iteration 7751, loss = 0.00597579\n",
      "Iteration 7752, loss = 0.00597567\n",
      "Iteration 7753, loss = 0.00597565\n",
      "Iteration 7754, loss = 0.00597567\n",
      "Iteration 7755, loss = 0.00597586\n",
      "Iteration 7756, loss = 0.00597613\n",
      "Iteration 7757, loss = 0.00597677\n",
      "Iteration 7758, loss = 0.00597763\n",
      "Iteration 7759, loss = 0.00597919\n",
      "Iteration 7760, loss = 0.00598117\n",
      "Iteration 7761, loss = 0.00598451\n",
      "Iteration 7762, loss = 0.00598854\n",
      "Iteration 7763, loss = 0.00599529\n",
      "Iteration 7764, loss = 0.00600311\n",
      "Iteration 7765, loss = 0.00601643\n",
      "Iteration 7766, loss = 0.00603078\n",
      "Iteration 7767, loss = 0.00605595\n",
      "Iteration 7768, loss = 0.00607950\n",
      "Iteration 7769, loss = 0.00612236\n",
      "Iteration 7770, loss = 0.00615203\n",
      "Iteration 7771, loss = 0.00621027\n",
      "Iteration 7772, loss = 0.00622569\n",
      "Iteration 7773, loss = 0.00627216\n",
      "Iteration 7774, loss = 0.00623750\n",
      "Iteration 7775, loss = 0.00622748\n",
      "Iteration 7776, loss = 0.00614386\n",
      "Iteration 7777, loss = 0.00608491\n",
      "Iteration 7778, loss = 0.00601979\n",
      "Iteration 7779, loss = 0.00598644\n",
      "Iteration 7780, loss = 0.00598041\n",
      "Iteration 7781, loss = 0.00599544\n",
      "Iteration 7782, loss = 0.00602190\n",
      "Iteration 7783, loss = 0.00603916\n",
      "Iteration 7784, loss = 0.00605064\n",
      "Iteration 7785, loss = 0.00603879\n",
      "Iteration 7786, loss = 0.00602105\n",
      "Iteration 7787, loss = 0.00599523\n",
      "Iteration 7788, loss = 0.00597441\n",
      "Iteration 7789, loss = 0.00596226\n",
      "Iteration 7790, loss = 0.00596116\n",
      "Iteration 7791, loss = 0.00596889\n",
      "Iteration 7792, loss = 0.00597965\n",
      "Iteration 7793, loss = 0.00598922\n",
      "Iteration 7794, loss = 0.00599100\n",
      "Iteration 7795, loss = 0.00598686\n",
      "Iteration 7796, loss = 0.00597609\n",
      "Iteration 7797, loss = 0.00596497\n",
      "Iteration 7798, loss = 0.00595636\n",
      "Iteration 7799, loss = 0.00595287\n",
      "Iteration 7800, loss = 0.00595398\n",
      "Iteration 7801, loss = 0.00595751\n",
      "Iteration 7802, loss = 0.00596102\n",
      "Iteration 7803, loss = 0.00596249\n",
      "Iteration 7804, loss = 0.00596196\n",
      "Iteration 7805, loss = 0.00595951\n",
      "Iteration 7806, loss = 0.00595679\n",
      "Iteration 7807, loss = 0.00595416\n",
      "Iteration 7808, loss = 0.00595235\n",
      "Iteration 7809, loss = 0.00595103\n",
      "Iteration 7810, loss = 0.00595012\n",
      "Iteration 7811, loss = 0.00594937\n",
      "Iteration 7812, loss = 0.00594873\n",
      "Iteration 7813, loss = 0.00594825\n",
      "Iteration 7814, loss = 0.00594785\n",
      "Iteration 7815, loss = 0.00594758\n",
      "Iteration 7816, loss = 0.00594715\n",
      "Iteration 7817, loss = 0.00594663\n",
      "Iteration 7818, loss = 0.00594587\n",
      "Iteration 7819, loss = 0.00594505\n",
      "Iteration 7820, loss = 0.00594422\n",
      "Iteration 7821, loss = 0.00594355\n",
      "Iteration 7822, loss = 0.00594304\n",
      "Iteration 7823, loss = 0.00594269\n",
      "Iteration 7824, loss = 0.00594241\n",
      "Iteration 7825, loss = 0.00594208\n",
      "Iteration 7826, loss = 0.00594170\n",
      "Iteration 7827, loss = 0.00594115\n",
      "Iteration 7828, loss = 0.00594057\n",
      "Iteration 7829, loss = 0.00593989\n",
      "Iteration 7830, loss = 0.00593925\n",
      "Iteration 7831, loss = 0.00593860\n",
      "Iteration 7832, loss = 0.00593801\n",
      "Iteration 7833, loss = 0.00593742\n",
      "Iteration 7834, loss = 0.00593686\n",
      "Iteration 7835, loss = 0.00593630\n",
      "Iteration 7836, loss = 0.00593574\n",
      "Iteration 7837, loss = 0.00593519\n",
      "Iteration 7838, loss = 0.00593466\n",
      "Iteration 7839, loss = 0.00593416\n",
      "Iteration 7840, loss = 0.00593368\n",
      "Iteration 7841, loss = 0.00593322\n",
      "Iteration 7842, loss = 0.00593278\n",
      "Iteration 7843, loss = 0.00593235\n",
      "Iteration 7844, loss = 0.00593192\n",
      "Iteration 7845, loss = 0.00593148\n",
      "Iteration 7846, loss = 0.00593104\n",
      "Iteration 7847, loss = 0.00593059\n",
      "Iteration 7848, loss = 0.00593015\n",
      "Iteration 7849, loss = 0.00592971\n",
      "Iteration 7850, loss = 0.00592927\n",
      "Iteration 7851, loss = 0.00592885\n",
      "Iteration 7852, loss = 0.00592843\n",
      "Iteration 7853, loss = 0.00592801\n",
      "Iteration 7854, loss = 0.00592761\n",
      "Iteration 7855, loss = 0.00592721\n",
      "Iteration 7856, loss = 0.00592683\n",
      "Iteration 7857, loss = 0.00592648\n",
      "Iteration 7858, loss = 0.00592616\n",
      "Iteration 7859, loss = 0.00592591\n",
      "Iteration 7860, loss = 0.00592577\n",
      "Iteration 7861, loss = 0.00592578\n",
      "Iteration 7862, loss = 0.00592610\n",
      "Iteration 7863, loss = 0.00592679\n",
      "Iteration 7864, loss = 0.00592840\n",
      "Iteration 7865, loss = 0.00593088\n",
      "Iteration 7866, loss = 0.00593608\n",
      "Iteration 7867, loss = 0.00594302\n",
      "Iteration 7868, loss = 0.00595812\n",
      "Iteration 7869, loss = 0.00597430\n",
      "Iteration 7870, loss = 0.00601273\n",
      "Iteration 7871, loss = 0.00603689\n",
      "Iteration 7872, loss = 0.00610525\n",
      "Iteration 7873, loss = 0.00610105\n",
      "Iteration 7874, loss = 0.00614134\n",
      "Iteration 7875, loss = 0.00608633\n",
      "Iteration 7876, loss = 0.00606112\n",
      "Iteration 7877, loss = 0.00603821\n",
      "Iteration 7878, loss = 0.00604292\n",
      "Iteration 7879, loss = 0.00607863\n",
      "Iteration 7880, loss = 0.00611053\n",
      "Iteration 7881, loss = 0.00617066\n",
      "Iteration 7882, loss = 0.00619403\n",
      "Iteration 7883, loss = 0.00627944\n",
      "Iteration 7884, loss = 0.00629968\n",
      "Iteration 7885, loss = 0.00636062\n",
      "Iteration 7886, loss = 0.00625978\n",
      "Iteration 7887, loss = 0.00614664\n",
      "Iteration 7888, loss = 0.00599431\n",
      "Iteration 7889, loss = 0.00592062\n",
      "Iteration 7890, loss = 0.00593542\n",
      "Iteration 7891, loss = 0.00599877\n",
      "Iteration 7892, loss = 0.00605979\n",
      "Iteration 7893, loss = 0.00606297\n",
      "Iteration 7894, loss = 0.00604438\n",
      "Iteration 7895, loss = 0.00599540\n",
      "Iteration 7896, loss = 0.00596045\n",
      "Iteration 7897, loss = 0.00593435\n",
      "Iteration 7898, loss = 0.00592769\n",
      "Iteration 7899, loss = 0.00593973\n",
      "Iteration 7900, loss = 0.00596059\n",
      "Iteration 7901, loss = 0.00597878\n",
      "Iteration 7902, loss = 0.00596954\n",
      "Iteration 7903, loss = 0.00594737\n",
      "Iteration 7904, loss = 0.00592105\n",
      "Iteration 7905, loss = 0.00591027\n",
      "Iteration 7906, loss = 0.00591600\n",
      "Iteration 7907, loss = 0.00592684\n",
      "Iteration 7908, loss = 0.00593453\n",
      "Iteration 7909, loss = 0.00593247\n",
      "Iteration 7910, loss = 0.00592738\n",
      "Iteration 7911, loss = 0.00592015\n",
      "Iteration 7912, loss = 0.00591411\n",
      "Iteration 7913, loss = 0.00590931\n",
      "Iteration 7914, loss = 0.00590671\n",
      "Iteration 7915, loss = 0.00590818\n",
      "Iteration 7916, loss = 0.00591214\n",
      "Iteration 7917, loss = 0.00591556\n",
      "Iteration 7918, loss = 0.00591461\n",
      "Iteration 7919, loss = 0.00591019\n",
      "Iteration 7920, loss = 0.00590467\n",
      "Iteration 7921, loss = 0.00590132\n",
      "Iteration 7922, loss = 0.00590096\n",
      "Iteration 7923, loss = 0.00590235\n",
      "Iteration 7924, loss = 0.00590372\n",
      "Iteration 7925, loss = 0.00590405\n",
      "Iteration 7926, loss = 0.00590357\n",
      "Iteration 7927, loss = 0.00590234\n",
      "Iteration 7928, loss = 0.00590081\n",
      "Iteration 7929, loss = 0.00589900\n",
      "Iteration 7930, loss = 0.00589743\n",
      "Iteration 7931, loss = 0.00589656\n",
      "Iteration 7932, loss = 0.00589660\n",
      "Iteration 7933, loss = 0.00589719\n",
      "Iteration 7934, loss = 0.00589760\n",
      "Iteration 7935, loss = 0.00589740\n",
      "Iteration 7936, loss = 0.00589650\n",
      "Iteration 7937, loss = 0.00589534\n",
      "Iteration 7938, loss = 0.00589428\n",
      "Iteration 7939, loss = 0.00589355\n",
      "Iteration 7940, loss = 0.00589308\n",
      "Iteration 7941, loss = 0.00589274\n",
      "Iteration 7942, loss = 0.00589243\n",
      "Iteration 7943, loss = 0.00589216\n",
      "Iteration 7944, loss = 0.00589193\n",
      "Iteration 7945, loss = 0.00589167\n",
      "Iteration 7946, loss = 0.00589131\n",
      "Iteration 7947, loss = 0.00589079\n",
      "Iteration 7948, loss = 0.00589017\n",
      "Iteration 7949, loss = 0.00588954\n",
      "Iteration 7950, loss = 0.00588899\n",
      "Iteration 7951, loss = 0.00588854\n",
      "Iteration 7952, loss = 0.00588817\n",
      "Iteration 7953, loss = 0.00588781\n",
      "Iteration 7954, loss = 0.00588746\n",
      "Iteration 7955, loss = 0.00588710\n",
      "Iteration 7956, loss = 0.00588674\n",
      "Iteration 7957, loss = 0.00588639\n",
      "Iteration 7958, loss = 0.00588604\n",
      "Iteration 7959, loss = 0.00588565\n",
      "Iteration 7960, loss = 0.00588523\n",
      "Iteration 7961, loss = 0.00588478\n",
      "Iteration 7962, loss = 0.00588433\n",
      "Iteration 7963, loss = 0.00588388\n",
      "Iteration 7964, loss = 0.00588346\n",
      "Iteration 7965, loss = 0.00588306\n",
      "Iteration 7966, loss = 0.00588267\n",
      "Iteration 7967, loss = 0.00588228\n",
      "Iteration 7968, loss = 0.00588190\n",
      "Iteration 7969, loss = 0.00588152\n",
      "Iteration 7970, loss = 0.00588114\n",
      "Iteration 7971, loss = 0.00588077\n",
      "Iteration 7972, loss = 0.00588041\n",
      "Iteration 7973, loss = 0.00588004\n",
      "Iteration 7974, loss = 0.00587966\n",
      "Iteration 7975, loss = 0.00587928\n",
      "Iteration 7976, loss = 0.00587889\n",
      "Iteration 7977, loss = 0.00587851\n",
      "Iteration 7978, loss = 0.00587813\n",
      "Iteration 7979, loss = 0.00587775\n",
      "Iteration 7980, loss = 0.00587738\n",
      "Iteration 7981, loss = 0.00587700\n",
      "Iteration 7982, loss = 0.00587662\n",
      "Iteration 7983, loss = 0.00587624\n",
      "Iteration 7984, loss = 0.00587586\n",
      "Iteration 7985, loss = 0.00587549\n",
      "Iteration 7986, loss = 0.00587512\n",
      "Iteration 7987, loss = 0.00587475\n",
      "Iteration 7988, loss = 0.00587439\n",
      "Iteration 7989, loss = 0.00587404\n",
      "Iteration 7990, loss = 0.00587369\n",
      "Iteration 7991, loss = 0.00587335\n",
      "Iteration 7992, loss = 0.00587302\n",
      "Iteration 7993, loss = 0.00587271\n",
      "Iteration 7994, loss = 0.00587243\n",
      "Iteration 7995, loss = 0.00587218\n",
      "Iteration 7996, loss = 0.00587199\n",
      "Iteration 7997, loss = 0.00587186\n",
      "Iteration 7998, loss = 0.00587183\n",
      "Iteration 7999, loss = 0.00587193\n",
      "Iteration 8000, loss = 0.00587226\n",
      "Iteration 8001, loss = 0.00587285\n",
      "Iteration 8002, loss = 0.00587394\n",
      "Iteration 8003, loss = 0.00587557\n",
      "Iteration 8004, loss = 0.00587834\n",
      "Iteration 8005, loss = 0.00588218\n",
      "Iteration 8006, loss = 0.00588870\n",
      "Iteration 8007, loss = 0.00589725\n",
      "Iteration 8008, loss = 0.00591198\n",
      "Iteration 8009, loss = 0.00592979\n",
      "Iteration 8010, loss = 0.00596134\n",
      "Iteration 8011, loss = 0.00599368\n",
      "Iteration 8012, loss = 0.00605238\n",
      "Iteration 8013, loss = 0.00609340\n",
      "Iteration 8014, loss = 0.00616978\n",
      "Iteration 8015, loss = 0.00617742\n",
      "Iteration 8016, loss = 0.00620826\n",
      "Iteration 8017, loss = 0.00613776\n",
      "Iteration 8018, loss = 0.00608408\n",
      "Iteration 8019, loss = 0.00599778\n",
      "Iteration 8020, loss = 0.00595196\n",
      "Iteration 8021, loss = 0.00593158\n",
      "Iteration 8022, loss = 0.00594541\n",
      "Iteration 8023, loss = 0.00596837\n",
      "Iteration 8024, loss = 0.00597653\n",
      "Iteration 8025, loss = 0.00597178\n",
      "Iteration 8026, loss = 0.00593558\n",
      "Iteration 8027, loss = 0.00590192\n",
      "Iteration 8028, loss = 0.00587702\n",
      "Iteration 8029, loss = 0.00587382\n",
      "Iteration 8030, loss = 0.00588742\n",
      "Iteration 8031, loss = 0.00590704\n",
      "Iteration 8032, loss = 0.00591987\n",
      "Iteration 8033, loss = 0.00591642\n",
      "Iteration 8034, loss = 0.00590146\n",
      "Iteration 8035, loss = 0.00587975\n",
      "Iteration 8036, loss = 0.00586337\n",
      "Iteration 8037, loss = 0.00585722\n",
      "Iteration 8038, loss = 0.00586119\n",
      "Iteration 8039, loss = 0.00587054\n",
      "Iteration 8040, loss = 0.00587873\n",
      "Iteration 8041, loss = 0.00588190\n",
      "Iteration 8042, loss = 0.00587814\n",
      "Iteration 8043, loss = 0.00587022\n",
      "Iteration 8044, loss = 0.00586183\n",
      "Iteration 8045, loss = 0.00585599\n",
      "Iteration 8046, loss = 0.00585436\n",
      "Iteration 8047, loss = 0.00585604\n",
      "Iteration 8048, loss = 0.00585918\n",
      "Iteration 8049, loss = 0.00586141\n",
      "Iteration 8050, loss = 0.00586183\n",
      "Iteration 8051, loss = 0.00585995\n",
      "Iteration 8052, loss = 0.00585684\n",
      "Iteration 8053, loss = 0.00585378\n",
      "Iteration 8054, loss = 0.00585167\n",
      "Iteration 8055, loss = 0.00585100\n",
      "Iteration 8056, loss = 0.00585144\n",
      "Iteration 8057, loss = 0.00585235\n",
      "Iteration 8058, loss = 0.00585291\n",
      "Iteration 8059, loss = 0.00585282\n",
      "Iteration 8060, loss = 0.00585182\n",
      "Iteration 8061, loss = 0.00585035\n",
      "Iteration 8062, loss = 0.00584876\n",
      "Iteration 8063, loss = 0.00584749\n",
      "Iteration 8064, loss = 0.00584676\n",
      "Iteration 8065, loss = 0.00584656\n",
      "Iteration 8066, loss = 0.00584671\n",
      "Iteration 8067, loss = 0.00584694\n",
      "Iteration 8068, loss = 0.00584704\n",
      "Iteration 8069, loss = 0.00584683\n",
      "Iteration 8070, loss = 0.00584636\n",
      "Iteration 8071, loss = 0.00584564\n",
      "Iteration 8072, loss = 0.00584485\n",
      "Iteration 8073, loss = 0.00584409\n",
      "Iteration 8074, loss = 0.00584346\n",
      "Iteration 8075, loss = 0.00584297\n",
      "Iteration 8076, loss = 0.00584263\n",
      "Iteration 8077, loss = 0.00584237\n",
      "Iteration 8078, loss = 0.00584214\n",
      "Iteration 8079, loss = 0.00584190\n",
      "Iteration 8080, loss = 0.00584161\n",
      "Iteration 8081, loss = 0.00584126\n",
      "Iteration 8082, loss = 0.00584086\n",
      "Iteration 8083, loss = 0.00584042\n",
      "Iteration 8084, loss = 0.00583996\n",
      "Iteration 8085, loss = 0.00583951\n",
      "Iteration 8086, loss = 0.00583906\n",
      "Iteration 8087, loss = 0.00583865\n",
      "Iteration 8088, loss = 0.00583826\n",
      "Iteration 8089, loss = 0.00583790\n",
      "Iteration 8090, loss = 0.00583756\n",
      "Iteration 8091, loss = 0.00583723\n",
      "Iteration 8092, loss = 0.00583692\n",
      "Iteration 8093, loss = 0.00583660\n",
      "Iteration 8094, loss = 0.00583628\n",
      "Iteration 8095, loss = 0.00583596\n",
      "Iteration 8096, loss = 0.00583562\n",
      "Iteration 8097, loss = 0.00583528\n",
      "Iteration 8098, loss = 0.00583494\n",
      "Iteration 8099, loss = 0.00583459\n",
      "Iteration 8100, loss = 0.00583423\n",
      "Iteration 8101, loss = 0.00583388\n",
      "Iteration 8102, loss = 0.00583353\n",
      "Iteration 8103, loss = 0.00583318\n",
      "Iteration 8104, loss = 0.00583284\n",
      "Iteration 8105, loss = 0.00583252\n",
      "Iteration 8106, loss = 0.00583221\n",
      "Iteration 8107, loss = 0.00583192\n",
      "Iteration 8108, loss = 0.00583167\n",
      "Iteration 8109, loss = 0.00583146\n",
      "Iteration 8110, loss = 0.00583134\n",
      "Iteration 8111, loss = 0.00583134\n",
      "Iteration 8112, loss = 0.00583153\n",
      "Iteration 8113, loss = 0.00583201\n",
      "Iteration 8114, loss = 0.00583300\n",
      "Iteration 8115, loss = 0.00583467\n",
      "Iteration 8116, loss = 0.00583769\n",
      "Iteration 8117, loss = 0.00584239\n",
      "Iteration 8118, loss = 0.00585070\n",
      "Iteration 8119, loss = 0.00586291\n",
      "Iteration 8120, loss = 0.00588480\n",
      "Iteration 8121, loss = 0.00591404\n",
      "Iteration 8122, loss = 0.00596741\n",
      "Iteration 8123, loss = 0.00602543\n",
      "Iteration 8124, loss = 0.00613007\n",
      "Iteration 8125, loss = 0.00619549\n",
      "Iteration 8126, loss = 0.00630685\n",
      "Iteration 8127, loss = 0.00627971\n",
      "Iteration 8128, loss = 0.00627008\n",
      "Iteration 8129, loss = 0.00614363\n",
      "Iteration 8130, loss = 0.00607299\n",
      "Iteration 8131, loss = 0.00599566\n",
      "Iteration 8132, loss = 0.00598035\n",
      "Iteration 8133, loss = 0.00596752\n",
      "Iteration 8134, loss = 0.00593891\n",
      "Iteration 8135, loss = 0.00590639\n",
      "Iteration 8136, loss = 0.00587821\n",
      "Iteration 8137, loss = 0.00589120\n",
      "Iteration 8138, loss = 0.00592416\n",
      "Iteration 8139, loss = 0.00595688\n",
      "Iteration 8140, loss = 0.00593941\n",
      "Iteration 8141, loss = 0.00589015\n",
      "Iteration 8142, loss = 0.00583943\n",
      "Iteration 8143, loss = 0.00582139\n",
      "Iteration 8144, loss = 0.00583985\n",
      "Iteration 8145, loss = 0.00586978\n",
      "Iteration 8146, loss = 0.00588456\n",
      "Iteration 8147, loss = 0.00587273\n",
      "Iteration 8148, loss = 0.00585132\n",
      "Iteration 8149, loss = 0.00583489\n",
      "Iteration 8150, loss = 0.00583244\n",
      "Iteration 8151, loss = 0.00583570\n",
      "Iteration 8152, loss = 0.00583679\n",
      "Iteration 8153, loss = 0.00583238\n",
      "Iteration 8154, loss = 0.00582868\n",
      "Iteration 8155, loss = 0.00583018\n",
      "Iteration 8156, loss = 0.00583416\n",
      "Iteration 8157, loss = 0.00583597\n",
      "Iteration 8158, loss = 0.00583058\n",
      "Iteration 8159, loss = 0.00582201\n",
      "Iteration 8160, loss = 0.00581531\n",
      "Iteration 8161, loss = 0.00581470\n",
      "Iteration 8162, loss = 0.00581881\n",
      "Iteration 8163, loss = 0.00582288\n",
      "Iteration 8164, loss = 0.00582374\n",
      "Iteration 8165, loss = 0.00582078\n",
      "Iteration 8166, loss = 0.00581720\n",
      "Iteration 8167, loss = 0.00581502\n",
      "Iteration 8168, loss = 0.00581465\n",
      "Iteration 8169, loss = 0.00581468\n",
      "Iteration 8170, loss = 0.00581391\n",
      "Iteration 8171, loss = 0.00581258\n",
      "Iteration 8172, loss = 0.00581161\n",
      "Iteration 8173, loss = 0.00581173\n",
      "Iteration 8174, loss = 0.00581252\n",
      "Iteration 8175, loss = 0.00581303\n",
      "Iteration 8176, loss = 0.00581244\n",
      "Iteration 8177, loss = 0.00581095\n",
      "Iteration 8178, loss = 0.00580930\n",
      "Iteration 8179, loss = 0.00580825\n",
      "Iteration 8180, loss = 0.00580797\n",
      "Iteration 8181, loss = 0.00580810\n",
      "Iteration 8182, loss = 0.00580811\n",
      "Iteration 8183, loss = 0.00580780\n",
      "Iteration 8184, loss = 0.00580729\n",
      "Iteration 8185, loss = 0.00580683\n",
      "Iteration 8186, loss = 0.00580656\n",
      "Iteration 8187, loss = 0.00580639\n",
      "Iteration 8188, loss = 0.00580613\n",
      "Iteration 8189, loss = 0.00580564\n",
      "Iteration 8190, loss = 0.00580501\n",
      "Iteration 8191, loss = 0.00580437\n",
      "Iteration 8192, loss = 0.00580387\n",
      "Iteration 8193, loss = 0.00580355\n",
      "Iteration 8194, loss = 0.00580334\n",
      "Iteration 8195, loss = 0.00580313\n",
      "Iteration 8196, loss = 0.00580284\n",
      "Iteration 8197, loss = 0.00580250\n",
      "Iteration 8198, loss = 0.00580214\n",
      "Iteration 8199, loss = 0.00580182\n",
      "Iteration 8200, loss = 0.00580153\n",
      "Iteration 8201, loss = 0.00580126\n",
      "Iteration 8202, loss = 0.00580096\n",
      "Iteration 8203, loss = 0.00580062\n",
      "Iteration 8204, loss = 0.00580024\n",
      "Iteration 8205, loss = 0.00579985\n",
      "Iteration 8206, loss = 0.00579948\n",
      "Iteration 8207, loss = 0.00579913\n",
      "Iteration 8208, loss = 0.00579881\n",
      "Iteration 8209, loss = 0.00579850\n",
      "Iteration 8210, loss = 0.00579817\n",
      "Iteration 8211, loss = 0.00579784\n",
      "Iteration 8212, loss = 0.00579750\n",
      "Iteration 8213, loss = 0.00579717\n",
      "Iteration 8214, loss = 0.00579684\n",
      "Iteration 8215, loss = 0.00579653\n",
      "Iteration 8216, loss = 0.00579623\n",
      "Iteration 8217, loss = 0.00579592\n",
      "Iteration 8218, loss = 0.00579561\n",
      "Iteration 8219, loss = 0.00579530\n",
      "Iteration 8220, loss = 0.00579498\n",
      "Iteration 8221, loss = 0.00579467\n",
      "Iteration 8222, loss = 0.00579435\n",
      "Iteration 8223, loss = 0.00579405\n",
      "Iteration 8224, loss = 0.00579375\n",
      "Iteration 8225, loss = 0.00579345\n",
      "Iteration 8226, loss = 0.00579315\n",
      "Iteration 8227, loss = 0.00579285\n",
      "Iteration 8228, loss = 0.00579255\n",
      "Iteration 8229, loss = 0.00579226\n",
      "Iteration 8230, loss = 0.00579198\n",
      "Iteration 8231, loss = 0.00579171\n",
      "Iteration 8232, loss = 0.00579146\n",
      "Iteration 8233, loss = 0.00579123\n",
      "Iteration 8234, loss = 0.00579104\n",
      "Iteration 8235, loss = 0.00579089\n",
      "Iteration 8236, loss = 0.00579081\n",
      "Iteration 8237, loss = 0.00579081\n",
      "Iteration 8238, loss = 0.00579097\n",
      "Iteration 8239, loss = 0.00579132\n",
      "Iteration 8240, loss = 0.00579202\n",
      "Iteration 8241, loss = 0.00579311\n",
      "Iteration 8242, loss = 0.00579501\n",
      "Iteration 8243, loss = 0.00579776\n",
      "Iteration 8244, loss = 0.00580243\n",
      "Iteration 8245, loss = 0.00580884\n",
      "Iteration 8246, loss = 0.00581987\n",
      "Iteration 8247, loss = 0.00583401\n",
      "Iteration 8248, loss = 0.00585900\n",
      "Iteration 8249, loss = 0.00588717\n",
      "Iteration 8250, loss = 0.00593828\n",
      "Iteration 8251, loss = 0.00598152\n",
      "Iteration 8252, loss = 0.00606108\n",
      "Iteration 8253, loss = 0.00608729\n",
      "Iteration 8254, loss = 0.00614236\n",
      "Iteration 8255, loss = 0.00608497\n",
      "Iteration 8256, loss = 0.00603663\n",
      "Iteration 8257, loss = 0.00592680\n",
      "Iteration 8258, loss = 0.00584860\n",
      "Iteration 8259, loss = 0.00580320\n",
      "Iteration 8260, loss = 0.00580156\n",
      "Iteration 8261, loss = 0.00582961\n",
      "Iteration 8262, loss = 0.00586110\n",
      "Iteration 8263, loss = 0.00588740\n",
      "Iteration 8264, loss = 0.00587909\n",
      "Iteration 8265, loss = 0.00586187\n",
      "Iteration 8266, loss = 0.00583172\n",
      "Iteration 8267, loss = 0.00581160\n",
      "Iteration 8268, loss = 0.00580226\n",
      "Iteration 8269, loss = 0.00580388\n",
      "Iteration 8270, loss = 0.00580966\n",
      "Iteration 8271, loss = 0.00581208\n",
      "Iteration 8272, loss = 0.00581185\n",
      "Iteration 8273, loss = 0.00580613\n",
      "Iteration 8274, loss = 0.00580146\n",
      "Iteration 8275, loss = 0.00579757\n",
      "Iteration 8276, loss = 0.00579630\n",
      "Iteration 8277, loss = 0.00579507\n",
      "Iteration 8278, loss = 0.00579341\n",
      "Iteration 8279, loss = 0.00579043\n",
      "Iteration 8280, loss = 0.00578667\n",
      "Iteration 8281, loss = 0.00578401\n",
      "Iteration 8282, loss = 0.00578287\n",
      "Iteration 8283, loss = 0.00578367\n",
      "Iteration 8284, loss = 0.00578506\n",
      "Iteration 8285, loss = 0.00578610\n",
      "Iteration 8286, loss = 0.00578533\n",
      "Iteration 8287, loss = 0.00578305\n",
      "Iteration 8288, loss = 0.00577976\n",
      "Iteration 8289, loss = 0.00577670\n",
      "Iteration 8290, loss = 0.00577486\n",
      "Iteration 8291, loss = 0.00577456\n",
      "Iteration 8292, loss = 0.00577548\n",
      "Iteration 8293, loss = 0.00577677\n",
      "Iteration 8294, loss = 0.00577766\n",
      "Iteration 8295, loss = 0.00577753\n",
      "Iteration 8296, loss = 0.00577646\n",
      "Iteration 8297, loss = 0.00577471\n",
      "Iteration 8298, loss = 0.00577290\n",
      "Iteration 8299, loss = 0.00577150\n",
      "Iteration 8300, loss = 0.00577078\n",
      "Iteration 8301, loss = 0.00577069\n",
      "Iteration 8302, loss = 0.00577100\n",
      "Iteration 8303, loss = 0.00577137\n",
      "Iteration 8304, loss = 0.00577153\n",
      "Iteration 8305, loss = 0.00577135\n",
      "Iteration 8306, loss = 0.00577082\n",
      "Iteration 8307, loss = 0.00577009\n",
      "Iteration 8308, loss = 0.00576929\n",
      "Iteration 8309, loss = 0.00576860\n",
      "Iteration 8310, loss = 0.00576807\n",
      "Iteration 8311, loss = 0.00576771\n",
      "Iteration 8312, loss = 0.00576748\n",
      "Iteration 8313, loss = 0.00576732\n",
      "Iteration 8314, loss = 0.00576714\n",
      "Iteration 8315, loss = 0.00576691\n",
      "Iteration 8316, loss = 0.00576661\n",
      "Iteration 8317, loss = 0.00576625\n",
      "Iteration 8318, loss = 0.00576587\n",
      "Iteration 8319, loss = 0.00576549\n",
      "Iteration 8320, loss = 0.00576513\n",
      "Iteration 8321, loss = 0.00576481\n",
      "Iteration 8322, loss = 0.00576452\n",
      "Iteration 8323, loss = 0.00576425\n",
      "Iteration 8324, loss = 0.00576400\n",
      "Iteration 8325, loss = 0.00576374\n",
      "Iteration 8326, loss = 0.00576346\n",
      "Iteration 8327, loss = 0.00576317\n",
      "Iteration 8328, loss = 0.00576285\n",
      "Iteration 8329, loss = 0.00576253\n",
      "Iteration 8330, loss = 0.00576219\n",
      "Iteration 8331, loss = 0.00576185\n",
      "Iteration 8332, loss = 0.00576152\n",
      "Iteration 8333, loss = 0.00576120\n",
      "Iteration 8334, loss = 0.00576088\n",
      "Iteration 8335, loss = 0.00576058\n",
      "Iteration 8336, loss = 0.00576028\n",
      "Iteration 8337, loss = 0.00575999\n",
      "Iteration 8338, loss = 0.00575971\n",
      "Iteration 8339, loss = 0.00575943\n",
      "Iteration 8340, loss = 0.00575915\n",
      "Iteration 8341, loss = 0.00575887\n",
      "Iteration 8342, loss = 0.00575859\n",
      "Iteration 8343, loss = 0.00575831\n",
      "Iteration 8344, loss = 0.00575803\n",
      "Iteration 8345, loss = 0.00575774\n",
      "Iteration 8346, loss = 0.00575746\n",
      "Iteration 8347, loss = 0.00575717\n",
      "Iteration 8348, loss = 0.00575688\n",
      "Iteration 8349, loss = 0.00575660\n",
      "Iteration 8350, loss = 0.00575632\n",
      "Iteration 8351, loss = 0.00575604\n",
      "Iteration 8352, loss = 0.00575577\n",
      "Iteration 8353, loss = 0.00575550\n",
      "Iteration 8354, loss = 0.00575524\n",
      "Iteration 8355, loss = 0.00575500\n",
      "Iteration 8356, loss = 0.00575477\n",
      "Iteration 8357, loss = 0.00575457\n",
      "Iteration 8358, loss = 0.00575441\n",
      "Iteration 8359, loss = 0.00575431\n",
      "Iteration 8360, loss = 0.00575431\n",
      "Iteration 8361, loss = 0.00575444\n",
      "Iteration 8362, loss = 0.00575479\n",
      "Iteration 8363, loss = 0.00575545\n",
      "Iteration 8364, loss = 0.00575666\n",
      "Iteration 8365, loss = 0.00575859\n",
      "Iteration 8366, loss = 0.00576188\n",
      "Iteration 8367, loss = 0.00576689\n",
      "Iteration 8368, loss = 0.00577539\n",
      "Iteration 8369, loss = 0.00578778\n",
      "Iteration 8370, loss = 0.00580909\n",
      "Iteration 8371, loss = 0.00583813\n",
      "Iteration 8372, loss = 0.00588887\n",
      "Iteration 8373, loss = 0.00594838\n",
      "Iteration 8374, loss = 0.00605194\n",
      "Iteration 8375, loss = 0.00613468\n",
      "Iteration 8376, loss = 0.00627171\n",
      "Iteration 8377, loss = 0.00627722\n",
      "Iteration 8378, loss = 0.00629849\n",
      "Iteration 8379, loss = 0.00612400\n",
      "Iteration 8380, loss = 0.00597825\n",
      "Iteration 8381, loss = 0.00582717\n",
      "Iteration 8382, loss = 0.00577557\n",
      "Iteration 8383, loss = 0.00580715\n",
      "Iteration 8384, loss = 0.00587406\n",
      "Iteration 8385, loss = 0.00593413\n",
      "Iteration 8386, loss = 0.00592257\n",
      "Iteration 8387, loss = 0.00588482\n",
      "Iteration 8388, loss = 0.00581558\n",
      "Iteration 8389, loss = 0.00577885\n",
      "Iteration 8390, loss = 0.00577651\n",
      "Iteration 8391, loss = 0.00579652\n",
      "Iteration 8392, loss = 0.00581795\n",
      "Iteration 8393, loss = 0.00581645\n",
      "Iteration 8394, loss = 0.00580430\n",
      "Iteration 8395, loss = 0.00578076\n",
      "Iteration 8396, loss = 0.00576843\n",
      "Iteration 8397, loss = 0.00576698\n",
      "Iteration 8398, loss = 0.00577133\n",
      "Iteration 8399, loss = 0.00577465\n",
      "Iteration 8400, loss = 0.00576931\n",
      "Iteration 8401, loss = 0.00576321\n",
      "Iteration 8402, loss = 0.00575819\n",
      "Iteration 8403, loss = 0.00575879\n",
      "Iteration 8404, loss = 0.00576104\n",
      "Iteration 8405, loss = 0.00575989\n",
      "Iteration 8406, loss = 0.00575514\n",
      "Iteration 8407, loss = 0.00574780\n",
      "Iteration 8408, loss = 0.00574436\n",
      "Iteration 8409, loss = 0.00574612\n",
      "Iteration 8410, loss = 0.00575076\n",
      "Iteration 8411, loss = 0.00575355\n",
      "Iteration 8412, loss = 0.00575112\n",
      "Iteration 8413, loss = 0.00574558\n",
      "Iteration 8414, loss = 0.00574017\n",
      "Iteration 8415, loss = 0.00573841\n",
      "Iteration 8416, loss = 0.00574034\n",
      "Iteration 8417, loss = 0.00574334\n",
      "Iteration 8418, loss = 0.00574478\n",
      "Iteration 8419, loss = 0.00574338\n",
      "Iteration 8420, loss = 0.00574058\n",
      "Iteration 8421, loss = 0.00573817\n",
      "Iteration 8422, loss = 0.00573721\n",
      "Iteration 8423, loss = 0.00573750\n",
      "Iteration 8424, loss = 0.00573800\n",
      "Iteration 8425, loss = 0.00573806\n",
      "Iteration 8426, loss = 0.00573749\n",
      "Iteration 8427, loss = 0.00573674\n",
      "Iteration 8428, loss = 0.00573617\n",
      "Iteration 8429, loss = 0.00573580\n",
      "Iteration 8430, loss = 0.00573553\n",
      "Iteration 8431, loss = 0.00573513\n",
      "Iteration 8432, loss = 0.00573466\n",
      "Iteration 8433, loss = 0.00573416\n",
      "Iteration 8434, loss = 0.00573374\n",
      "Iteration 8435, loss = 0.00573342\n",
      "Iteration 8436, loss = 0.00573316\n",
      "Iteration 8437, loss = 0.00573293\n",
      "Iteration 8438, loss = 0.00573271\n",
      "Iteration 8439, loss = 0.00573248\n",
      "Iteration 8440, loss = 0.00573222\n",
      "Iteration 8441, loss = 0.00573188\n",
      "Iteration 8442, loss = 0.00573145\n",
      "Iteration 8443, loss = 0.00573099\n",
      "Iteration 8444, loss = 0.00573057\n",
      "Iteration 8445, loss = 0.00573025\n",
      "Iteration 8446, loss = 0.00573005\n",
      "Iteration 8447, loss = 0.00572991\n",
      "Iteration 8448, loss = 0.00572975\n",
      "Iteration 8449, loss = 0.00572950\n",
      "Iteration 8450, loss = 0.00572918\n",
      "Iteration 8451, loss = 0.00572882\n",
      "Iteration 8452, loss = 0.00572848\n",
      "Iteration 8453, loss = 0.00572818\n",
      "Iteration 8454, loss = 0.00572792\n",
      "Iteration 8455, loss = 0.00572767\n",
      "Iteration 8456, loss = 0.00572742\n",
      "Iteration 8457, loss = 0.00572714\n",
      "Iteration 8458, loss = 0.00572685\n",
      "Iteration 8459, loss = 0.00572655\n",
      "Iteration 8460, loss = 0.00572627\n",
      "Iteration 8461, loss = 0.00572601\n",
      "Iteration 8462, loss = 0.00572576\n",
      "Iteration 8463, loss = 0.00572551\n",
      "Iteration 8464, loss = 0.00572525\n",
      "Iteration 8465, loss = 0.00572499\n",
      "Iteration 8466, loss = 0.00572473\n",
      "Iteration 8467, loss = 0.00572447\n",
      "Iteration 8468, loss = 0.00572420\n",
      "Iteration 8469, loss = 0.00572393\n",
      "Iteration 8470, loss = 0.00572366\n",
      "Iteration 8471, loss = 0.00572338\n",
      "Iteration 8472, loss = 0.00572311\n",
      "Iteration 8473, loss = 0.00572284\n",
      "Iteration 8474, loss = 0.00572258\n",
      "Iteration 8475, loss = 0.00572232\n",
      "Iteration 8476, loss = 0.00572206\n",
      "Iteration 8477, loss = 0.00572180\n",
      "Iteration 8478, loss = 0.00572153\n",
      "Iteration 8479, loss = 0.00572127\n",
      "Iteration 8480, loss = 0.00572100\n",
      "Iteration 8481, loss = 0.00572074\n",
      "Iteration 8482, loss = 0.00572047\n",
      "Iteration 8483, loss = 0.00572021\n",
      "Iteration 8484, loss = 0.00571995\n",
      "Iteration 8485, loss = 0.00571969\n",
      "Iteration 8486, loss = 0.00571943\n",
      "Iteration 8487, loss = 0.00571917\n",
      "Iteration 8488, loss = 0.00571891\n",
      "Iteration 8489, loss = 0.00571865\n",
      "Iteration 8490, loss = 0.00571839\n",
      "Iteration 8491, loss = 0.00571813\n",
      "Iteration 8492, loss = 0.00571787\n",
      "Iteration 8493, loss = 0.00571761\n",
      "Iteration 8494, loss = 0.00571735\n",
      "Iteration 8495, loss = 0.00571709\n",
      "Iteration 8496, loss = 0.00571683\n",
      "Iteration 8497, loss = 0.00571657\n",
      "Iteration 8498, loss = 0.00571632\n",
      "Iteration 8499, loss = 0.00571606\n",
      "Iteration 8500, loss = 0.00571581\n",
      "Iteration 8501, loss = 0.00571556\n",
      "Iteration 8502, loss = 0.00571532\n",
      "Iteration 8503, loss = 0.00571508\n",
      "Iteration 8504, loss = 0.00571485\n",
      "Iteration 8505, loss = 0.00571464\n",
      "Iteration 8506, loss = 0.00571445\n",
      "Iteration 8507, loss = 0.00571429\n",
      "Iteration 8508, loss = 0.00571419\n",
      "Iteration 8509, loss = 0.00571417\n",
      "Iteration 8510, loss = 0.00571428\n",
      "Iteration 8511, loss = 0.00571462\n",
      "Iteration 8512, loss = 0.00571526\n",
      "Iteration 8513, loss = 0.00571648\n",
      "Iteration 8514, loss = 0.00571846\n",
      "Iteration 8515, loss = 0.00572201\n",
      "Iteration 8516, loss = 0.00572742\n",
      "Iteration 8517, loss = 0.00573710\n",
      "Iteration 8518, loss = 0.00575098\n",
      "Iteration 8519, loss = 0.00577638\n",
      "Iteration 8520, loss = 0.00580892\n",
      "Iteration 8521, loss = 0.00586973\n",
      "Iteration 8522, loss = 0.00592926\n",
      "Iteration 8523, loss = 0.00603907\n",
      "Iteration 8524, loss = 0.00608229\n",
      "Iteration 8525, loss = 0.00615869\n",
      "Iteration 8526, loss = 0.00606505\n",
      "Iteration 8527, loss = 0.00596958\n",
      "Iteration 8528, loss = 0.00581503\n",
      "Iteration 8529, loss = 0.00572517\n",
      "Iteration 8530, loss = 0.00571444\n",
      "Iteration 8531, loss = 0.00576678\n",
      "Iteration 8532, loss = 0.00584164\n",
      "Iteration 8533, loss = 0.00586786\n",
      "Iteration 8534, loss = 0.00585655\n",
      "Iteration 8535, loss = 0.00578579\n",
      "Iteration 8536, loss = 0.00572855\n",
      "Iteration 8537, loss = 0.00570765\n",
      "Iteration 8538, loss = 0.00572727\n",
      "Iteration 8539, loss = 0.00576417\n",
      "Iteration 8540, loss = 0.00578228\n",
      "Iteration 8541, loss = 0.00577758\n",
      "Iteration 8542, loss = 0.00574398\n",
      "Iteration 8543, loss = 0.00571522\n",
      "Iteration 8544, loss = 0.00570516\n",
      "Iteration 8545, loss = 0.00571561\n",
      "Iteration 8546, loss = 0.00573428\n",
      "Iteration 8547, loss = 0.00574323\n",
      "Iteration 8548, loss = 0.00573976\n",
      "Iteration 8549, loss = 0.00572295\n",
      "Iteration 8550, loss = 0.00570830\n",
      "Iteration 8551, loss = 0.00570318\n",
      "Iteration 8552, loss = 0.00570813\n",
      "Iteration 8553, loss = 0.00571706\n",
      "Iteration 8554, loss = 0.00572156\n",
      "Iteration 8555, loss = 0.00571977\n",
      "Iteration 8556, loss = 0.00571183\n",
      "Iteration 8557, loss = 0.00570445\n",
      "Iteration 8558, loss = 0.00570133\n",
      "Iteration 8559, loss = 0.00570304\n",
      "Iteration 8560, loss = 0.00570704\n",
      "Iteration 8561, loss = 0.00570964\n",
      "Iteration 8562, loss = 0.00570926\n",
      "Iteration 8563, loss = 0.00570580\n",
      "Iteration 8564, loss = 0.00570193\n",
      "Iteration 8565, loss = 0.00569963\n",
      "Iteration 8566, loss = 0.00569964\n",
      "Iteration 8567, loss = 0.00570119\n",
      "Iteration 8568, loss = 0.00570270\n",
      "Iteration 8569, loss = 0.00570304\n",
      "Iteration 8570, loss = 0.00570181\n",
      "Iteration 8571, loss = 0.00569991\n",
      "Iteration 8572, loss = 0.00569823\n",
      "Iteration 8573, loss = 0.00569746\n",
      "Iteration 8574, loss = 0.00569763\n",
      "Iteration 8575, loss = 0.00569824\n",
      "Iteration 8576, loss = 0.00569868\n",
      "Iteration 8577, loss = 0.00569852\n",
      "Iteration 8578, loss = 0.00569783\n",
      "Iteration 8579, loss = 0.00569685\n",
      "Iteration 8580, loss = 0.00569599\n",
      "Iteration 8581, loss = 0.00569549\n",
      "Iteration 8582, loss = 0.00569538\n",
      "Iteration 8583, loss = 0.00569548\n",
      "Iteration 8584, loss = 0.00569556\n",
      "Iteration 8585, loss = 0.00569544\n",
      "Iteration 8586, loss = 0.00569507\n",
      "Iteration 8587, loss = 0.00569455\n",
      "Iteration 8588, loss = 0.00569401\n",
      "Iteration 8589, loss = 0.00569357\n",
      "Iteration 8590, loss = 0.00569329\n",
      "Iteration 8591, loss = 0.00569314\n",
      "Iteration 8592, loss = 0.00569304\n",
      "Iteration 8593, loss = 0.00569291\n",
      "Iteration 8594, loss = 0.00569270\n",
      "Iteration 8595, loss = 0.00569241\n",
      "Iteration 8596, loss = 0.00569207\n",
      "Iteration 8597, loss = 0.00569171\n",
      "Iteration 8598, loss = 0.00569139\n",
      "Iteration 8599, loss = 0.00569112\n",
      "Iteration 8600, loss = 0.00569091\n",
      "Iteration 8601, loss = 0.00569074\n",
      "Iteration 8602, loss = 0.00569058\n",
      "Iteration 8603, loss = 0.00569044\n",
      "Iteration 8604, loss = 0.00569030\n",
      "Iteration 8605, loss = 0.00569021\n",
      "Iteration 8606, loss = 0.00569022\n",
      "Iteration 8607, loss = 0.00569042\n",
      "Iteration 8608, loss = 0.00569098\n",
      "Iteration 8609, loss = 0.00569211\n",
      "Iteration 8610, loss = 0.00569430\n",
      "Iteration 8611, loss = 0.00569813\n",
      "Iteration 8612, loss = 0.00570512\n",
      "Iteration 8613, loss = 0.00571662\n",
      "Iteration 8614, loss = 0.00573746\n",
      "Iteration 8615, loss = 0.00576887\n",
      "Iteration 8616, loss = 0.00582391\n",
      "Iteration 8617, loss = 0.00588950\n",
      "Iteration 8618, loss = 0.00598790\n",
      "Iteration 8619, loss = 0.00603724\n",
      "Iteration 8620, loss = 0.00607258\n",
      "Iteration 8621, loss = 0.00598690\n",
      "Iteration 8622, loss = 0.00590841\n",
      "Iteration 8623, loss = 0.00582720\n",
      "Iteration 8624, loss = 0.00581795\n",
      "Iteration 8625, loss = 0.00580514\n",
      "Iteration 8626, loss = 0.00578204\n",
      "Iteration 8627, loss = 0.00574036\n",
      "Iteration 8628, loss = 0.00571674\n",
      "Iteration 8629, loss = 0.00573638\n",
      "Iteration 8630, loss = 0.00576960\n",
      "Iteration 8631, loss = 0.00579338\n",
      "Iteration 8632, loss = 0.00576380\n",
      "Iteration 8633, loss = 0.00572610\n",
      "Iteration 8634, loss = 0.00570006\n",
      "Iteration 8635, loss = 0.00570430\n",
      "Iteration 8636, loss = 0.00571949\n",
      "Iteration 8637, loss = 0.00571938\n",
      "Iteration 8638, loss = 0.00570850\n",
      "Iteration 8639, loss = 0.00569827\n",
      "Iteration 8640, loss = 0.00570364\n",
      "Iteration 8641, loss = 0.00571375\n",
      "Iteration 8642, loss = 0.00571418\n",
      "Iteration 8643, loss = 0.00570022\n",
      "Iteration 8644, loss = 0.00568575\n",
      "Iteration 8645, loss = 0.00568297\n",
      "Iteration 8646, loss = 0.00568954\n",
      "Iteration 8647, loss = 0.00569449\n",
      "Iteration 8648, loss = 0.00569220\n",
      "Iteration 8649, loss = 0.00568780\n",
      "Iteration 8650, loss = 0.00568711\n",
      "Iteration 8651, loss = 0.00568973\n",
      "Iteration 8652, loss = 0.00568950\n",
      "Iteration 8653, loss = 0.00568537\n",
      "Iteration 8654, loss = 0.00568031\n",
      "Iteration 8655, loss = 0.00567870\n",
      "Iteration 8656, loss = 0.00568048\n",
      "Iteration 8657, loss = 0.00568240\n",
      "Iteration 8658, loss = 0.00568231\n",
      "Iteration 8659, loss = 0.00568070\n",
      "Iteration 8660, loss = 0.00567954\n",
      "Iteration 8661, loss = 0.00567951\n",
      "Iteration 8662, loss = 0.00567987\n",
      "Iteration 8663, loss = 0.00567943\n",
      "Iteration 8664, loss = 0.00567808\n",
      "Iteration 8665, loss = 0.00567650\n",
      "Iteration 8666, loss = 0.00567563\n",
      "Iteration 8667, loss = 0.00567569\n",
      "Iteration 8668, loss = 0.00567614\n",
      "Iteration 8669, loss = 0.00567628\n",
      "Iteration 8670, loss = 0.00567584\n",
      "Iteration 8671, loss = 0.00567524\n",
      "Iteration 8672, loss = 0.00567490\n",
      "Iteration 8673, loss = 0.00567488\n",
      "Iteration 8674, loss = 0.00567478\n",
      "Iteration 8675, loss = 0.00567431\n",
      "Iteration 8676, loss = 0.00567359\n",
      "Iteration 8677, loss = 0.00567302\n",
      "Iteration 8678, loss = 0.00567278\n",
      "Iteration 8679, loss = 0.00567269\n",
      "Iteration 8680, loss = 0.00567249\n",
      "Iteration 8681, loss = 0.00567212\n",
      "Iteration 8682, loss = 0.00567174\n",
      "Iteration 8683, loss = 0.00567153\n",
      "Iteration 8684, loss = 0.00567145\n",
      "Iteration 8685, loss = 0.00567136\n",
      "Iteration 8686, loss = 0.00567114\n",
      "Iteration 8687, loss = 0.00567081\n",
      "Iteration 8688, loss = 0.00567049\n",
      "Iteration 8689, loss = 0.00567023\n",
      "Iteration 8690, loss = 0.00567003\n",
      "Iteration 8691, loss = 0.00566982\n",
      "Iteration 8692, loss = 0.00566957\n",
      "Iteration 8693, loss = 0.00566928\n",
      "Iteration 8694, loss = 0.00566898\n",
      "Iteration 8695, loss = 0.00566870\n",
      "Iteration 8696, loss = 0.00566846\n",
      "Iteration 8697, loss = 0.00566824\n",
      "Iteration 8698, loss = 0.00566802\n",
      "Iteration 8699, loss = 0.00566777\n",
      "Iteration 8700, loss = 0.00566751\n",
      "Iteration 8701, loss = 0.00566726\n",
      "Iteration 8702, loss = 0.00566702\n",
      "Iteration 8703, loss = 0.00566681\n",
      "Iteration 8704, loss = 0.00566659\n",
      "Iteration 8705, loss = 0.00566636\n",
      "Iteration 8706, loss = 0.00566611\n",
      "Iteration 8707, loss = 0.00566587\n",
      "Iteration 8708, loss = 0.00566565\n",
      "Iteration 8709, loss = 0.00566543\n",
      "Iteration 8710, loss = 0.00566521\n",
      "Iteration 8711, loss = 0.00566498\n",
      "Iteration 8712, loss = 0.00566475\n",
      "Iteration 8713, loss = 0.00566452\n",
      "Iteration 8714, loss = 0.00566429\n",
      "Iteration 8715, loss = 0.00566407\n",
      "Iteration 8716, loss = 0.00566385\n",
      "Iteration 8717, loss = 0.00566363\n",
      "Iteration 8718, loss = 0.00566341\n",
      "Iteration 8719, loss = 0.00566319\n",
      "Iteration 8720, loss = 0.00566298\n",
      "Iteration 8721, loss = 0.00566277\n",
      "Iteration 8722, loss = 0.00566258\n",
      "Iteration 8723, loss = 0.00566240\n",
      "Iteration 8724, loss = 0.00566224\n",
      "Iteration 8725, loss = 0.00566211\n",
      "Iteration 8726, loss = 0.00566202\n",
      "Iteration 8727, loss = 0.00566199\n",
      "Iteration 8728, loss = 0.00566205\n",
      "Iteration 8729, loss = 0.00566227\n",
      "Iteration 8730, loss = 0.00566269\n",
      "Iteration 8731, loss = 0.00566347\n",
      "Iteration 8732, loss = 0.00566469\n",
      "Iteration 8733, loss = 0.00566680\n",
      "Iteration 8734, loss = 0.00566992\n",
      "Iteration 8735, loss = 0.00567524\n",
      "Iteration 8736, loss = 0.00568278\n",
      "Iteration 8737, loss = 0.00569585\n",
      "Iteration 8738, loss = 0.00571319\n",
      "Iteration 8739, loss = 0.00574405\n",
      "Iteration 8740, loss = 0.00577988\n",
      "Iteration 8741, loss = 0.00584495\n",
      "Iteration 8742, loss = 0.00590016\n",
      "Iteration 8743, loss = 0.00599986\n",
      "Iteration 8744, loss = 0.00602621\n",
      "Iteration 8745, loss = 0.00607965\n",
      "Iteration 8746, loss = 0.00599289\n",
      "Iteration 8747, loss = 0.00591323\n",
      "Iteration 8748, loss = 0.00578576\n",
      "Iteration 8749, loss = 0.00570629\n",
      "Iteration 8750, loss = 0.00567771\n",
      "Iteration 8751, loss = 0.00569825\n",
      "Iteration 8752, loss = 0.00574334\n",
      "Iteration 8753, loss = 0.00577108\n",
      "Iteration 8754, loss = 0.00578359\n",
      "Iteration 8755, loss = 0.00575484\n",
      "Iteration 8756, loss = 0.00572353\n",
      "Iteration 8757, loss = 0.00569356\n",
      "Iteration 8758, loss = 0.00568261\n",
      "Iteration 8759, loss = 0.00568359\n",
      "Iteration 8760, loss = 0.00569061\n",
      "Iteration 8761, loss = 0.00569677\n",
      "Iteration 8762, loss = 0.00569393\n",
      "Iteration 8763, loss = 0.00568928\n",
      "Iteration 8764, loss = 0.00568195\n",
      "Iteration 8765, loss = 0.00567755\n",
      "Iteration 8766, loss = 0.00567246\n",
      "Iteration 8767, loss = 0.00566948\n",
      "Iteration 8768, loss = 0.00566660\n",
      "Iteration 8769, loss = 0.00566470\n",
      "Iteration 8770, loss = 0.00566489\n",
      "Iteration 8771, loss = 0.00566577\n",
      "Iteration 8772, loss = 0.00566697\n",
      "Iteration 8773, loss = 0.00566556\n",
      "Iteration 8774, loss = 0.00566286\n",
      "Iteration 8775, loss = 0.00565838\n",
      "Iteration 8776, loss = 0.00565478\n",
      "Iteration 8777, loss = 0.00565326\n",
      "Iteration 8778, loss = 0.00565394\n",
      "Iteration 8779, loss = 0.00565592\n",
      "Iteration 8780, loss = 0.00565730\n",
      "Iteration 8781, loss = 0.00565742\n",
      "Iteration 8782, loss = 0.00565552\n",
      "Iteration 8783, loss = 0.00565286\n",
      "Iteration 8784, loss = 0.00565036\n",
      "Iteration 8785, loss = 0.00564902\n",
      "Iteration 8786, loss = 0.00564904\n",
      "Iteration 8787, loss = 0.00564995\n",
      "Iteration 8788, loss = 0.00565102\n",
      "Iteration 8789, loss = 0.00565142\n",
      "Iteration 8790, loss = 0.00565097\n",
      "Iteration 8791, loss = 0.00564973\n",
      "Iteration 8792, loss = 0.00564827\n",
      "Iteration 8793, loss = 0.00564707\n",
      "Iteration 8794, loss = 0.00564647\n",
      "Iteration 8795, loss = 0.00564648\n",
      "Iteration 8796, loss = 0.00564683\n",
      "Iteration 8797, loss = 0.00564717\n",
      "Iteration 8798, loss = 0.00564721\n",
      "Iteration 8799, loss = 0.00564689\n",
      "Iteration 8800, loss = 0.00564627\n",
      "Iteration 8801, loss = 0.00564559\n",
      "Iteration 8802, loss = 0.00564501\n",
      "Iteration 8803, loss = 0.00564463\n",
      "Iteration 8804, loss = 0.00564443\n",
      "Iteration 8805, loss = 0.00564434\n",
      "Iteration 8806, loss = 0.00564426\n",
      "Iteration 8807, loss = 0.00564411\n",
      "Iteration 8808, loss = 0.00564388\n",
      "Iteration 8809, loss = 0.00564360\n",
      "Iteration 8810, loss = 0.00564330\n",
      "Iteration 8811, loss = 0.00564301\n",
      "Iteration 8812, loss = 0.00564275\n",
      "Iteration 8813, loss = 0.00564252\n",
      "Iteration 8814, loss = 0.00564230\n",
      "Iteration 8815, loss = 0.00564208\n",
      "Iteration 8816, loss = 0.00564185\n",
      "Iteration 8817, loss = 0.00564160\n",
      "Iteration 8818, loss = 0.00564135\n",
      "Iteration 8819, loss = 0.00564109\n",
      "Iteration 8820, loss = 0.00564085\n",
      "Iteration 8821, loss = 0.00564063\n",
      "Iteration 8822, loss = 0.00564043\n",
      "Iteration 8823, loss = 0.00564023\n",
      "Iteration 8824, loss = 0.00564004\n",
      "Iteration 8825, loss = 0.00563985\n",
      "Iteration 8826, loss = 0.00563964\n",
      "Iteration 8827, loss = 0.00563942\n",
      "Iteration 8828, loss = 0.00563918\n",
      "Iteration 8829, loss = 0.00563894\n",
      "Iteration 8830, loss = 0.00563870\n",
      "Iteration 8831, loss = 0.00563847\n",
      "Iteration 8832, loss = 0.00563824\n",
      "Iteration 8833, loss = 0.00563801\n",
      "Iteration 8834, loss = 0.00563779\n",
      "Iteration 8835, loss = 0.00563758\n",
      "Iteration 8836, loss = 0.00563737\n",
      "Iteration 8837, loss = 0.00563716\n",
      "Iteration 8838, loss = 0.00563695\n",
      "Iteration 8839, loss = 0.00563673\n",
      "Iteration 8840, loss = 0.00563652\n",
      "Iteration 8841, loss = 0.00563631\n",
      "Iteration 8842, loss = 0.00563609\n",
      "Iteration 8843, loss = 0.00563587\n",
      "Iteration 8844, loss = 0.00563566\n",
      "Iteration 8845, loss = 0.00563544\n",
      "Iteration 8846, loss = 0.00563523\n",
      "Iteration 8847, loss = 0.00563501\n",
      "Iteration 8848, loss = 0.00563480\n",
      "Iteration 8849, loss = 0.00563459\n",
      "Iteration 8850, loss = 0.00563438\n",
      "Iteration 8851, loss = 0.00563417\n",
      "Iteration 8852, loss = 0.00563397\n",
      "Iteration 8853, loss = 0.00563376\n",
      "Iteration 8854, loss = 0.00563356\n",
      "Iteration 8855, loss = 0.00563336\n",
      "Iteration 8856, loss = 0.00563316\n",
      "Iteration 8857, loss = 0.00563297\n",
      "Iteration 8858, loss = 0.00563278\n",
      "Iteration 8859, loss = 0.00563261\n",
      "Iteration 8860, loss = 0.00563245\n",
      "Iteration 8861, loss = 0.00563231\n",
      "Iteration 8862, loss = 0.00563220\n",
      "Iteration 8863, loss = 0.00563213\n",
      "Iteration 8864, loss = 0.00563212\n",
      "Iteration 8865, loss = 0.00563219\n",
      "Iteration 8866, loss = 0.00563240\n",
      "Iteration 8867, loss = 0.00563279\n",
      "Iteration 8868, loss = 0.00563348\n",
      "Iteration 8869, loss = 0.00563455\n",
      "Iteration 8870, loss = 0.00563632\n",
      "Iteration 8871, loss = 0.00563893\n",
      "Iteration 8872, loss = 0.00564318\n",
      "Iteration 8873, loss = 0.00564922\n",
      "Iteration 8874, loss = 0.00565922\n",
      "Iteration 8875, loss = 0.00567282\n",
      "Iteration 8876, loss = 0.00569577\n",
      "Iteration 8877, loss = 0.00572457\n",
      "Iteration 8878, loss = 0.00577393\n",
      "Iteration 8879, loss = 0.00582574\n",
      "Iteration 8880, loss = 0.00591381\n",
      "Iteration 8881, loss = 0.00597186\n",
      "Iteration 8882, loss = 0.00606540\n",
      "Iteration 8883, loss = 0.00604817\n",
      "Iteration 8884, loss = 0.00603386\n",
      "Iteration 8885, loss = 0.00589595\n",
      "Iteration 8886, loss = 0.00577735\n",
      "Iteration 8887, loss = 0.00567230\n",
      "Iteration 8888, loss = 0.00563624\n",
      "Iteration 8889, loss = 0.00566078\n",
      "Iteration 8890, loss = 0.00571417\n",
      "Iteration 8891, loss = 0.00576621\n",
      "Iteration 8892, loss = 0.00576993\n",
      "Iteration 8893, loss = 0.00574834\n",
      "Iteration 8894, loss = 0.00569299\n",
      "Iteration 8895, loss = 0.00565221\n",
      "Iteration 8896, loss = 0.00563616\n",
      "Iteration 8897, loss = 0.00564680\n",
      "Iteration 8898, loss = 0.00566938\n",
      "Iteration 8899, loss = 0.00568163\n",
      "Iteration 8900, loss = 0.00568127\n",
      "Iteration 8901, loss = 0.00566326\n",
      "Iteration 8902, loss = 0.00564649\n",
      "Iteration 8903, loss = 0.00563641\n",
      "Iteration 8904, loss = 0.00563704\n",
      "Iteration 8905, loss = 0.00564287\n",
      "Iteration 8906, loss = 0.00564552\n",
      "Iteration 8907, loss = 0.00564417\n",
      "Iteration 8908, loss = 0.00563808\n",
      "Iteration 8909, loss = 0.00563309\n",
      "Iteration 8910, loss = 0.00563086\n",
      "Iteration 8911, loss = 0.00563192\n",
      "Iteration 8912, loss = 0.00563365\n",
      "Iteration 8913, loss = 0.00563332\n",
      "Iteration 8914, loss = 0.00563101\n",
      "Iteration 8915, loss = 0.00562718\n",
      "Iteration 8916, loss = 0.00562421\n",
      "Iteration 8917, loss = 0.00562316\n",
      "Iteration 8918, loss = 0.00562410\n",
      "Iteration 8919, loss = 0.00562583\n",
      "Iteration 8920, loss = 0.00562684\n",
      "Iteration 8921, loss = 0.00562637\n",
      "Iteration 8922, loss = 0.00562434\n",
      "Iteration 8923, loss = 0.00562180\n",
      "Iteration 8924, loss = 0.00561978\n",
      "Iteration 8925, loss = 0.00561899\n",
      "Iteration 8926, loss = 0.00561942\n",
      "Iteration 8927, loss = 0.00562044\n",
      "Iteration 8928, loss = 0.00562128\n",
      "Iteration 8929, loss = 0.00562135\n",
      "Iteration 8930, loss = 0.00562057\n",
      "Iteration 8931, loss = 0.00561930\n",
      "Iteration 8932, loss = 0.00561803\n",
      "Iteration 8933, loss = 0.00561720\n",
      "Iteration 8934, loss = 0.00561694\n",
      "Iteration 8935, loss = 0.00561710\n",
      "Iteration 8936, loss = 0.00561735\n",
      "Iteration 8937, loss = 0.00561742\n",
      "Iteration 8938, loss = 0.00561720\n",
      "Iteration 8939, loss = 0.00561674\n",
      "Iteration 8940, loss = 0.00561621\n",
      "Iteration 8941, loss = 0.00561577\n",
      "Iteration 8942, loss = 0.00561548\n",
      "Iteration 8943, loss = 0.00561530\n",
      "Iteration 8944, loss = 0.00561516\n",
      "Iteration 8945, loss = 0.00561498\n",
      "Iteration 8946, loss = 0.00561474\n",
      "Iteration 8947, loss = 0.00561444\n",
      "Iteration 8948, loss = 0.00561413\n",
      "Iteration 8949, loss = 0.00561384\n",
      "Iteration 8950, loss = 0.00561359\n",
      "Iteration 8951, loss = 0.00561339\n",
      "Iteration 8952, loss = 0.00561322\n",
      "Iteration 8953, loss = 0.00561308\n",
      "Iteration 8954, loss = 0.00561293\n",
      "Iteration 8955, loss = 0.00561275\n",
      "Iteration 8956, loss = 0.00561253\n",
      "Iteration 8957, loss = 0.00561228\n",
      "Iteration 8958, loss = 0.00561200\n",
      "Iteration 8959, loss = 0.00561172\n",
      "Iteration 8960, loss = 0.00561147\n",
      "Iteration 8961, loss = 0.00561124\n",
      "Iteration 8962, loss = 0.00561104\n",
      "Iteration 8963, loss = 0.00561085\n",
      "Iteration 8964, loss = 0.00561067\n",
      "Iteration 8965, loss = 0.00561048\n",
      "Iteration 8966, loss = 0.00561028\n",
      "Iteration 8967, loss = 0.00561008\n",
      "Iteration 8968, loss = 0.00560987\n",
      "Iteration 8969, loss = 0.00560966\n",
      "Iteration 8970, loss = 0.00560944\n",
      "Iteration 8971, loss = 0.00560923\n",
      "Iteration 8972, loss = 0.00560902\n",
      "Iteration 8973, loss = 0.00560882\n",
      "Iteration 8974, loss = 0.00560862\n",
      "Iteration 8975, loss = 0.00560843\n",
      "Iteration 8976, loss = 0.00560823\n",
      "Iteration 8977, loss = 0.00560804\n",
      "Iteration 8978, loss = 0.00560784\n",
      "Iteration 8979, loss = 0.00560764\n",
      "Iteration 8980, loss = 0.00560745\n",
      "Iteration 8981, loss = 0.00560725\n",
      "Iteration 8982, loss = 0.00560705\n",
      "Iteration 8983, loss = 0.00560685\n",
      "Iteration 8984, loss = 0.00560666\n",
      "Iteration 8985, loss = 0.00560647\n",
      "Iteration 8986, loss = 0.00560628\n",
      "Iteration 8987, loss = 0.00560610\n",
      "Iteration 8988, loss = 0.00560594\n",
      "Iteration 8989, loss = 0.00560578\n",
      "Iteration 8990, loss = 0.00560564\n",
      "Iteration 8991, loss = 0.00560551\n",
      "Iteration 8992, loss = 0.00560542\n",
      "Iteration 8993, loss = 0.00560536\n",
      "Iteration 8994, loss = 0.00560537\n",
      "Iteration 8995, loss = 0.00560544\n",
      "Iteration 8996, loss = 0.00560563\n",
      "Iteration 8997, loss = 0.00560596\n",
      "Iteration 8998, loss = 0.00560654\n",
      "Iteration 8999, loss = 0.00560740\n",
      "Iteration 9000, loss = 0.00560881\n",
      "Iteration 9001, loss = 0.00561077\n",
      "Iteration 9002, loss = 0.00561396\n",
      "Iteration 9003, loss = 0.00561821\n",
      "Iteration 9004, loss = 0.00562521\n",
      "Iteration 9005, loss = 0.00563406\n",
      "Iteration 9006, loss = 0.00564896\n",
      "Iteration 9007, loss = 0.00566611\n",
      "Iteration 9008, loss = 0.00569552\n",
      "Iteration 9009, loss = 0.00572343\n",
      "Iteration 9010, loss = 0.00577143\n",
      "Iteration 9011, loss = 0.00579957\n",
      "Iteration 9012, loss = 0.00584709\n",
      "Iteration 9013, loss = 0.00583844\n",
      "Iteration 9014, loss = 0.00583288\n",
      "Iteration 9015, loss = 0.00576675\n",
      "Iteration 9016, loss = 0.00570481\n",
      "Iteration 9017, loss = 0.00564159\n",
      "Iteration 9018, loss = 0.00560685\n",
      "Iteration 9019, loss = 0.00560315\n",
      "Iteration 9020, loss = 0.00562327\n",
      "Iteration 9021, loss = 0.00565409\n",
      "Iteration 9022, loss = 0.00567444\n",
      "Iteration 9023, loss = 0.00568360\n",
      "Iteration 9024, loss = 0.00566610\n",
      "Iteration 9025, loss = 0.00564267\n",
      "Iteration 9026, loss = 0.00561617\n",
      "Iteration 9027, loss = 0.00560094\n",
      "Iteration 9028, loss = 0.00559912\n",
      "Iteration 9029, loss = 0.00560777\n",
      "Iteration 9030, loss = 0.00562034\n",
      "Iteration 9031, loss = 0.00562841\n",
      "Iteration 9032, loss = 0.00563057\n",
      "Iteration 9033, loss = 0.00562310\n",
      "Iteration 9034, loss = 0.00561306\n",
      "Iteration 9035, loss = 0.00560278\n",
      "Iteration 9036, loss = 0.00559701\n",
      "Iteration 9037, loss = 0.00559656\n",
      "Iteration 9038, loss = 0.00560010\n",
      "Iteration 9039, loss = 0.00560503\n",
      "Iteration 9040, loss = 0.00560823\n",
      "Iteration 9041, loss = 0.00560891\n",
      "Iteration 9042, loss = 0.00560616\n",
      "Iteration 9043, loss = 0.00560211\n",
      "Iteration 9044, loss = 0.00559794\n",
      "Iteration 9045, loss = 0.00559525\n",
      "Iteration 9046, loss = 0.00559452\n",
      "Iteration 9047, loss = 0.00559542\n",
      "Iteration 9048, loss = 0.00559711\n",
      "Iteration 9049, loss = 0.00559854\n",
      "Iteration 9050, loss = 0.00559924\n",
      "Iteration 9051, loss = 0.00559871\n",
      "Iteration 9052, loss = 0.00559750\n",
      "Iteration 9053, loss = 0.00559587\n",
      "Iteration 9054, loss = 0.00559451\n",
      "Iteration 9055, loss = 0.00559371\n",
      "Iteration 9056, loss = 0.00559365\n",
      "Iteration 9057, loss = 0.00559420\n",
      "Iteration 9058, loss = 0.00559515\n",
      "Iteration 9059, loss = 0.00559623\n",
      "Iteration 9060, loss = 0.00559736\n",
      "Iteration 9061, loss = 0.00559841\n",
      "Iteration 9062, loss = 0.00559971\n",
      "Iteration 9063, loss = 0.00560128\n",
      "Iteration 9064, loss = 0.00560387\n",
      "Iteration 9065, loss = 0.00560733\n",
      "Iteration 9066, loss = 0.00561294\n",
      "Iteration 9067, loss = 0.00561989\n",
      "Iteration 9068, loss = 0.00563057\n",
      "Iteration 9069, loss = 0.00564223\n",
      "Iteration 9070, loss = 0.00565956\n",
      "Iteration 9071, loss = 0.00567489\n",
      "Iteration 9072, loss = 0.00569660\n",
      "Iteration 9073, loss = 0.00570796\n",
      "Iteration 9074, loss = 0.00572237\n",
      "Iteration 9075, loss = 0.00571579\n",
      "Iteration 9076, loss = 0.00570788\n",
      "Iteration 9077, loss = 0.00568101\n",
      "Iteration 9078, loss = 0.00565656\n",
      "Iteration 9079, loss = 0.00562960\n",
      "Iteration 9080, loss = 0.00561172\n",
      "Iteration 9081, loss = 0.00560096\n",
      "Iteration 9082, loss = 0.00559807\n",
      "Iteration 9083, loss = 0.00559976\n",
      "Iteration 9084, loss = 0.00560328\n",
      "Iteration 9085, loss = 0.00560692\n",
      "Iteration 9086, loss = 0.00560836\n",
      "Iteration 9087, loss = 0.00560924\n",
      "Iteration 9088, loss = 0.00560817\n",
      "Iteration 9089, loss = 0.00560776\n",
      "Iteration 9090, loss = 0.00560617\n",
      "Iteration 9091, loss = 0.00560518\n",
      "Iteration 9092, loss = 0.00560218\n",
      "Iteration 9093, loss = 0.00559900\n",
      "Iteration 9094, loss = 0.00559444\n",
      "Iteration 9095, loss = 0.00559022\n",
      "Iteration 9096, loss = 0.00558687\n",
      "Iteration 9097, loss = 0.00558507\n",
      "Iteration 9098, loss = 0.00558497\n",
      "Iteration 9099, loss = 0.00558615\n",
      "Iteration 9100, loss = 0.00558803\n",
      "Iteration 9101, loss = 0.00558969\n",
      "Iteration 9102, loss = 0.00559095\n",
      "Iteration 9103, loss = 0.00559111\n",
      "Iteration 9104, loss = 0.00559065\n",
      "Iteration 9105, loss = 0.00558934\n",
      "Iteration 9106, loss = 0.00558786\n",
      "Iteration 9107, loss = 0.00558626\n",
      "Iteration 9108, loss = 0.00558496\n",
      "Iteration 9109, loss = 0.00558396\n",
      "Iteration 9110, loss = 0.00558332\n",
      "Iteration 9111, loss = 0.00558290\n",
      "Iteration 9112, loss = 0.00558258\n",
      "Iteration 9113, loss = 0.00558224\n",
      "Iteration 9114, loss = 0.00558185\n",
      "Iteration 9115, loss = 0.00558142\n",
      "Iteration 9116, loss = 0.00558101\n",
      "Iteration 9117, loss = 0.00558068\n",
      "Iteration 9118, loss = 0.00558046\n",
      "Iteration 9119, loss = 0.00558035\n",
      "Iteration 9120, loss = 0.00558033\n",
      "Iteration 9121, loss = 0.00558036\n",
      "Iteration 9122, loss = 0.00558039\n",
      "Iteration 9123, loss = 0.00558043\n",
      "Iteration 9124, loss = 0.00558042\n",
      "Iteration 9125, loss = 0.00558040\n",
      "Iteration 9126, loss = 0.00558035\n",
      "Iteration 9127, loss = 0.00558029\n",
      "Iteration 9128, loss = 0.00558022\n",
      "Iteration 9129, loss = 0.00558019\n",
      "Iteration 9130, loss = 0.00558018\n",
      "Iteration 9131, loss = 0.00558027\n",
      "Iteration 9132, loss = 0.00558040\n",
      "Iteration 9133, loss = 0.00558069\n",
      "Iteration 9134, loss = 0.00558105\n",
      "Iteration 9135, loss = 0.00558165\n",
      "Iteration 9136, loss = 0.00558237\n",
      "Iteration 9137, loss = 0.00558349\n",
      "Iteration 9138, loss = 0.00558479\n",
      "Iteration 9139, loss = 0.00558678\n",
      "Iteration 9140, loss = 0.00558904\n",
      "Iteration 9141, loss = 0.00559254\n",
      "Iteration 9142, loss = 0.00559645\n",
      "Iteration 9143, loss = 0.00560255\n",
      "Iteration 9144, loss = 0.00560911\n",
      "Iteration 9145, loss = 0.00561956\n",
      "Iteration 9146, loss = 0.00563001\n",
      "Iteration 9147, loss = 0.00564699\n",
      "Iteration 9148, loss = 0.00566191\n",
      "Iteration 9149, loss = 0.00568659\n",
      "Iteration 9150, loss = 0.00570322\n",
      "Iteration 9151, loss = 0.00573158\n",
      "Iteration 9152, loss = 0.00574023\n",
      "Iteration 9153, loss = 0.00575820\n",
      "Iteration 9154, loss = 0.00574462\n",
      "Iteration 9155, loss = 0.00573492\n",
      "Iteration 9156, loss = 0.00569711\n",
      "Iteration 9157, loss = 0.00566336\n",
      "Iteration 9158, loss = 0.00562345\n",
      "Iteration 9159, loss = 0.00559465\n",
      "Iteration 9160, loss = 0.00557724\n",
      "Iteration 9161, loss = 0.00557321\n",
      "Iteration 9162, loss = 0.00557982\n",
      "Iteration 9163, loss = 0.00559217\n",
      "Iteration 9164, loss = 0.00560628\n",
      "Iteration 9165, loss = 0.00561533\n",
      "Iteration 9166, loss = 0.00562057\n",
      "Iteration 9167, loss = 0.00561619\n",
      "Iteration 9168, loss = 0.00560868\n",
      "Iteration 9169, loss = 0.00559622\n",
      "Iteration 9170, loss = 0.00558496\n",
      "Iteration 9171, loss = 0.00557575\n",
      "Iteration 9172, loss = 0.00557068\n",
      "Iteration 9173, loss = 0.00556967\n",
      "Iteration 9174, loss = 0.00557182\n",
      "Iteration 9175, loss = 0.00557571\n",
      "Iteration 9176, loss = 0.00557964\n",
      "Iteration 9177, loss = 0.00558283\n",
      "Iteration 9178, loss = 0.00558385\n",
      "Iteration 9179, loss = 0.00558348\n",
      "Iteration 9180, loss = 0.00558103\n",
      "Iteration 9181, loss = 0.00557800\n",
      "Iteration 9182, loss = 0.00557443\n",
      "Iteration 9183, loss = 0.00557139\n",
      "Iteration 9184, loss = 0.00556906\n",
      "Iteration 9185, loss = 0.00556772\n",
      "Iteration 9186, loss = 0.00556727\n",
      "Iteration 9187, loss = 0.00556750\n",
      "Iteration 9188, loss = 0.00556814\n",
      "Iteration 9189, loss = 0.00556890\n",
      "Iteration 9190, loss = 0.00556959\n",
      "Iteration 9191, loss = 0.00556999\n",
      "Iteration 9192, loss = 0.00557013\n",
      "Iteration 9193, loss = 0.00556989\n",
      "Iteration 9194, loss = 0.00556943\n",
      "Iteration 9195, loss = 0.00556873\n",
      "Iteration 9196, loss = 0.00556796\n",
      "Iteration 9197, loss = 0.00556714\n",
      "Iteration 9198, loss = 0.00556639\n",
      "Iteration 9199, loss = 0.00556570\n",
      "Iteration 9200, loss = 0.00556514\n",
      "Iteration 9201, loss = 0.00556469\n",
      "Iteration 9202, loss = 0.00556434\n",
      "Iteration 9203, loss = 0.00556408\n",
      "Iteration 9204, loss = 0.00556389\n",
      "Iteration 9205, loss = 0.00556375\n",
      "Iteration 9206, loss = 0.00556363\n",
      "Iteration 9207, loss = 0.00556353\n",
      "Iteration 9208, loss = 0.00556343\n",
      "Iteration 9209, loss = 0.00556333\n",
      "Iteration 9210, loss = 0.00556322\n",
      "Iteration 9211, loss = 0.00556312\n",
      "Iteration 9212, loss = 0.00556300\n",
      "Iteration 9213, loss = 0.00556289\n",
      "Iteration 9214, loss = 0.00556277\n",
      "Iteration 9215, loss = 0.00556267\n",
      "Iteration 9216, loss = 0.00556257\n",
      "Iteration 9217, loss = 0.00556249\n",
      "Iteration 9218, loss = 0.00556242\n",
      "Iteration 9219, loss = 0.00556240\n",
      "Iteration 9220, loss = 0.00556239\n",
      "Iteration 9221, loss = 0.00556245\n",
      "Iteration 9222, loss = 0.00556255\n",
      "Iteration 9223, loss = 0.00556277\n",
      "Iteration 9224, loss = 0.00556305\n",
      "Iteration 9225, loss = 0.00556353\n",
      "Iteration 9226, loss = 0.00556413\n",
      "Iteration 9227, loss = 0.00556506\n",
      "Iteration 9228, loss = 0.00556622\n",
      "Iteration 9229, loss = 0.00556795\n",
      "Iteration 9230, loss = 0.00557007\n",
      "Iteration 9231, loss = 0.00557325\n",
      "Iteration 9232, loss = 0.00557707\n",
      "Iteration 9233, loss = 0.00558282\n",
      "Iteration 9234, loss = 0.00558950\n",
      "Iteration 9235, loss = 0.00559965\n",
      "Iteration 9236, loss = 0.00561070\n",
      "Iteration 9237, loss = 0.00562764\n",
      "Iteration 9238, loss = 0.00564399\n",
      "Iteration 9239, loss = 0.00566909\n",
      "Iteration 9240, loss = 0.00568805\n",
      "Iteration 9241, loss = 0.00571722\n",
      "Iteration 9242, loss = 0.00572853\n",
      "Iteration 9243, loss = 0.00574816\n",
      "Iteration 9244, loss = 0.00573746\n",
      "Iteration 9245, loss = 0.00573294\n",
      "Iteration 9246, loss = 0.00569929\n",
      "Iteration 9247, loss = 0.00567547\n",
      "Iteration 9248, loss = 0.00564008\n",
      "Iteration 9249, loss = 0.00562055\n",
      "Iteration 9250, loss = 0.00560494\n",
      "Iteration 9251, loss = 0.00560274\n",
      "Iteration 9252, loss = 0.00560445\n",
      "Iteration 9253, loss = 0.00560677\n",
      "Iteration 9254, loss = 0.00560615\n",
      "Iteration 9255, loss = 0.00559789\n",
      "Iteration 9256, loss = 0.00558782\n",
      "Iteration 9257, loss = 0.00557637\n",
      "Iteration 9258, loss = 0.00556986\n",
      "Iteration 9259, loss = 0.00556820\n",
      "Iteration 9260, loss = 0.00557211\n",
      "Iteration 9261, loss = 0.00557696\n",
      "Iteration 9262, loss = 0.00558144\n",
      "Iteration 9263, loss = 0.00558164\n",
      "Iteration 9264, loss = 0.00557767\n",
      "Iteration 9265, loss = 0.00557085\n",
      "Iteration 9266, loss = 0.00556287\n",
      "Iteration 9267, loss = 0.00555663\n",
      "Iteration 9268, loss = 0.00555316\n",
      "Iteration 9269, loss = 0.00555281\n",
      "Iteration 9270, loss = 0.00555468\n",
      "Iteration 9271, loss = 0.00555747\n",
      "Iteration 9272, loss = 0.00555984\n",
      "Iteration 9273, loss = 0.00556086\n",
      "Iteration 9274, loss = 0.00556045\n",
      "Iteration 9275, loss = 0.00555871\n",
      "Iteration 9276, loss = 0.00555651\n",
      "Iteration 9277, loss = 0.00555442\n",
      "Iteration 9278, loss = 0.00555294\n",
      "Iteration 9279, loss = 0.00555226\n",
      "Iteration 9280, loss = 0.00555216\n",
      "Iteration 9281, loss = 0.00555239\n",
      "Iteration 9282, loss = 0.00555251\n",
      "Iteration 9283, loss = 0.00555238\n",
      "Iteration 9284, loss = 0.00555177\n",
      "Iteration 9285, loss = 0.00555090\n",
      "Iteration 9286, loss = 0.00554983\n",
      "Iteration 9287, loss = 0.00554886\n",
      "Iteration 9288, loss = 0.00554814\n",
      "Iteration 9289, loss = 0.00554775\n",
      "Iteration 9290, loss = 0.00554768\n",
      "Iteration 9291, loss = 0.00554782\n",
      "Iteration 9292, loss = 0.00554804\n",
      "Iteration 9293, loss = 0.00554820\n",
      "Iteration 9294, loss = 0.00554824\n",
      "Iteration 9295, loss = 0.00554809\n",
      "Iteration 9296, loss = 0.00554783\n",
      "Iteration 9297, loss = 0.00554747\n",
      "Iteration 9298, loss = 0.00554712\n",
      "Iteration 9299, loss = 0.00554681\n",
      "Iteration 9300, loss = 0.00554661\n",
      "Iteration 9301, loss = 0.00554649\n",
      "Iteration 9302, loss = 0.00554648\n",
      "Iteration 9303, loss = 0.00554653\n",
      "Iteration 9304, loss = 0.00554666\n",
      "Iteration 9305, loss = 0.00554683\n",
      "Iteration 9306, loss = 0.00554708\n",
      "Iteration 9307, loss = 0.00554738\n",
      "Iteration 9308, loss = 0.00554782\n",
      "Iteration 9309, loss = 0.00554837\n",
      "Iteration 9310, loss = 0.00554921\n",
      "Iteration 9311, loss = 0.00555025\n",
      "Iteration 9312, loss = 0.00555186\n",
      "Iteration 9313, loss = 0.00555379\n",
      "Iteration 9314, loss = 0.00555680\n",
      "Iteration 9315, loss = 0.00556025\n",
      "Iteration 9316, loss = 0.00556571\n",
      "Iteration 9317, loss = 0.00557164\n",
      "Iteration 9318, loss = 0.00558120\n",
      "Iteration 9319, loss = 0.00559065\n",
      "Iteration 9320, loss = 0.00560616\n",
      "Iteration 9321, loss = 0.00561906\n",
      "Iteration 9322, loss = 0.00564059\n",
      "Iteration 9323, loss = 0.00565274\n",
      "Iteration 9324, loss = 0.00567399\n",
      "Iteration 9325, loss = 0.00567452\n",
      "Iteration 9326, loss = 0.00568061\n",
      "Iteration 9327, loss = 0.00565953\n",
      "Iteration 9328, loss = 0.00564104\n",
      "Iteration 9329, loss = 0.00560701\n",
      "Iteration 9330, loss = 0.00557940\n",
      "Iteration 9331, loss = 0.00555583\n",
      "Iteration 9332, loss = 0.00554327\n",
      "Iteration 9333, loss = 0.00554104\n",
      "Iteration 9334, loss = 0.00554688\n",
      "Iteration 9335, loss = 0.00555717\n",
      "Iteration 9336, loss = 0.00556708\n",
      "Iteration 9337, loss = 0.00557519\n",
      "Iteration 9338, loss = 0.00557633\n",
      "Iteration 9339, loss = 0.00557430\n",
      "Iteration 9340, loss = 0.00556606\n",
      "Iteration 9341, loss = 0.00555735\n",
      "Iteration 9342, loss = 0.00554814\n",
      "Iteration 9343, loss = 0.00554160\n",
      "Iteration 9344, loss = 0.00553814\n",
      "Iteration 9345, loss = 0.00553782\n",
      "Iteration 9346, loss = 0.00553980\n",
      "Iteration 9347, loss = 0.00554284\n",
      "Iteration 9348, loss = 0.00554599\n",
      "Iteration 9349, loss = 0.00554787\n",
      "Iteration 9350, loss = 0.00554872\n",
      "Iteration 9351, loss = 0.00554761\n",
      "Iteration 9352, loss = 0.00554574\n",
      "Iteration 9353, loss = 0.00554286\n",
      "Iteration 9354, loss = 0.00554014\n",
      "Iteration 9355, loss = 0.00553772\n",
      "Iteration 9356, loss = 0.00553607\n",
      "Iteration 9357, loss = 0.00553523\n",
      "Iteration 9358, loss = 0.00553513\n",
      "Iteration 9359, loss = 0.00553556\n",
      "Iteration 9360, loss = 0.00553625\n",
      "Iteration 9361, loss = 0.00553701\n",
      "Iteration 9362, loss = 0.00553757\n",
      "Iteration 9363, loss = 0.00553794\n",
      "Iteration 9364, loss = 0.00553796\n",
      "Iteration 9365, loss = 0.00553782\n",
      "Iteration 9366, loss = 0.00553747\n",
      "Iteration 9367, loss = 0.00553717\n",
      "Iteration 9368, loss = 0.00553698\n",
      "Iteration 9369, loss = 0.00553712\n",
      "Iteration 9370, loss = 0.00553779\n",
      "Iteration 9371, loss = 0.00553915\n",
      "Iteration 9372, loss = 0.00554169\n",
      "Iteration 9373, loss = 0.00554550\n",
      "Iteration 9374, loss = 0.00555182\n",
      "Iteration 9375, loss = 0.00556041\n",
      "Iteration 9376, loss = 0.00557433\n",
      "Iteration 9377, loss = 0.00559133\n",
      "Iteration 9378, loss = 0.00561840\n",
      "Iteration 9379, loss = 0.00564504\n",
      "Iteration 9380, loss = 0.00568456\n",
      "Iteration 9381, loss = 0.00570523\n",
      "Iteration 9382, loss = 0.00572955\n",
      "Iteration 9383, loss = 0.00570855\n",
      "Iteration 9384, loss = 0.00568270\n",
      "Iteration 9385, loss = 0.00563044\n",
      "Iteration 9386, loss = 0.00559466\n",
      "Iteration 9387, loss = 0.00557350\n",
      "Iteration 9388, loss = 0.00557566\n",
      "Iteration 9389, loss = 0.00558607\n",
      "Iteration 9390, loss = 0.00559547\n",
      "Iteration 9391, loss = 0.00559226\n",
      "Iteration 9392, loss = 0.00557671\n",
      "Iteration 9393, loss = 0.00555611\n",
      "Iteration 9394, loss = 0.00553847\n",
      "Iteration 9395, loss = 0.00553218\n",
      "Iteration 9396, loss = 0.00553652\n",
      "Iteration 9397, loss = 0.00554663\n",
      "Iteration 9398, loss = 0.00555512\n",
      "Iteration 9399, loss = 0.00555759\n",
      "Iteration 9400, loss = 0.00555242\n",
      "Iteration 9401, loss = 0.00554435\n",
      "Iteration 9402, loss = 0.00553777\n",
      "Iteration 9403, loss = 0.00553646\n",
      "Iteration 9404, loss = 0.00553903\n",
      "Iteration 9405, loss = 0.00554243\n",
      "Iteration 9406, loss = 0.00554242\n",
      "Iteration 9407, loss = 0.00553905\n",
      "Iteration 9408, loss = 0.00553313\n",
      "Iteration 9409, loss = 0.00552842\n",
      "Iteration 9410, loss = 0.00552654\n",
      "Iteration 9411, loss = 0.00552743\n",
      "Iteration 9412, loss = 0.00552925\n",
      "Iteration 9413, loss = 0.00553025\n",
      "Iteration 9414, loss = 0.00552967\n",
      "Iteration 9415, loss = 0.00552798\n",
      "Iteration 9416, loss = 0.00552639\n",
      "Iteration 9417, loss = 0.00552563\n",
      "Iteration 9418, loss = 0.00552585\n",
      "Iteration 9419, loss = 0.00552647\n",
      "Iteration 9420, loss = 0.00552710\n",
      "Iteration 9421, loss = 0.00552721\n",
      "Iteration 9422, loss = 0.00552698\n",
      "Iteration 9423, loss = 0.00552644\n",
      "Iteration 9424, loss = 0.00552590\n",
      "Iteration 9425, loss = 0.00552545\n",
      "Iteration 9426, loss = 0.00552523\n",
      "Iteration 9427, loss = 0.00552521\n",
      "Iteration 9428, loss = 0.00552537\n",
      "Iteration 9429, loss = 0.00552552\n",
      "Iteration 9430, loss = 0.00552562\n",
      "Iteration 9431, loss = 0.00552552\n",
      "Iteration 9432, loss = 0.00552539\n",
      "Iteration 9433, loss = 0.00552523\n",
      "Iteration 9434, loss = 0.00552536\n",
      "Iteration 9435, loss = 0.00552566\n",
      "Iteration 9436, loss = 0.00552633\n",
      "Iteration 9437, loss = 0.00552705\n",
      "Iteration 9438, loss = 0.00552810\n",
      "Iteration 9439, loss = 0.00552910\n",
      "Iteration 9440, loss = 0.00553063\n",
      "Iteration 9441, loss = 0.00553226\n",
      "Iteration 9442, loss = 0.00553490\n",
      "Iteration 9443, loss = 0.00553772\n",
      "Iteration 9444, loss = 0.00554216\n",
      "Iteration 9445, loss = 0.00554665\n",
      "Iteration 9446, loss = 0.00555364\n",
      "Iteration 9447, loss = 0.00556019\n",
      "Iteration 9448, loss = 0.00557054\n",
      "Iteration 9449, loss = 0.00557900\n",
      "Iteration 9450, loss = 0.00559272\n",
      "Iteration 9451, loss = 0.00560112\n",
      "Iteration 9452, loss = 0.00561558\n",
      "Iteration 9453, loss = 0.00561903\n",
      "Iteration 9454, loss = 0.00562773\n",
      "Iteration 9455, loss = 0.00562018\n",
      "Iteration 9456, loss = 0.00561594\n",
      "Iteration 9457, loss = 0.00559682\n",
      "Iteration 9458, loss = 0.00558060\n",
      "Iteration 9459, loss = 0.00555879\n",
      "Iteration 9460, loss = 0.00554171\n",
      "Iteration 9461, loss = 0.00552784\n",
      "Iteration 9462, loss = 0.00551981\n",
      "Iteration 9463, loss = 0.00551697\n",
      "Iteration 9464, loss = 0.00551835\n",
      "Iteration 9465, loss = 0.00552249\n",
      "Iteration 9466, loss = 0.00552756\n",
      "Iteration 9467, loss = 0.00553286\n",
      "Iteration 9468, loss = 0.00553626\n",
      "Iteration 9469, loss = 0.00553879\n",
      "Iteration 9470, loss = 0.00553833\n",
      "Iteration 9471, loss = 0.00553711\n",
      "Iteration 9472, loss = 0.00553360\n",
      "Iteration 9473, loss = 0.00553001\n",
      "Iteration 9474, loss = 0.00552562\n",
      "Iteration 9475, loss = 0.00552181\n",
      "Iteration 9476, loss = 0.00551838\n",
      "Iteration 9477, loss = 0.00551586\n",
      "Iteration 9478, loss = 0.00551416\n",
      "Iteration 9479, loss = 0.00551329\n",
      "Iteration 9480, loss = 0.00551311\n",
      "Iteration 9481, loss = 0.00551344\n",
      "Iteration 9482, loss = 0.00551410\n",
      "Iteration 9483, loss = 0.00551488\n",
      "Iteration 9484, loss = 0.00551569\n",
      "Iteration 9485, loss = 0.00551632\n",
      "Iteration 9486, loss = 0.00551684\n",
      "Iteration 9487, loss = 0.00551703\n",
      "Iteration 9488, loss = 0.00551709\n",
      "Iteration 9489, loss = 0.00551683\n",
      "Iteration 9490, loss = 0.00551650\n",
      "Iteration 9491, loss = 0.00551592\n",
      "Iteration 9492, loss = 0.00551535\n",
      "Iteration 9493, loss = 0.00551465\n",
      "Iteration 9494, loss = 0.00551401\n",
      "Iteration 9495, loss = 0.00551332\n",
      "Iteration 9496, loss = 0.00551272\n",
      "Iteration 9497, loss = 0.00551212\n",
      "Iteration 9498, loss = 0.00551161\n",
      "Iteration 9499, loss = 0.00551113\n",
      "Iteration 9500, loss = 0.00551072\n",
      "Iteration 9501, loss = 0.00551034\n",
      "Iteration 9502, loss = 0.00551002\n",
      "Iteration 9503, loss = 0.00550972\n",
      "Iteration 9504, loss = 0.00550947\n",
      "Iteration 9505, loss = 0.00550923\n",
      "Iteration 9506, loss = 0.00550904\n",
      "Iteration 9507, loss = 0.00550887\n",
      "Iteration 9508, loss = 0.00550874\n",
      "Iteration 9509, loss = 0.00550863\n",
      "Iteration 9510, loss = 0.00550858\n",
      "Iteration 9511, loss = 0.00550857\n",
      "Iteration 9512, loss = 0.00550864\n",
      "Iteration 9513, loss = 0.00550877\n",
      "Iteration 9514, loss = 0.00550904\n",
      "Iteration 9515, loss = 0.00550942\n",
      "Iteration 9516, loss = 0.00551006\n",
      "Iteration 9517, loss = 0.00551088\n",
      "Iteration 9518, loss = 0.00551217\n",
      "Iteration 9519, loss = 0.00551382\n",
      "Iteration 9520, loss = 0.00551634\n",
      "Iteration 9521, loss = 0.00551950\n",
      "Iteration 9522, loss = 0.00552438\n",
      "Iteration 9523, loss = 0.00553029\n",
      "Iteration 9524, loss = 0.00553958\n",
      "Iteration 9525, loss = 0.00555029\n",
      "Iteration 9526, loss = 0.00556742\n",
      "Iteration 9527, loss = 0.00558536\n",
      "Iteration 9528, loss = 0.00561451\n",
      "Iteration 9529, loss = 0.00563967\n",
      "Iteration 9530, loss = 0.00568069\n",
      "Iteration 9531, loss = 0.00570265\n",
      "Iteration 9532, loss = 0.00573891\n",
      "Iteration 9533, loss = 0.00573147\n",
      "Iteration 9534, loss = 0.00572842\n",
      "Iteration 9535, loss = 0.00567635\n",
      "Iteration 9536, loss = 0.00562811\n",
      "Iteration 9537, loss = 0.00556797\n",
      "Iteration 9538, loss = 0.00552719\n",
      "Iteration 9539, loss = 0.00550739\n",
      "Iteration 9540, loss = 0.00550981\n",
      "Iteration 9541, loss = 0.00552707\n",
      "Iteration 9542, loss = 0.00554727\n",
      "Iteration 9543, loss = 0.00556405\n",
      "Iteration 9544, loss = 0.00556568\n",
      "Iteration 9545, loss = 0.00555942\n",
      "Iteration 9546, loss = 0.00554216\n",
      "Iteration 9547, loss = 0.00552623\n",
      "Iteration 9548, loss = 0.00551353\n",
      "Iteration 9549, loss = 0.00550853\n",
      "Iteration 9550, loss = 0.00550959\n",
      "Iteration 9551, loss = 0.00551386\n",
      "Iteration 9552, loss = 0.00551830\n",
      "Iteration 9553, loss = 0.00551957\n",
      "Iteration 9554, loss = 0.00551826\n",
      "Iteration 9555, loss = 0.00551408\n",
      "Iteration 9556, loss = 0.00550981\n",
      "Iteration 9557, loss = 0.00550625\n",
      "Iteration 9558, loss = 0.00550479\n",
      "Iteration 9559, loss = 0.00550485\n",
      "Iteration 9560, loss = 0.00550589\n",
      "Iteration 9561, loss = 0.00550676\n",
      "Iteration 9562, loss = 0.00550655\n",
      "Iteration 9563, loss = 0.00550528\n",
      "Iteration 9564, loss = 0.00550300\n",
      "Iteration 9565, loss = 0.00550064\n",
      "Iteration 9566, loss = 0.00549873\n",
      "Iteration 9567, loss = 0.00549778\n",
      "Iteration 9568, loss = 0.00549777\n",
      "Iteration 9569, loss = 0.00549846\n",
      "Iteration 9570, loss = 0.00549936\n",
      "Iteration 9571, loss = 0.00550003\n",
      "Iteration 9572, loss = 0.00550020\n",
      "Iteration 9573, loss = 0.00549975\n",
      "Iteration 9574, loss = 0.00549882\n",
      "Iteration 9575, loss = 0.00549762\n",
      "Iteration 9576, loss = 0.00549641\n",
      "Iteration 9577, loss = 0.00549542\n",
      "Iteration 9578, loss = 0.00549476\n",
      "Iteration 9579, loss = 0.00549444\n",
      "Iteration 9580, loss = 0.00549438\n",
      "Iteration 9581, loss = 0.00549447\n",
      "Iteration 9582, loss = 0.00549458\n",
      "Iteration 9583, loss = 0.00549464\n",
      "Iteration 9584, loss = 0.00549458\n",
      "Iteration 9585, loss = 0.00549440\n",
      "Iteration 9586, loss = 0.00549413\n",
      "Iteration 9587, loss = 0.00549381\n",
      "Iteration 9588, loss = 0.00549346\n",
      "Iteration 9589, loss = 0.00549313\n",
      "Iteration 9590, loss = 0.00549284\n",
      "Iteration 9591, loss = 0.00549259\n",
      "Iteration 9592, loss = 0.00549238\n",
      "Iteration 9593, loss = 0.00549220\n",
      "Iteration 9594, loss = 0.00549204\n",
      "Iteration 9595, loss = 0.00549188\n",
      "Iteration 9596, loss = 0.00549171\n",
      "Iteration 9597, loss = 0.00549153\n",
      "Iteration 9598, loss = 0.00549133\n",
      "Iteration 9599, loss = 0.00549112\n",
      "Iteration 9600, loss = 0.00549090\n",
      "Iteration 9601, loss = 0.00549067\n",
      "Iteration 9602, loss = 0.00549045\n",
      "Iteration 9603, loss = 0.00549022\n",
      "Iteration 9604, loss = 0.00549001\n",
      "Iteration 9605, loss = 0.00548980\n",
      "Iteration 9606, loss = 0.00548961\n",
      "Iteration 9607, loss = 0.00548943\n",
      "Iteration 9608, loss = 0.00548928\n",
      "Iteration 9609, loss = 0.00548914\n",
      "Iteration 9610, loss = 0.00548903\n",
      "Iteration 9611, loss = 0.00548894\n",
      "Iteration 9612, loss = 0.00548890\n",
      "Iteration 9613, loss = 0.00548891\n",
      "Iteration 9614, loss = 0.00548899\n",
      "Iteration 9615, loss = 0.00548915\n",
      "Iteration 9616, loss = 0.00548945\n",
      "Iteration 9617, loss = 0.00548988\n",
      "Iteration 9618, loss = 0.00549059\n",
      "Iteration 9619, loss = 0.00549154\n",
      "Iteration 9620, loss = 0.00549303\n",
      "Iteration 9621, loss = 0.00549495\n",
      "Iteration 9622, loss = 0.00549796\n",
      "Iteration 9623, loss = 0.00550168\n",
      "Iteration 9624, loss = 0.00550756\n",
      "Iteration 9625, loss = 0.00551441\n",
      "Iteration 9626, loss = 0.00552540\n",
      "Iteration 9627, loss = 0.00553699\n",
      "Iteration 9628, loss = 0.00555575\n",
      "Iteration 9629, loss = 0.00557229\n",
      "Iteration 9630, loss = 0.00559876\n",
      "Iteration 9631, loss = 0.00561479\n",
      "Iteration 9632, loss = 0.00563937\n",
      "Iteration 9633, loss = 0.00564078\n",
      "Iteration 9634, loss = 0.00564305\n",
      "Iteration 9635, loss = 0.00561760\n",
      "Iteration 9636, loss = 0.00558904\n",
      "Iteration 9637, loss = 0.00554931\n",
      "Iteration 9638, loss = 0.00551641\n",
      "Iteration 9639, loss = 0.00549400\n",
      "Iteration 9640, loss = 0.00548619\n",
      "Iteration 9641, loss = 0.00549103\n",
      "Iteration 9642, loss = 0.00550292\n",
      "Iteration 9643, loss = 0.00551663\n",
      "Iteration 9644, loss = 0.00552491\n",
      "Iteration 9645, loss = 0.00552769\n",
      "Iteration 9646, loss = 0.00552107\n",
      "Iteration 9647, loss = 0.00551095\n",
      "Iteration 9648, loss = 0.00549836\n",
      "Iteration 9649, loss = 0.00548847\n",
      "Iteration 9650, loss = 0.00548296\n",
      "Iteration 9651, loss = 0.00548234\n",
      "Iteration 9652, loss = 0.00548544\n",
      "Iteration 9653, loss = 0.00549004\n",
      "Iteration 9654, loss = 0.00549422\n",
      "Iteration 9655, loss = 0.00549588\n",
      "Iteration 9656, loss = 0.00549519\n",
      "Iteration 9657, loss = 0.00549188\n",
      "Iteration 9658, loss = 0.00548762\n",
      "Iteration 9659, loss = 0.00548340\n",
      "Iteration 9660, loss = 0.00548036\n",
      "Iteration 9661, loss = 0.00547895\n",
      "Iteration 9662, loss = 0.00547904\n",
      "Iteration 9663, loss = 0.00548015\n",
      "Iteration 9664, loss = 0.00548153\n",
      "Iteration 9665, loss = 0.00548269\n",
      "Iteration 9666, loss = 0.00548305\n",
      "Iteration 9667, loss = 0.00548271\n",
      "Iteration 9668, loss = 0.00548159\n",
      "Iteration 9669, loss = 0.00548014\n",
      "Iteration 9670, loss = 0.00547861\n",
      "Iteration 9671, loss = 0.00547736\n",
      "Iteration 9672, loss = 0.00547654\n",
      "Iteration 9673, loss = 0.00547623\n",
      "Iteration 9674, loss = 0.00547632\n",
      "Iteration 9675, loss = 0.00547667\n",
      "Iteration 9676, loss = 0.00547711\n",
      "Iteration 9677, loss = 0.00547751\n",
      "Iteration 9678, loss = 0.00547777\n",
      "Iteration 9679, loss = 0.00547794\n",
      "Iteration 9680, loss = 0.00547802\n",
      "Iteration 9681, loss = 0.00547821\n",
      "Iteration 9682, loss = 0.00547856\n",
      "Iteration 9683, loss = 0.00547941\n",
      "Iteration 9684, loss = 0.00548078\n",
      "Iteration 9685, loss = 0.00548324\n",
      "Iteration 9686, loss = 0.00548670\n",
      "Iteration 9687, loss = 0.00549235\n",
      "Iteration 9688, loss = 0.00549969\n",
      "Iteration 9689, loss = 0.00551151\n",
      "Iteration 9690, loss = 0.00552559\n",
      "Iteration 9691, loss = 0.00554841\n",
      "Iteration 9692, loss = 0.00557181\n",
      "Iteration 9693, loss = 0.00560951\n",
      "Iteration 9694, loss = 0.00563692\n",
      "Iteration 9695, loss = 0.00567965\n",
      "Iteration 9696, loss = 0.00568592\n",
      "Iteration 9697, loss = 0.00569890\n",
      "Iteration 9698, loss = 0.00565958\n",
      "Iteration 9699, loss = 0.00562587\n",
      "Iteration 9700, loss = 0.00556989\n",
      "Iteration 9701, loss = 0.00553239\n",
      "Iteration 9702, loss = 0.00550540\n",
      "Iteration 9703, loss = 0.00549638\n",
      "Iteration 9704, loss = 0.00549747\n",
      "Iteration 9705, loss = 0.00550186\n",
      "Iteration 9706, loss = 0.00550738\n",
      "Iteration 9707, loss = 0.00550843\n",
      "Iteration 9708, loss = 0.00551084\n",
      "Iteration 9709, loss = 0.00551008\n",
      "Iteration 9710, loss = 0.00551085\n",
      "Iteration 9711, loss = 0.00550544\n",
      "Iteration 9712, loss = 0.00549833\n",
      "Iteration 9713, loss = 0.00548677\n",
      "Iteration 9714, loss = 0.00547645\n",
      "Iteration 9715, loss = 0.00547033\n",
      "Iteration 9716, loss = 0.00547016\n",
      "Iteration 9717, loss = 0.00547502\n",
      "Iteration 9718, loss = 0.00548123\n",
      "Iteration 9719, loss = 0.00548624\n",
      "Iteration 9720, loss = 0.00548612\n",
      "Iteration 9721, loss = 0.00548281\n",
      "Iteration 9722, loss = 0.00547650\n",
      "Iteration 9723, loss = 0.00547070\n",
      "Iteration 9724, loss = 0.00546682\n",
      "Iteration 9725, loss = 0.00546561\n",
      "Iteration 9726, loss = 0.00546651\n",
      "Iteration 9727, loss = 0.00546829\n",
      "Iteration 9728, loss = 0.00546982\n",
      "Iteration 9729, loss = 0.00547025\n",
      "Iteration 9730, loss = 0.00546972\n",
      "Iteration 9731, loss = 0.00546841\n",
      "Iteration 9732, loss = 0.00546709\n",
      "Iteration 9733, loss = 0.00546597\n",
      "Iteration 9734, loss = 0.00546527\n",
      "Iteration 9735, loss = 0.00546476\n",
      "Iteration 9736, loss = 0.00546427\n",
      "Iteration 9737, loss = 0.00546367\n",
      "Iteration 9738, loss = 0.00546303\n",
      "Iteration 9739, loss = 0.00546247\n",
      "Iteration 9740, loss = 0.00546213\n",
      "Iteration 9741, loss = 0.00546204\n",
      "Iteration 9742, loss = 0.00546212\n",
      "Iteration 9743, loss = 0.00546224\n",
      "Iteration 9744, loss = 0.00546224\n",
      "Iteration 9745, loss = 0.00546205\n",
      "Iteration 9746, loss = 0.00546162\n",
      "Iteration 9747, loss = 0.00546105\n",
      "Iteration 9748, loss = 0.00546038\n",
      "Iteration 9749, loss = 0.00545976\n",
      "Iteration 9750, loss = 0.00545925\n",
      "Iteration 9751, loss = 0.00545889\n",
      "Iteration 9752, loss = 0.00545867\n",
      "Iteration 9753, loss = 0.00545853\n",
      "Iteration 9754, loss = 0.00545842\n",
      "Iteration 9755, loss = 0.00545830\n",
      "Iteration 9756, loss = 0.00545813\n",
      "Iteration 9757, loss = 0.00545792\n",
      "Iteration 9758, loss = 0.00545767\n",
      "Iteration 9759, loss = 0.00545740\n",
      "Iteration 9760, loss = 0.00545714\n",
      "Iteration 9761, loss = 0.00545688\n",
      "Iteration 9762, loss = 0.00545663\n",
      "Iteration 9763, loss = 0.00545638\n",
      "Iteration 9764, loss = 0.00545614\n",
      "Iteration 9765, loss = 0.00545588\n",
      "Iteration 9766, loss = 0.00545562\n",
      "Iteration 9767, loss = 0.00545535\n",
      "Iteration 9768, loss = 0.00545507\n",
      "Iteration 9769, loss = 0.00545479\n",
      "Iteration 9770, loss = 0.00545450\n",
      "Iteration 9771, loss = 0.00545423\n",
      "Iteration 9772, loss = 0.00545396\n",
      "Iteration 9773, loss = 0.00545370\n",
      "Iteration 9774, loss = 0.00545345\n",
      "Iteration 9775, loss = 0.00545320\n",
      "Iteration 9776, loss = 0.00545296\n",
      "Iteration 9777, loss = 0.00545272\n",
      "Iteration 9778, loss = 0.00545247\n",
      "Iteration 9779, loss = 0.00545223\n",
      "Iteration 9780, loss = 0.00545198\n",
      "Iteration 9781, loss = 0.00545173\n",
      "Iteration 9782, loss = 0.00545149\n",
      "Iteration 9783, loss = 0.00545124\n",
      "Iteration 9784, loss = 0.00545099\n",
      "Iteration 9785, loss = 0.00545074\n",
      "Iteration 9786, loss = 0.00545049\n",
      "Iteration 9787, loss = 0.00545025\n",
      "Iteration 9788, loss = 0.00545000\n",
      "Iteration 9789, loss = 0.00544976\n",
      "Iteration 9790, loss = 0.00544952\n",
      "Iteration 9791, loss = 0.00544929\n",
      "Iteration 9792, loss = 0.00544906\n",
      "Iteration 9793, loss = 0.00544883\n",
      "Iteration 9794, loss = 0.00544862\n",
      "Iteration 9795, loss = 0.00544842\n",
      "Iteration 9796, loss = 0.00544824\n",
      "Iteration 9797, loss = 0.00544809\n",
      "Iteration 9798, loss = 0.00544799\n",
      "Iteration 9799, loss = 0.00544795\n",
      "Iteration 9800, loss = 0.00544802\n",
      "Iteration 9801, loss = 0.00544822\n",
      "Iteration 9802, loss = 0.00544867\n",
      "Iteration 9803, loss = 0.00544942\n",
      "Iteration 9804, loss = 0.00545076\n",
      "Iteration 9805, loss = 0.00545276\n",
      "Iteration 9806, loss = 0.00545621\n",
      "Iteration 9807, loss = 0.00546109\n",
      "Iteration 9808, loss = 0.00546957\n",
      "Iteration 9809, loss = 0.00548083\n",
      "Iteration 9810, loss = 0.00550105\n",
      "Iteration 9811, loss = 0.00552500\n",
      "Iteration 9812, loss = 0.00556962\n",
      "Iteration 9813, loss = 0.00561145\n",
      "Iteration 9814, loss = 0.00569182\n",
      "Iteration 9815, loss = 0.00573540\n",
      "Iteration 9816, loss = 0.00582348\n",
      "Iteration 9817, loss = 0.00581357\n",
      "Iteration 9818, loss = 0.00582211\n",
      "Iteration 9819, loss = 0.00572618\n",
      "Iteration 9820, loss = 0.00563594\n",
      "Iteration 9821, loss = 0.00553081\n",
      "Iteration 9822, loss = 0.00547075\n",
      "Iteration 9823, loss = 0.00546276\n",
      "Iteration 9824, loss = 0.00549445\n",
      "Iteration 9825, loss = 0.00554508\n",
      "Iteration 9826, loss = 0.00557111\n",
      "Iteration 9827, loss = 0.00557127\n",
      "Iteration 9828, loss = 0.00552751\n",
      "Iteration 9829, loss = 0.00547963\n",
      "Iteration 9830, loss = 0.00544803\n",
      "Iteration 9831, loss = 0.00544807\n",
      "Iteration 9832, loss = 0.00547045\n",
      "Iteration 9833, loss = 0.00549236\n",
      "Iteration 9834, loss = 0.00550018\n",
      "Iteration 9835, loss = 0.00548489\n",
      "Iteration 9836, loss = 0.00546340\n",
      "Iteration 9837, loss = 0.00544683\n",
      "Iteration 9838, loss = 0.00544406\n",
      "Iteration 9839, loss = 0.00545119\n",
      "Iteration 9840, loss = 0.00545896\n",
      "Iteration 9841, loss = 0.00546145\n",
      "Iteration 9842, loss = 0.00545610\n",
      "Iteration 9843, loss = 0.00544891\n",
      "Iteration 9844, loss = 0.00544360\n",
      "Iteration 9845, loss = 0.00544262\n",
      "Iteration 9846, loss = 0.00544377\n",
      "Iteration 9847, loss = 0.00544454\n",
      "Iteration 9848, loss = 0.00544385\n",
      "Iteration 9849, loss = 0.00544179\n",
      "Iteration 9850, loss = 0.00544008\n",
      "Iteration 9851, loss = 0.00543906\n",
      "Iteration 9852, loss = 0.00543889\n",
      "Iteration 9853, loss = 0.00543850\n",
      "Iteration 9854, loss = 0.00543776\n",
      "Iteration 9855, loss = 0.00543674\n",
      "Iteration 9856, loss = 0.00543568\n",
      "Iteration 9857, loss = 0.00543497\n",
      "Iteration 9858, loss = 0.00543444\n",
      "Iteration 9859, loss = 0.00543418\n",
      "Iteration 9860, loss = 0.00543387\n",
      "Iteration 9861, loss = 0.00543358\n",
      "Iteration 9862, loss = 0.00543323\n",
      "Iteration 9863, loss = 0.00543277\n",
      "Iteration 9864, loss = 0.00543220\n",
      "Iteration 9865, loss = 0.00543147\n",
      "Iteration 9866, loss = 0.00543080\n",
      "Iteration 9867, loss = 0.00543028\n",
      "Iteration 9868, loss = 0.00543006\n",
      "Iteration 9869, loss = 0.00543005\n",
      "Iteration 9870, loss = 0.00543008\n",
      "Iteration 9871, loss = 0.00542993\n",
      "Iteration 9872, loss = 0.00542949\n",
      "Iteration 9873, loss = 0.00542884\n",
      "Iteration 9874, loss = 0.00542816\n",
      "Iteration 9875, loss = 0.00542765\n",
      "Iteration 9876, loss = 0.00542738\n",
      "Iteration 9877, loss = 0.00542729\n",
      "Iteration 9878, loss = 0.00542726\n",
      "Iteration 9879, loss = 0.00542715\n",
      "Iteration 9880, loss = 0.00542690\n",
      "Iteration 9881, loss = 0.00542652\n",
      "Iteration 9882, loss = 0.00542608\n",
      "Iteration 9883, loss = 0.00542566\n",
      "Iteration 9884, loss = 0.00542531\n",
      "Iteration 9885, loss = 0.00542503\n",
      "Iteration 9886, loss = 0.00542480\n",
      "Iteration 9887, loss = 0.00542460\n",
      "Iteration 9888, loss = 0.00542440\n",
      "Iteration 9889, loss = 0.00542417\n",
      "Iteration 9890, loss = 0.00542392\n",
      "Iteration 9891, loss = 0.00542365\n",
      "Iteration 9892, loss = 0.00542336\n",
      "Iteration 9893, loss = 0.00542306\n",
      "Iteration 9894, loss = 0.00542276\n",
      "Iteration 9895, loss = 0.00542247\n",
      "Iteration 9896, loss = 0.00542220\n",
      "Iteration 9897, loss = 0.00542194\n",
      "Iteration 9898, loss = 0.00542170\n",
      "Iteration 9899, loss = 0.00542147\n",
      "Iteration 9900, loss = 0.00542124\n",
      "Iteration 9901, loss = 0.00542100\n",
      "Iteration 9902, loss = 0.00542075\n",
      "Iteration 9903, loss = 0.00542049\n",
      "Iteration 9904, loss = 0.00542022\n",
      "Iteration 9905, loss = 0.00541995\n",
      "Iteration 9906, loss = 0.00541968\n",
      "Iteration 9907, loss = 0.00541942\n",
      "Iteration 9908, loss = 0.00541917\n",
      "Iteration 9909, loss = 0.00541891\n",
      "Iteration 9910, loss = 0.00541867\n",
      "Iteration 9911, loss = 0.00541842\n",
      "Iteration 9912, loss = 0.00541817\n",
      "Iteration 9913, loss = 0.00541793\n",
      "Iteration 9914, loss = 0.00541768\n",
      "Iteration 9915, loss = 0.00541743\n",
      "Iteration 9916, loss = 0.00541719\n",
      "Iteration 9917, loss = 0.00541694\n",
      "Iteration 9918, loss = 0.00541669\n",
      "Iteration 9919, loss = 0.00541644\n",
      "Iteration 9920, loss = 0.00541619\n",
      "Iteration 9921, loss = 0.00541594\n",
      "Iteration 9922, loss = 0.00541569\n",
      "Iteration 9923, loss = 0.00541544\n",
      "Iteration 9924, loss = 0.00541519\n",
      "Iteration 9925, loss = 0.00541495\n",
      "Iteration 9926, loss = 0.00541470\n",
      "Iteration 9927, loss = 0.00541446\n",
      "Iteration 9928, loss = 0.00541421\n",
      "Iteration 9929, loss = 0.00541397\n",
      "Iteration 9930, loss = 0.00541372\n",
      "Iteration 9931, loss = 0.00541348\n",
      "Iteration 9932, loss = 0.00541324\n",
      "Iteration 9933, loss = 0.00541300\n",
      "Iteration 9934, loss = 0.00541275\n",
      "Iteration 9935, loss = 0.00541251\n",
      "Iteration 9936, loss = 0.00541227\n",
      "Iteration 9937, loss = 0.00541203\n",
      "Iteration 9938, loss = 0.00541179\n",
      "Iteration 9939, loss = 0.00541155\n",
      "Iteration 9940, loss = 0.00541131\n",
      "Iteration 9941, loss = 0.00541108\n",
      "Iteration 9942, loss = 0.00541084\n",
      "Iteration 9943, loss = 0.00541060\n",
      "Iteration 9944, loss = 0.00541037\n",
      "Iteration 9945, loss = 0.00541014\n",
      "Iteration 9946, loss = 0.00540991\n",
      "Iteration 9947, loss = 0.00540969\n",
      "Iteration 9948, loss = 0.00540947\n",
      "Iteration 9949, loss = 0.00540926\n",
      "Iteration 9950, loss = 0.00540907\n",
      "Iteration 9951, loss = 0.00540891\n",
      "Iteration 9952, loss = 0.00540878\n",
      "Iteration 9953, loss = 0.00540872\n",
      "Iteration 9954, loss = 0.00540877\n",
      "Iteration 9955, loss = 0.00540897\n",
      "Iteration 9956, loss = 0.00540948\n",
      "Iteration 9957, loss = 0.00541039\n",
      "Iteration 9958, loss = 0.00541220\n",
      "Iteration 9959, loss = 0.00541505\n",
      "Iteration 9960, loss = 0.00542059\n",
      "Iteration 9961, loss = 0.00542859\n",
      "Iteration 9962, loss = 0.00544481\n",
      "Iteration 9963, loss = 0.00546488\n",
      "Iteration 9964, loss = 0.00550835\n",
      "Iteration 9965, loss = 0.00554658\n",
      "Iteration 9966, loss = 0.00563491\n",
      "Iteration 9967, loss = 0.00566740\n",
      "Iteration 9968, loss = 0.00575733\n",
      "Iteration 9969, loss = 0.00573834\n",
      "Iteration 9970, loss = 0.00575018\n",
      "Iteration 9971, loss = 0.00567362\n",
      "Iteration 9972, loss = 0.00560835\n",
      "Iteration 9973, loss = 0.00551744\n",
      "Iteration 9974, loss = 0.00545939\n",
      "Iteration 9975, loss = 0.00544058\n",
      "Iteration 9976, loss = 0.00546053\n",
      "Iteration 9977, loss = 0.00549997\n",
      "Iteration 9978, loss = 0.00551450\n",
      "Iteration 9979, loss = 0.00550077\n",
      "Iteration 9980, loss = 0.00545727\n",
      "Iteration 9981, loss = 0.00542656\n",
      "Iteration 9982, loss = 0.00542685\n",
      "Iteration 9983, loss = 0.00545041\n",
      "Iteration 9984, loss = 0.00546713\n",
      "Iteration 9985, loss = 0.00545664\n",
      "Iteration 9986, loss = 0.00542922\n",
      "Iteration 9987, loss = 0.00540722\n",
      "Iteration 9988, loss = 0.00540617\n",
      "Iteration 9989, loss = 0.00541909\n",
      "Iteration 9990, loss = 0.00543041\n",
      "Iteration 9991, loss = 0.00543090\n",
      "Iteration 9992, loss = 0.00542177\n",
      "Iteration 9993, loss = 0.00541226\n",
      "Iteration 9994, loss = 0.00540721\n",
      "Iteration 9995, loss = 0.00540743\n",
      "Iteration 9996, loss = 0.00540975\n",
      "Iteration 9997, loss = 0.00541028\n",
      "Iteration 9998, loss = 0.00540868\n",
      "Iteration 9999, loss = 0.00540567\n",
      "Iteration 10000, loss = 0.00540376\n",
      "Iteration 10001, loss = 0.00540293\n",
      "Iteration 10002, loss = 0.00540296\n",
      "Iteration 10003, loss = 0.00540338\n",
      "Iteration 10004, loss = 0.00540365\n",
      "Iteration 10005, loss = 0.00540362\n",
      "Iteration 10006, loss = 0.00540186\n",
      "Iteration 10007, loss = 0.00539961\n",
      "Iteration 10008, loss = 0.00539748\n",
      "Iteration 10009, loss = 0.00539711\n",
      "Iteration 10010, loss = 0.00539823\n",
      "Iteration 10011, loss = 0.00539923\n",
      "Iteration 10012, loss = 0.00539917\n",
      "Iteration 10013, loss = 0.00539765\n",
      "Iteration 10014, loss = 0.00539622\n",
      "Iteration 10015, loss = 0.00539563\n",
      "Iteration 10016, loss = 0.00539596\n",
      "Iteration 10017, loss = 0.00539651\n",
      "Iteration 10018, loss = 0.00539655\n",
      "Iteration 10019, loss = 0.00539594\n",
      "Iteration 10020, loss = 0.00539490\n",
      "Iteration 10021, loss = 0.00539403\n",
      "Iteration 10022, loss = 0.00539367\n",
      "Iteration 10023, loss = 0.00539374\n",
      "Iteration 10024, loss = 0.00539390\n",
      "Iteration 10025, loss = 0.00539382\n",
      "Iteration 10026, loss = 0.00539348\n",
      "Iteration 10027, loss = 0.00539304\n",
      "Iteration 10028, loss = 0.00539269\n",
      "Iteration 10029, loss = 0.00539246\n",
      "Iteration 10030, loss = 0.00539229\n",
      "Iteration 10031, loss = 0.00539213\n",
      "Iteration 10032, loss = 0.00539196\n",
      "Iteration 10033, loss = 0.00539180\n",
      "Iteration 10034, loss = 0.00539160\n",
      "Iteration 10035, loss = 0.00539135\n",
      "Iteration 10036, loss = 0.00539105\n",
      "Iteration 10037, loss = 0.00539076\n",
      "Iteration 10038, loss = 0.00539050\n",
      "Iteration 10039, loss = 0.00539029\n",
      "Iteration 10040, loss = 0.00539013\n",
      "Iteration 10041, loss = 0.00538998\n",
      "Iteration 10042, loss = 0.00538981\n",
      "Iteration 10043, loss = 0.00538960\n",
      "Iteration 10044, loss = 0.00538935\n",
      "Iteration 10045, loss = 0.00538909\n",
      "Iteration 10046, loss = 0.00538886\n",
      "Iteration 10047, loss = 0.00538867\n",
      "Iteration 10048, loss = 0.00538851\n",
      "Iteration 10049, loss = 0.00538835\n",
      "Iteration 10050, loss = 0.00538817\n",
      "Iteration 10051, loss = 0.00538797\n",
      "Iteration 10052, loss = 0.00538777\n",
      "Iteration 10053, loss = 0.00538756\n",
      "Iteration 10054, loss = 0.00538736\n",
      "Iteration 10055, loss = 0.00538717\n",
      "Iteration 10056, loss = 0.00538698\n",
      "Iteration 10057, loss = 0.00538679\n",
      "Iteration 10058, loss = 0.00538661\n",
      "Iteration 10059, loss = 0.00538643\n",
      "Iteration 10060, loss = 0.00538625\n",
      "Iteration 10061, loss = 0.00538607\n",
      "Iteration 10062, loss = 0.00538589\n",
      "Iteration 10063, loss = 0.00538571\n",
      "Iteration 10064, loss = 0.00538554\n",
      "Iteration 10065, loss = 0.00538536\n",
      "Iteration 10066, loss = 0.00538520\n",
      "Iteration 10067, loss = 0.00538504\n",
      "Iteration 10068, loss = 0.00538489\n",
      "Iteration 10069, loss = 0.00538477\n",
      "Iteration 10070, loss = 0.00538467\n",
      "Iteration 10071, loss = 0.00538461\n",
      "Iteration 10072, loss = 0.00538460\n",
      "Iteration 10073, loss = 0.00538465\n",
      "Iteration 10074, loss = 0.00538482\n",
      "Iteration 10075, loss = 0.00538513\n",
      "Iteration 10076, loss = 0.00538570\n",
      "Iteration 10077, loss = 0.00538657\n",
      "Iteration 10078, loss = 0.00538804\n",
      "Iteration 10079, loss = 0.00539016\n",
      "Iteration 10080, loss = 0.00539371\n",
      "Iteration 10081, loss = 0.00539859\n",
      "Iteration 10082, loss = 0.00540689\n",
      "Iteration 10083, loss = 0.00541760\n",
      "Iteration 10084, loss = 0.00543639\n",
      "Iteration 10085, loss = 0.00545785\n",
      "Iteration 10086, loss = 0.00549661\n",
      "Iteration 10087, loss = 0.00553064\n",
      "Iteration 10088, loss = 0.00559309\n",
      "Iteration 10089, loss = 0.00561808\n",
      "Iteration 10090, loss = 0.00566698\n",
      "Iteration 10091, loss = 0.00562956\n",
      "Iteration 10092, loss = 0.00559534\n",
      "Iteration 10093, loss = 0.00550407\n",
      "Iteration 10094, loss = 0.00543283\n",
      "Iteration 10095, loss = 0.00538897\n",
      "Iteration 10096, loss = 0.00538671\n",
      "Iteration 10097, loss = 0.00541531\n",
      "Iteration 10098, loss = 0.00544977\n",
      "Iteration 10099, loss = 0.00547611\n",
      "Iteration 10100, loss = 0.00546753\n",
      "Iteration 10101, loss = 0.00544360\n",
      "Iteration 10102, loss = 0.00540730\n",
      "Iteration 10103, loss = 0.00538381\n",
      "Iteration 10104, loss = 0.00538018\n",
      "Iteration 10105, loss = 0.00539303\n",
      "Iteration 10106, loss = 0.00541099\n",
      "Iteration 10107, loss = 0.00541940\n",
      "Iteration 10108, loss = 0.00541670\n",
      "Iteration 10109, loss = 0.00540177\n",
      "Iteration 10110, loss = 0.00538679\n",
      "Iteration 10111, loss = 0.00537816\n",
      "Iteration 10112, loss = 0.00537879\n",
      "Iteration 10113, loss = 0.00538556\n",
      "Iteration 10114, loss = 0.00539230\n",
      "Iteration 10115, loss = 0.00539514\n",
      "Iteration 10116, loss = 0.00539154\n",
      "Iteration 10117, loss = 0.00538520\n",
      "Iteration 10118, loss = 0.00537896\n",
      "Iteration 10119, loss = 0.00537609\n",
      "Iteration 10120, loss = 0.00537697\n",
      "Iteration 10121, loss = 0.00537998\n",
      "Iteration 10122, loss = 0.00538284\n",
      "Iteration 10123, loss = 0.00538344\n",
      "Iteration 10124, loss = 0.00538199\n",
      "Iteration 10125, loss = 0.00537890\n",
      "Iteration 10126, loss = 0.00537613\n",
      "Iteration 10127, loss = 0.00537465\n",
      "Iteration 10128, loss = 0.00537482\n",
      "Iteration 10129, loss = 0.00537603\n",
      "Iteration 10130, loss = 0.00537726\n",
      "Iteration 10131, loss = 0.00537777\n",
      "Iteration 10132, loss = 0.00537718\n",
      "Iteration 10133, loss = 0.00537594\n",
      "Iteration 10134, loss = 0.00537457\n",
      "Iteration 10135, loss = 0.00537365\n",
      "Iteration 10136, loss = 0.00537344\n",
      "Iteration 10137, loss = 0.00537381\n",
      "Iteration 10138, loss = 0.00537445\n",
      "Iteration 10139, loss = 0.00537494\n",
      "Iteration 10140, loss = 0.00537519\n",
      "Iteration 10141, loss = 0.00537510\n",
      "Iteration 10142, loss = 0.00537500\n",
      "Iteration 10143, loss = 0.00537503\n",
      "Iteration 10144, loss = 0.00537558\n",
      "Iteration 10145, loss = 0.00537666\n",
      "Iteration 10146, loss = 0.00537856\n",
      "Iteration 10147, loss = 0.00538109\n",
      "Iteration 10148, loss = 0.00538482\n",
      "Iteration 10149, loss = 0.00538936\n",
      "Iteration 10150, loss = 0.00539619\n",
      "Iteration 10151, loss = 0.00540413\n",
      "Iteration 10152, loss = 0.00541651\n",
      "Iteration 10153, loss = 0.00542924\n",
      "Iteration 10154, loss = 0.00544893\n",
      "Iteration 10155, loss = 0.00546381\n",
      "Iteration 10156, loss = 0.00548580\n",
      "Iteration 10157, loss = 0.00549032\n",
      "Iteration 10158, loss = 0.00549713\n",
      "Iteration 10159, loss = 0.00547640\n",
      "Iteration 10160, loss = 0.00545587\n",
      "Iteration 10161, loss = 0.00542137\n",
      "Iteration 10162, loss = 0.00539567\n",
      "Iteration 10163, loss = 0.00537816\n",
      "Iteration 10164, loss = 0.00537356\n",
      "Iteration 10165, loss = 0.00537839\n",
      "Iteration 10166, loss = 0.00538737\n",
      "Iteration 10167, loss = 0.00539613\n",
      "Iteration 10168, loss = 0.00539896\n",
      "Iteration 10169, loss = 0.00539846\n",
      "Iteration 10170, loss = 0.00539187\n",
      "Iteration 10171, loss = 0.00538580\n",
      "Iteration 10172, loss = 0.00537961\n",
      "Iteration 10173, loss = 0.00537627\n",
      "Iteration 10174, loss = 0.00537459\n",
      "Iteration 10175, loss = 0.00537419\n",
      "Iteration 10176, loss = 0.00537402\n",
      "Iteration 10177, loss = 0.00537360\n",
      "Iteration 10178, loss = 0.00537326\n",
      "Iteration 10179, loss = 0.00537286\n",
      "Iteration 10180, loss = 0.00537305\n",
      "Iteration 10181, loss = 0.00537319\n",
      "Iteration 10182, loss = 0.00537337\n",
      "Iteration 10183, loss = 0.00537275\n",
      "Iteration 10184, loss = 0.00537157\n",
      "Iteration 10185, loss = 0.00536968\n",
      "Iteration 10186, loss = 0.00536773\n",
      "Iteration 10187, loss = 0.00536612\n",
      "Iteration 10188, loss = 0.00536525\n",
      "Iteration 10189, loss = 0.00536520\n",
      "Iteration 10190, loss = 0.00536576\n",
      "Iteration 10191, loss = 0.00536659\n",
      "Iteration 10192, loss = 0.00536724\n",
      "Iteration 10193, loss = 0.00536755\n",
      "Iteration 10194, loss = 0.00536728\n",
      "Iteration 10195, loss = 0.00536665\n",
      "Iteration 10196, loss = 0.00536572\n",
      "Iteration 10197, loss = 0.00536481\n",
      "Iteration 10198, loss = 0.00536402\n",
      "Iteration 10199, loss = 0.00536350\n",
      "Iteration 10200, loss = 0.00536321\n",
      "Iteration 10201, loss = 0.00536311\n",
      "Iteration 10202, loss = 0.00536310\n",
      "Iteration 10203, loss = 0.00536309\n",
      "Iteration 10204, loss = 0.00536303\n",
      "Iteration 10205, loss = 0.00536289\n",
      "Iteration 10206, loss = 0.00536269\n",
      "Iteration 10207, loss = 0.00536247\n",
      "Iteration 10208, loss = 0.00536226\n",
      "Iteration 10209, loss = 0.00536208\n",
      "Iteration 10210, loss = 0.00536196\n",
      "Iteration 10211, loss = 0.00536187\n",
      "Iteration 10212, loss = 0.00536180\n",
      "Iteration 10213, loss = 0.00536174\n",
      "Iteration 10214, loss = 0.00536166\n",
      "Iteration 10215, loss = 0.00536156\n",
      "Iteration 10216, loss = 0.00536144\n",
      "Iteration 10217, loss = 0.00536130\n",
      "Iteration 10218, loss = 0.00536115\n",
      "Iteration 10219, loss = 0.00536100\n",
      "Iteration 10220, loss = 0.00536087\n",
      "Iteration 10221, loss = 0.00536076\n",
      "Iteration 10222, loss = 0.00536070\n",
      "Iteration 10223, loss = 0.00536067\n",
      "Iteration 10224, loss = 0.00536070\n",
      "Iteration 10225, loss = 0.00536078\n",
      "Iteration 10226, loss = 0.00536096\n",
      "Iteration 10227, loss = 0.00536120\n",
      "Iteration 10228, loss = 0.00536163\n",
      "Iteration 10229, loss = 0.00536216\n",
      "Iteration 10230, loss = 0.00536305\n",
      "Iteration 10231, loss = 0.00536411\n",
      "Iteration 10232, loss = 0.00536590\n",
      "Iteration 10233, loss = 0.00536798\n",
      "Iteration 10234, loss = 0.00537153\n",
      "Iteration 10235, loss = 0.00537549\n",
      "Iteration 10236, loss = 0.00538248\n",
      "Iteration 10237, loss = 0.00538965\n",
      "Iteration 10238, loss = 0.00540290\n",
      "Iteration 10239, loss = 0.00541469\n",
      "Iteration 10240, loss = 0.00543757\n",
      "Iteration 10241, loss = 0.00545340\n",
      "Iteration 10242, loss = 0.00548581\n",
      "Iteration 10243, loss = 0.00549900\n",
      "Iteration 10244, loss = 0.00552952\n",
      "Iteration 10245, loss = 0.00552624\n",
      "Iteration 10246, loss = 0.00553260\n",
      "Iteration 10247, loss = 0.00550227\n",
      "Iteration 10248, loss = 0.00547462\n",
      "Iteration 10249, loss = 0.00543028\n",
      "Iteration 10250, loss = 0.00539476\n",
      "Iteration 10251, loss = 0.00536880\n",
      "Iteration 10252, loss = 0.00535886\n",
      "Iteration 10253, loss = 0.00536277\n",
      "Iteration 10254, loss = 0.00537479\n",
      "Iteration 10255, loss = 0.00538899\n",
      "Iteration 10256, loss = 0.00539731\n",
      "Iteration 10257, loss = 0.00540059\n",
      "Iteration 10258, loss = 0.00539454\n",
      "Iteration 10259, loss = 0.00538651\n",
      "Iteration 10260, loss = 0.00537591\n",
      "Iteration 10261, loss = 0.00536834\n",
      "Iteration 10262, loss = 0.00536325\n",
      "Iteration 10263, loss = 0.00536154\n",
      "Iteration 10264, loss = 0.00536193\n",
      "Iteration 10265, loss = 0.00536308\n",
      "Iteration 10266, loss = 0.00536430\n",
      "Iteration 10267, loss = 0.00536434\n",
      "Iteration 10268, loss = 0.00536394\n",
      "Iteration 10269, loss = 0.00536255\n",
      "Iteration 10270, loss = 0.00536147\n",
      "Iteration 10271, loss = 0.00536034\n",
      "Iteration 10272, loss = 0.00535977\n",
      "Iteration 10273, loss = 0.00535919\n",
      "Iteration 10274, loss = 0.00535862\n",
      "Iteration 10275, loss = 0.00535773\n",
      "Iteration 10276, loss = 0.00535655\n",
      "Iteration 10277, loss = 0.00535528\n",
      "Iteration 10278, loss = 0.00535410\n",
      "Iteration 10279, loss = 0.00535330\n",
      "Iteration 10280, loss = 0.00535292\n",
      "Iteration 10281, loss = 0.00535297\n",
      "Iteration 10282, loss = 0.00535327\n",
      "Iteration 10283, loss = 0.00535369\n",
      "Iteration 10284, loss = 0.00535401\n",
      "Iteration 10285, loss = 0.00535417\n",
      "Iteration 10286, loss = 0.00535403\n",
      "Iteration 10287, loss = 0.00535366\n",
      "Iteration 10288, loss = 0.00535304\n",
      "Iteration 10289, loss = 0.00535231\n",
      "Iteration 10290, loss = 0.00535154\n",
      "Iteration 10291, loss = 0.00535084\n",
      "Iteration 10292, loss = 0.00535029\n",
      "Iteration 10293, loss = 0.00534990\n",
      "Iteration 10294, loss = 0.00534968\n",
      "Iteration 10295, loss = 0.00534960\n",
      "Iteration 10296, loss = 0.00534960\n",
      "Iteration 10297, loss = 0.00534963\n",
      "Iteration 10298, loss = 0.00534967\n",
      "Iteration 10299, loss = 0.00534968\n",
      "Iteration 10300, loss = 0.00534966\n",
      "Iteration 10301, loss = 0.00534959\n",
      "Iteration 10302, loss = 0.00534949\n",
      "Iteration 10303, loss = 0.00534934\n",
      "Iteration 10304, loss = 0.00534918\n",
      "Iteration 10305, loss = 0.00534899\n",
      "Iteration 10306, loss = 0.00534881\n",
      "Iteration 10307, loss = 0.00534862\n",
      "Iteration 10308, loss = 0.00534846\n",
      "Iteration 10309, loss = 0.00534831\n",
      "Iteration 10310, loss = 0.00534818\n",
      "Iteration 10311, loss = 0.00534807\n",
      "Iteration 10312, loss = 0.00534800\n",
      "Iteration 10313, loss = 0.00534795\n",
      "Iteration 10314, loss = 0.00534794\n",
      "Iteration 10315, loss = 0.00534795\n",
      "Iteration 10316, loss = 0.00534804\n",
      "Iteration 10317, loss = 0.00534816\n",
      "Iteration 10318, loss = 0.00534841\n",
      "Iteration 10319, loss = 0.00534873\n",
      "Iteration 10320, loss = 0.00534926\n",
      "Iteration 10321, loss = 0.00534993\n",
      "Iteration 10322, loss = 0.00535100\n",
      "Iteration 10323, loss = 0.00535233\n",
      "Iteration 10324, loss = 0.00535445\n",
      "Iteration 10325, loss = 0.00535702\n",
      "Iteration 10326, loss = 0.00536121\n",
      "Iteration 10327, loss = 0.00536608\n",
      "Iteration 10328, loss = 0.00537429\n",
      "Iteration 10329, loss = 0.00538318\n",
      "Iteration 10330, loss = 0.00539878\n",
      "Iteration 10331, loss = 0.00541366\n",
      "Iteration 10332, loss = 0.00544092\n",
      "Iteration 10333, loss = 0.00546141\n",
      "Iteration 10334, loss = 0.00550069\n",
      "Iteration 10335, loss = 0.00551808\n",
      "Iteration 10336, loss = 0.00555504\n",
      "Iteration 10337, loss = 0.00554984\n",
      "Iteration 10338, loss = 0.00555496\n",
      "Iteration 10339, loss = 0.00551438\n",
      "Iteration 10340, loss = 0.00547761\n",
      "Iteration 10341, loss = 0.00542371\n",
      "Iteration 10342, loss = 0.00538334\n",
      "Iteration 10343, loss = 0.00535777\n",
      "Iteration 10344, loss = 0.00535279\n",
      "Iteration 10345, loss = 0.00536413\n",
      "Iteration 10346, loss = 0.00538217\n",
      "Iteration 10347, loss = 0.00540050\n",
      "Iteration 10348, loss = 0.00540595\n",
      "Iteration 10349, loss = 0.00540340\n",
      "Iteration 10350, loss = 0.00538732\n",
      "Iteration 10351, loss = 0.00537011\n",
      "Iteration 10352, loss = 0.00535421\n",
      "Iteration 10353, loss = 0.00534591\n",
      "Iteration 10354, loss = 0.00534538\n",
      "Iteration 10355, loss = 0.00535062\n",
      "Iteration 10356, loss = 0.00535823\n",
      "Iteration 10357, loss = 0.00536375\n",
      "Iteration 10358, loss = 0.00536639\n",
      "Iteration 10359, loss = 0.00536331\n",
      "Iteration 10360, loss = 0.00535795\n",
      "Iteration 10361, loss = 0.00535054\n",
      "Iteration 10362, loss = 0.00534465\n",
      "Iteration 10363, loss = 0.00534130\n",
      "Iteration 10364, loss = 0.00534103\n",
      "Iteration 10365, loss = 0.00534308\n",
      "Iteration 10366, loss = 0.00534600\n",
      "Iteration 10367, loss = 0.00534860\n",
      "Iteration 10368, loss = 0.00534950\n",
      "Iteration 10369, loss = 0.00534907\n",
      "Iteration 10370, loss = 0.00534701\n",
      "Iteration 10371, loss = 0.00534456\n",
      "Iteration 10372, loss = 0.00534213\n",
      "Iteration 10373, loss = 0.00534041\n",
      "Iteration 10374, loss = 0.00533959\n",
      "Iteration 10375, loss = 0.00533960\n",
      "Iteration 10376, loss = 0.00534019\n",
      "Iteration 10377, loss = 0.00534097\n",
      "Iteration 10378, loss = 0.00534165\n",
      "Iteration 10379, loss = 0.00534193\n",
      "Iteration 10380, loss = 0.00534183\n",
      "Iteration 10381, loss = 0.00534127\n",
      "Iteration 10382, loss = 0.00534050\n",
      "Iteration 10383, loss = 0.00533962\n",
      "Iteration 10384, loss = 0.00533885\n",
      "Iteration 10385, loss = 0.00533826\n",
      "Iteration 10386, loss = 0.00533792\n",
      "Iteration 10387, loss = 0.00533780\n",
      "Iteration 10388, loss = 0.00533783\n",
      "Iteration 10389, loss = 0.00533793\n",
      "Iteration 10390, loss = 0.00533804\n",
      "Iteration 10391, loss = 0.00533812\n",
      "Iteration 10392, loss = 0.00533811\n",
      "Iteration 10393, loss = 0.00533804\n",
      "Iteration 10394, loss = 0.00533788\n",
      "Iteration 10395, loss = 0.00533769\n",
      "Iteration 10396, loss = 0.00533744\n",
      "Iteration 10397, loss = 0.00533720\n",
      "Iteration 10398, loss = 0.00533697\n",
      "Iteration 10399, loss = 0.00533678\n",
      "Iteration 10400, loss = 0.00533665\n",
      "Iteration 10401, loss = 0.00533660\n",
      "Iteration 10402, loss = 0.00533665\n",
      "Iteration 10403, loss = 0.00533681\n",
      "Iteration 10404, loss = 0.00533714\n",
      "Iteration 10405, loss = 0.00533768\n",
      "Iteration 10406, loss = 0.00533856\n",
      "Iteration 10407, loss = 0.00533986\n",
      "Iteration 10408, loss = 0.00534194\n",
      "Iteration 10409, loss = 0.00534494\n",
      "Iteration 10410, loss = 0.00534978\n",
      "Iteration 10411, loss = 0.00535649\n",
      "Iteration 10412, loss = 0.00536744\n",
      "Iteration 10413, loss = 0.00538158\n",
      "Iteration 10414, loss = 0.00540442\n",
      "Iteration 10415, loss = 0.00542947\n",
      "Iteration 10416, loss = 0.00546736\n",
      "Iteration 10417, loss = 0.00549372\n",
      "Iteration 10418, loss = 0.00552500\n",
      "Iteration 10419, loss = 0.00551335\n",
      "Iteration 10420, loss = 0.00548887\n",
      "Iteration 10421, loss = 0.00542828\n",
      "Iteration 10422, loss = 0.00537561\n",
      "Iteration 10423, loss = 0.00534328\n",
      "Iteration 10424, loss = 0.00534219\n",
      "Iteration 10425, loss = 0.00536241\n",
      "Iteration 10426, loss = 0.00538390\n",
      "Iteration 10427, loss = 0.00539490\n",
      "Iteration 10428, loss = 0.00538439\n",
      "Iteration 10429, loss = 0.00536692\n",
      "Iteration 10430, loss = 0.00534892\n",
      "Iteration 10431, loss = 0.00534355\n",
      "Iteration 10432, loss = 0.00534703\n",
      "Iteration 10433, loss = 0.00535325\n",
      "Iteration 10434, loss = 0.00535550\n",
      "Iteration 10435, loss = 0.00535127\n",
      "Iteration 10436, loss = 0.00534509\n",
      "Iteration 10437, loss = 0.00534085\n",
      "Iteration 10438, loss = 0.00534126\n",
      "Iteration 10439, loss = 0.00534344\n",
      "Iteration 10440, loss = 0.00534433\n",
      "Iteration 10441, loss = 0.00534216\n",
      "Iteration 10442, loss = 0.00533784\n",
      "Iteration 10443, loss = 0.00533450\n",
      "Iteration 10444, loss = 0.00533391\n",
      "Iteration 10445, loss = 0.00533595\n",
      "Iteration 10446, loss = 0.00533822\n",
      "Iteration 10447, loss = 0.00533903\n",
      "Iteration 10448, loss = 0.00533723\n",
      "Iteration 10449, loss = 0.00533417\n",
      "Iteration 10450, loss = 0.00533152\n",
      "Iteration 10451, loss = 0.00533065\n",
      "Iteration 10452, loss = 0.00533158\n",
      "Iteration 10453, loss = 0.00533327\n",
      "Iteration 10454, loss = 0.00533438\n",
      "Iteration 10455, loss = 0.00533421\n",
      "Iteration 10456, loss = 0.00533293\n",
      "Iteration 10457, loss = 0.00533141\n",
      "Iteration 10458, loss = 0.00533047\n",
      "Iteration 10459, loss = 0.00533049\n",
      "Iteration 10460, loss = 0.00533122\n",
      "Iteration 10461, loss = 0.00533224\n",
      "Iteration 10462, loss = 0.00533298\n",
      "Iteration 10463, loss = 0.00533365\n",
      "Iteration 10464, loss = 0.00533412\n",
      "Iteration 10465, loss = 0.00533539\n",
      "Iteration 10466, loss = 0.00533722\n",
      "Iteration 10467, loss = 0.00534076\n",
      "Iteration 10468, loss = 0.00534533\n",
      "Iteration 10469, loss = 0.00535279\n",
      "Iteration 10470, loss = 0.00536173\n",
      "Iteration 10471, loss = 0.00537665\n",
      "Iteration 10472, loss = 0.00539322\n",
      "Iteration 10473, loss = 0.00542206\n",
      "Iteration 10474, loss = 0.00544796\n",
      "Iteration 10475, loss = 0.00549421\n",
      "Iteration 10476, loss = 0.00551718\n",
      "Iteration 10477, loss = 0.00556101\n",
      "Iteration 10478, loss = 0.00554602\n",
      "Iteration 10479, loss = 0.00553857\n",
      "Iteration 10480, loss = 0.00547460\n",
      "Iteration 10481, loss = 0.00541890\n",
      "Iteration 10482, loss = 0.00536413\n",
      "Iteration 10483, loss = 0.00533574\n",
      "Iteration 10484, loss = 0.00533389\n",
      "Iteration 10485, loss = 0.00535115\n",
      "Iteration 10486, loss = 0.00537633\n",
      "Iteration 10487, loss = 0.00539198\n",
      "Iteration 10488, loss = 0.00539841\n",
      "Iteration 10489, loss = 0.00538495\n",
      "Iteration 10490, loss = 0.00536653\n",
      "Iteration 10491, loss = 0.00534623\n",
      "Iteration 10492, loss = 0.00533428\n",
      "Iteration 10493, loss = 0.00533184\n",
      "Iteration 10494, loss = 0.00533670\n",
      "Iteration 10495, loss = 0.00534406\n",
      "Iteration 10496, loss = 0.00534845\n",
      "Iteration 10497, loss = 0.00534932\n",
      "Iteration 10498, loss = 0.00534534\n",
      "Iteration 10499, loss = 0.00534042\n",
      "Iteration 10500, loss = 0.00533532\n",
      "Iteration 10501, loss = 0.00533228\n",
      "Iteration 10502, loss = 0.00533075\n",
      "Iteration 10503, loss = 0.00533047\n",
      "Iteration 10504, loss = 0.00533065\n",
      "Iteration 10505, loss = 0.00533069\n",
      "Iteration 10506, loss = 0.00533070\n",
      "Iteration 10507, loss = 0.00533038\n",
      "Iteration 10508, loss = 0.00533017\n",
      "Iteration 10509, loss = 0.00532966\n",
      "Iteration 10510, loss = 0.00532913\n",
      "Iteration 10511, loss = 0.00532821\n",
      "Iteration 10512, loss = 0.00532716\n",
      "Iteration 10513, loss = 0.00532600\n",
      "Iteration 10514, loss = 0.00532500\n",
      "Iteration 10515, loss = 0.00532439\n",
      "Iteration 10516, loss = 0.00532423\n",
      "Iteration 10517, loss = 0.00532450\n",
      "Iteration 10518, loss = 0.00532496\n",
      "Iteration 10519, loss = 0.00532540\n",
      "Iteration 10520, loss = 0.00532550\n",
      "Iteration 10521, loss = 0.00532525\n",
      "Iteration 10522, loss = 0.00532459\n",
      "Iteration 10523, loss = 0.00532376\n",
      "Iteration 10524, loss = 0.00532293\n",
      "Iteration 10525, loss = 0.00532229\n",
      "Iteration 10526, loss = 0.00532194\n",
      "Iteration 10527, loss = 0.00532187\n",
      "Iteration 10528, loss = 0.00532199\n",
      "Iteration 10529, loss = 0.00532220\n",
      "Iteration 10530, loss = 0.00532237\n",
      "Iteration 10531, loss = 0.00532244\n",
      "Iteration 10532, loss = 0.00532238\n",
      "Iteration 10533, loss = 0.00532219\n",
      "Iteration 10534, loss = 0.00532193\n",
      "Iteration 10535, loss = 0.00532162\n",
      "Iteration 10536, loss = 0.00532133\n",
      "Iteration 10537, loss = 0.00532107\n",
      "Iteration 10538, loss = 0.00532086\n",
      "Iteration 10539, loss = 0.00532070\n",
      "Iteration 10540, loss = 0.00532057\n",
      "Iteration 10541, loss = 0.00532046\n",
      "Iteration 10542, loss = 0.00532036\n",
      "Iteration 10543, loss = 0.00532026\n",
      "Iteration 10544, loss = 0.00532014\n",
      "Iteration 10545, loss = 0.00532002\n",
      "Iteration 10546, loss = 0.00531990\n",
      "Iteration 10547, loss = 0.00531979\n",
      "Iteration 10548, loss = 0.00531968\n",
      "Iteration 10549, loss = 0.00531958\n",
      "Iteration 10550, loss = 0.00531950\n",
      "Iteration 10551, loss = 0.00531942\n",
      "Iteration 10552, loss = 0.00531935\n",
      "Iteration 10553, loss = 0.00531928\n",
      "Iteration 10554, loss = 0.00531921\n",
      "Iteration 10555, loss = 0.00531914\n",
      "Iteration 10556, loss = 0.00531907\n",
      "Iteration 10557, loss = 0.00531902\n",
      "Iteration 10558, loss = 0.00531896\n",
      "Iteration 10559, loss = 0.00531892\n",
      "Iteration 10560, loss = 0.00531889\n",
      "Iteration 10561, loss = 0.00531890\n",
      "Iteration 10562, loss = 0.00531893\n",
      "Iteration 10563, loss = 0.00531902\n",
      "Iteration 10564, loss = 0.00531916\n",
      "Iteration 10565, loss = 0.00531941\n",
      "Iteration 10566, loss = 0.00531977\n",
      "Iteration 10567, loss = 0.00532034\n",
      "Iteration 10568, loss = 0.00532111\n",
      "Iteration 10569, loss = 0.00532231\n",
      "Iteration 10570, loss = 0.00532390\n",
      "Iteration 10571, loss = 0.00532642\n",
      "Iteration 10572, loss = 0.00532966\n",
      "Iteration 10573, loss = 0.00533492\n",
      "Iteration 10574, loss = 0.00534138\n",
      "Iteration 10575, loss = 0.00535224\n",
      "Iteration 10576, loss = 0.00536456\n",
      "Iteration 10577, loss = 0.00538602\n",
      "Iteration 10578, loss = 0.00540682\n",
      "Iteration 10579, loss = 0.00544424\n",
      "Iteration 10580, loss = 0.00546997\n",
      "Iteration 10581, loss = 0.00551793\n",
      "Iteration 10582, loss = 0.00552763\n",
      "Iteration 10583, loss = 0.00555374\n",
      "Iteration 10584, loss = 0.00552142\n",
      "Iteration 10585, loss = 0.00549712\n",
      "Iteration 10586, loss = 0.00544107\n",
      "Iteration 10587, loss = 0.00540097\n",
      "Iteration 10588, loss = 0.00536811\n",
      "Iteration 10589, loss = 0.00535518\n",
      "Iteration 10590, loss = 0.00535406\n",
      "Iteration 10591, loss = 0.00535706\n",
      "Iteration 10592, loss = 0.00536031\n",
      "Iteration 10593, loss = 0.00535609\n",
      "Iteration 10594, loss = 0.00535159\n",
      "Iteration 10595, loss = 0.00534675\n",
      "Iteration 10596, loss = 0.00534728\n",
      "Iteration 10597, loss = 0.00534927\n",
      "Iteration 10598, loss = 0.00535195\n",
      "Iteration 10599, loss = 0.00534928\n",
      "Iteration 10600, loss = 0.00534166\n",
      "Iteration 10601, loss = 0.00533081\n",
      "Iteration 10602, loss = 0.00532094\n",
      "Iteration 10603, loss = 0.00531617\n",
      "Iteration 10604, loss = 0.00531753\n",
      "Iteration 10605, loss = 0.00532325\n",
      "Iteration 10606, loss = 0.00532944\n",
      "Iteration 10607, loss = 0.00533312\n",
      "Iteration 10608, loss = 0.00533214\n",
      "Iteration 10609, loss = 0.00532775\n",
      "Iteration 10610, loss = 0.00532172\n",
      "Iteration 10611, loss = 0.00531687\n",
      "Iteration 10612, loss = 0.00531446\n",
      "Iteration 10613, loss = 0.00531457\n",
      "Iteration 10614, loss = 0.00531609\n",
      "Iteration 10615, loss = 0.00531769\n",
      "Iteration 10616, loss = 0.00531835\n",
      "Iteration 10617, loss = 0.00531797\n",
      "Iteration 10618, loss = 0.00531696\n",
      "Iteration 10619, loss = 0.00531589\n",
      "Iteration 10620, loss = 0.00531531\n",
      "Iteration 10621, loss = 0.00531516\n",
      "Iteration 10622, loss = 0.00531530\n",
      "Iteration 10623, loss = 0.00531527\n",
      "Iteration 10624, loss = 0.00531493\n",
      "Iteration 10625, loss = 0.00531416\n",
      "Iteration 10626, loss = 0.00531322\n",
      "Iteration 10627, loss = 0.00531232\n",
      "Iteration 10628, loss = 0.00531172\n",
      "Iteration 10629, loss = 0.00531152\n",
      "Iteration 10630, loss = 0.00531165\n",
      "Iteration 10631, loss = 0.00531197\n",
      "Iteration 10632, loss = 0.00531229\n",
      "Iteration 10633, loss = 0.00531249\n",
      "Iteration 10634, loss = 0.00531248\n",
      "Iteration 10635, loss = 0.00531231\n",
      "Iteration 10636, loss = 0.00531201\n",
      "Iteration 10637, loss = 0.00531169\n",
      "Iteration 10638, loss = 0.00531138\n",
      "Iteration 10639, loss = 0.00531115\n",
      "Iteration 10640, loss = 0.00531098\n",
      "Iteration 10641, loss = 0.00531086\n",
      "Iteration 10642, loss = 0.00531075\n",
      "Iteration 10643, loss = 0.00531063\n",
      "Iteration 10644, loss = 0.00531048\n",
      "Iteration 10645, loss = 0.00531031\n",
      "Iteration 10646, loss = 0.00531011\n",
      "Iteration 10647, loss = 0.00530991\n",
      "Iteration 10648, loss = 0.00530972\n",
      "Iteration 10649, loss = 0.00530955\n",
      "Iteration 10650, loss = 0.00530940\n",
      "Iteration 10651, loss = 0.00530929\n",
      "Iteration 10652, loss = 0.00530919\n",
      "Iteration 10653, loss = 0.00530911\n",
      "Iteration 10654, loss = 0.00530903\n",
      "Iteration 10655, loss = 0.00530895\n",
      "Iteration 10656, loss = 0.00530887\n",
      "Iteration 10657, loss = 0.00530877\n",
      "Iteration 10658, loss = 0.00530867\n",
      "Iteration 10659, loss = 0.00530856\n",
      "Iteration 10660, loss = 0.00530844\n",
      "Iteration 10661, loss = 0.00530833\n",
      "Iteration 10662, loss = 0.00530821\n",
      "Iteration 10663, loss = 0.00530810\n",
      "Iteration 10664, loss = 0.00530800\n",
      "Iteration 10665, loss = 0.00530790\n",
      "Iteration 10666, loss = 0.00530781\n",
      "Iteration 10667, loss = 0.00530772\n",
      "Iteration 10668, loss = 0.00530764\n",
      "Iteration 10669, loss = 0.00530756\n",
      "Iteration 10670, loss = 0.00530749\n",
      "Iteration 10671, loss = 0.00530743\n",
      "Iteration 10672, loss = 0.00530737\n",
      "Iteration 10673, loss = 0.00530734\n",
      "Iteration 10674, loss = 0.00530733\n",
      "Iteration 10675, loss = 0.00530737\n",
      "Iteration 10676, loss = 0.00530748\n",
      "Iteration 10677, loss = 0.00530769\n",
      "Iteration 10678, loss = 0.00530805\n",
      "Iteration 10679, loss = 0.00530868\n",
      "Iteration 10680, loss = 0.00530967\n",
      "Iteration 10681, loss = 0.00531135\n",
      "Iteration 10682, loss = 0.00531386\n",
      "Iteration 10683, loss = 0.00531821\n",
      "Iteration 10684, loss = 0.00532448\n",
      "Iteration 10685, loss = 0.00533568\n",
      "Iteration 10686, loss = 0.00535075\n",
      "Iteration 10687, loss = 0.00537882\n",
      "Iteration 10688, loss = 0.00541161\n",
      "Iteration 10689, loss = 0.00547507\n",
      "Iteration 10690, loss = 0.00552869\n",
      "Iteration 10691, loss = 0.00563379\n",
      "Iteration 10692, loss = 0.00566521\n",
      "Iteration 10693, loss = 0.00573263\n",
      "Iteration 10694, loss = 0.00565913\n",
      "Iteration 10695, loss = 0.00559114\n",
      "Iteration 10696, loss = 0.00547002\n",
      "Iteration 10697, loss = 0.00539988\n",
      "Iteration 10698, loss = 0.00537903\n",
      "Iteration 10699, loss = 0.00540645\n",
      "Iteration 10700, loss = 0.00544653\n",
      "Iteration 10701, loss = 0.00544774\n",
      "Iteration 10702, loss = 0.00542187\n",
      "Iteration 10703, loss = 0.00536562\n",
      "Iteration 10704, loss = 0.00532804\n",
      "Iteration 10705, loss = 0.00532581\n",
      "Iteration 10706, loss = 0.00535316\n",
      "Iteration 10707, loss = 0.00538317\n",
      "Iteration 10708, loss = 0.00538625\n",
      "Iteration 10709, loss = 0.00536274\n",
      "Iteration 10710, loss = 0.00532670\n",
      "Iteration 10711, loss = 0.00530580\n",
      "Iteration 10712, loss = 0.00531019\n",
      "Iteration 10713, loss = 0.00532952\n",
      "Iteration 10714, loss = 0.00534520\n",
      "Iteration 10715, loss = 0.00534299\n",
      "Iteration 10716, loss = 0.00532764\n",
      "Iteration 10717, loss = 0.00531165\n",
      "Iteration 10718, loss = 0.00530641\n",
      "Iteration 10719, loss = 0.00531226\n",
      "Iteration 10720, loss = 0.00532025\n",
      "Iteration 10721, loss = 0.00532262\n",
      "Iteration 10722, loss = 0.00531675\n",
      "Iteration 10723, loss = 0.00530874\n",
      "Iteration 10724, loss = 0.00530445\n",
      "Iteration 10725, loss = 0.00530615\n",
      "Iteration 10726, loss = 0.00531088\n",
      "Iteration 10727, loss = 0.00531365\n",
      "Iteration 10728, loss = 0.00531248\n",
      "Iteration 10729, loss = 0.00530795\n",
      "Iteration 10730, loss = 0.00530372\n",
      "Iteration 10731, loss = 0.00530211\n",
      "Iteration 10732, loss = 0.00530331\n",
      "Iteration 10733, loss = 0.00530552\n",
      "Iteration 10734, loss = 0.00530666\n",
      "Iteration 10735, loss = 0.00530613\n",
      "Iteration 10736, loss = 0.00530436\n",
      "Iteration 10737, loss = 0.00530281\n",
      "Iteration 10738, loss = 0.00530216\n",
      "Iteration 10739, loss = 0.00530245\n",
      "Iteration 10740, loss = 0.00530299\n",
      "Iteration 10741, loss = 0.00530314\n",
      "Iteration 10742, loss = 0.00530273\n",
      "Iteration 10743, loss = 0.00530193\n",
      "Iteration 10744, loss = 0.00530120\n",
      "Iteration 10745, loss = 0.00530079\n",
      "Iteration 10746, loss = 0.00530079\n",
      "Iteration 10747, loss = 0.00530102\n",
      "Iteration 10748, loss = 0.00530122\n",
      "Iteration 10749, loss = 0.00530124\n",
      "Iteration 10750, loss = 0.00530102\n",
      "Iteration 10751, loss = 0.00530066\n",
      "Iteration 10752, loss = 0.00530029\n",
      "Iteration 10753, loss = 0.00530003\n",
      "Iteration 10754, loss = 0.00529991\n",
      "Iteration 10755, loss = 0.00529989\n",
      "Iteration 10756, loss = 0.00529988\n",
      "Iteration 10757, loss = 0.00529982\n",
      "Iteration 10758, loss = 0.00529967\n",
      "Iteration 10759, loss = 0.00529944\n",
      "Iteration 10760, loss = 0.00529921\n",
      "Iteration 10761, loss = 0.00529902\n",
      "Iteration 10762, loss = 0.00529891\n",
      "Iteration 10763, loss = 0.00529888\n",
      "Iteration 10764, loss = 0.00529887\n",
      "Iteration 10765, loss = 0.00529884\n",
      "Iteration 10766, loss = 0.00529875\n",
      "Iteration 10767, loss = 0.00529862\n",
      "Iteration 10768, loss = 0.00529847\n",
      "Iteration 10769, loss = 0.00529832\n",
      "Iteration 10770, loss = 0.00529822\n",
      "Iteration 10771, loss = 0.00529814\n",
      "Iteration 10772, loss = 0.00529810\n",
      "Iteration 10773, loss = 0.00529806\n",
      "Iteration 10774, loss = 0.00529800\n",
      "Iteration 10775, loss = 0.00529792\n",
      "Iteration 10776, loss = 0.00529783\n",
      "Iteration 10777, loss = 0.00529773\n",
      "Iteration 10778, loss = 0.00529765\n",
      "Iteration 10779, loss = 0.00529759\n",
      "Iteration 10780, loss = 0.00529755\n",
      "Iteration 10781, loss = 0.00529755\n",
      "Iteration 10782, loss = 0.00529757\n",
      "Iteration 10783, loss = 0.00529763\n",
      "Iteration 10784, loss = 0.00529772\n",
      "Iteration 10785, loss = 0.00529787\n",
      "Iteration 10786, loss = 0.00529811\n",
      "Iteration 10787, loss = 0.00529851\n",
      "Iteration 10788, loss = 0.00529908\n",
      "Iteration 10789, loss = 0.00529999\n",
      "Iteration 10790, loss = 0.00530126\n",
      "Iteration 10791, loss = 0.00530325\n",
      "Iteration 10792, loss = 0.00530596\n",
      "Iteration 10793, loss = 0.00531027\n",
      "Iteration 10794, loss = 0.00531593\n",
      "Iteration 10795, loss = 0.00532509\n",
      "Iteration 10796, loss = 0.00533626\n",
      "Iteration 10797, loss = 0.00535456\n",
      "Iteration 10798, loss = 0.00537369\n",
      "Iteration 10799, loss = 0.00540464\n",
      "Iteration 10800, loss = 0.00542685\n",
      "Iteration 10801, loss = 0.00546080\n",
      "Iteration 10802, loss = 0.00546208\n",
      "Iteration 10803, loss = 0.00546540\n",
      "Iteration 10804, loss = 0.00542531\n",
      "Iteration 10805, loss = 0.00538940\n",
      "Iteration 10806, loss = 0.00534564\n",
      "Iteration 10807, loss = 0.00532283\n",
      "Iteration 10808, loss = 0.00531615\n",
      "Iteration 10809, loss = 0.00532277\n",
      "Iteration 10810, loss = 0.00533162\n",
      "Iteration 10811, loss = 0.00533357\n",
      "Iteration 10812, loss = 0.00532947\n",
      "Iteration 10813, loss = 0.00531966\n",
      "Iteration 10814, loss = 0.00531392\n",
      "Iteration 10815, loss = 0.00531281\n",
      "Iteration 10816, loss = 0.00531775\n",
      "Iteration 10817, loss = 0.00532156\n",
      "Iteration 10818, loss = 0.00532201\n",
      "Iteration 10819, loss = 0.00531580\n",
      "Iteration 10820, loss = 0.00530660\n",
      "Iteration 10821, loss = 0.00529834\n",
      "Iteration 10822, loss = 0.00529459\n",
      "Iteration 10823, loss = 0.00529598\n",
      "Iteration 10824, loss = 0.00530038\n",
      "Iteration 10825, loss = 0.00530482\n",
      "Iteration 10826, loss = 0.00530659\n",
      "Iteration 10827, loss = 0.00530559\n",
      "Iteration 10828, loss = 0.00530239\n",
      "Iteration 10829, loss = 0.00529913\n",
      "Iteration 10830, loss = 0.00529688\n",
      "Iteration 10831, loss = 0.00529615\n",
      "Iteration 10832, loss = 0.00529633\n",
      "Iteration 10833, loss = 0.00529664\n",
      "Iteration 10834, loss = 0.00529635\n",
      "Iteration 10835, loss = 0.00529551\n",
      "Iteration 10836, loss = 0.00529443\n",
      "Iteration 10837, loss = 0.00529368\n",
      "Iteration 10838, loss = 0.00529355\n",
      "Iteration 10839, loss = 0.00529397\n",
      "Iteration 10840, loss = 0.00529462\n",
      "Iteration 10841, loss = 0.00529506\n",
      "Iteration 10842, loss = 0.00529512\n",
      "Iteration 10843, loss = 0.00529468\n",
      "Iteration 10844, loss = 0.00529401\n",
      "Iteration 10845, loss = 0.00529326\n",
      "Iteration 10846, loss = 0.00529270\n",
      "Iteration 10847, loss = 0.00529234\n",
      "Iteration 10848, loss = 0.00529220\n",
      "Iteration 10849, loss = 0.00529212\n",
      "Iteration 10850, loss = 0.00529203\n",
      "Iteration 10851, loss = 0.00529186\n",
      "Iteration 10852, loss = 0.00529161\n",
      "Iteration 10853, loss = 0.00529134\n",
      "Iteration 10854, loss = 0.00529111\n",
      "Iteration 10855, loss = 0.00529096\n",
      "Iteration 10856, loss = 0.00529089\n",
      "Iteration 10857, loss = 0.00529089\n",
      "Iteration 10858, loss = 0.00529091\n",
      "Iteration 10859, loss = 0.00529091\n",
      "Iteration 10860, loss = 0.00529088\n",
      "Iteration 10861, loss = 0.00529082\n",
      "Iteration 10862, loss = 0.00529073\n",
      "Iteration 10863, loss = 0.00529063\n",
      "Iteration 10864, loss = 0.00529056\n",
      "Iteration 10865, loss = 0.00529051\n",
      "Iteration 10866, loss = 0.00529050\n",
      "Iteration 10867, loss = 0.00529052\n",
      "Iteration 10868, loss = 0.00529057\n",
      "Iteration 10869, loss = 0.00529066\n",
      "Iteration 10870, loss = 0.00529077\n",
      "Iteration 10871, loss = 0.00529093\n",
      "Iteration 10872, loss = 0.00529114\n",
      "Iteration 10873, loss = 0.00529148\n",
      "Iteration 10874, loss = 0.00529192\n",
      "Iteration 10875, loss = 0.00529261\n",
      "Iteration 10876, loss = 0.00529349\n",
      "Iteration 10877, loss = 0.00529488\n",
      "Iteration 10878, loss = 0.00529659\n",
      "Iteration 10879, loss = 0.00529932\n",
      "Iteration 10880, loss = 0.00530255\n",
      "Iteration 10881, loss = 0.00530788\n",
      "Iteration 10882, loss = 0.00531390\n",
      "Iteration 10883, loss = 0.00532416\n",
      "Iteration 10884, loss = 0.00533485\n",
      "Iteration 10885, loss = 0.00535375\n",
      "Iteration 10886, loss = 0.00537077\n",
      "Iteration 10887, loss = 0.00540185\n",
      "Iteration 10888, loss = 0.00542295\n",
      "Iteration 10889, loss = 0.00546277\n",
      "Iteration 10890, loss = 0.00547543\n",
      "Iteration 10891, loss = 0.00550331\n",
      "Iteration 10892, loss = 0.00548686\n",
      "Iteration 10893, loss = 0.00547517\n",
      "Iteration 10894, loss = 0.00542683\n",
      "Iteration 10895, loss = 0.00538354\n",
      "Iteration 10896, loss = 0.00533733\n",
      "Iteration 10897, loss = 0.00530807\n",
      "Iteration 10898, loss = 0.00529626\n",
      "Iteration 10899, loss = 0.00530076\n",
      "Iteration 10900, loss = 0.00531556\n",
      "Iteration 10901, loss = 0.00533059\n",
      "Iteration 10902, loss = 0.00534273\n",
      "Iteration 10903, loss = 0.00534278\n",
      "Iteration 10904, loss = 0.00533681\n",
      "Iteration 10905, loss = 0.00532272\n",
      "Iteration 10906, loss = 0.00530942\n",
      "Iteration 10907, loss = 0.00529860\n",
      "Iteration 10908, loss = 0.00529364\n",
      "Iteration 10909, loss = 0.00529369\n",
      "Iteration 10910, loss = 0.00529683\n",
      "Iteration 10911, loss = 0.00530088\n",
      "Iteration 10912, loss = 0.00530323\n",
      "Iteration 10913, loss = 0.00530390\n",
      "Iteration 10914, loss = 0.00530196\n",
      "Iteration 10915, loss = 0.00529928\n",
      "Iteration 10916, loss = 0.00529588\n",
      "Iteration 10917, loss = 0.00529326\n",
      "Training loss did not improve more than tol=0.000000 for 50 consecutive epochs. Stopping.\n",
      "Mean Squared Error: 0.16718689815933527\n"
     ]
    }
   ],
   "source": [
    "# separa os os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(y, dy, test_size=0.2, random_state=42)\n",
    "\n",
    "neurons = 10\n",
    "layers = 10\n",
    "\n",
    "# define modelo\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=tuple([neurons] * layers), \n",
    "    activation='tanh',\n",
    "    solver='adam',\n",
    "    max_iter=100000, \n",
    "    random_state=42,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init = 0.001,\n",
    "    n_iter_no_change = 50,\n",
    "    tol = 1e-8,\n",
    "    verbose=True)\n",
    "\n",
    "# treinamento\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# teste\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6510c078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "new_x = np.linspace(0, 1, y.shape[1]).reshape(1,-1)\n",
    "\n",
    "# test 1\n",
    "plt.subplot(131)\n",
    "new_y = np.sin(2*np.pi*new_x)\n",
    "new_dy = np.cos(2*np.pi*new_x)\n",
    "predicted_derivative = model.predict(new_y)\n",
    "\n",
    "plt.plot(new_x[0], new_y[0], label='Input', color='black')\n",
    "plt.plot(new_x[0], new_dy[0], label='True dy/dx', color='blue')\n",
    "plt.plot(new_x[0], predicted_derivative[0], label='Predicted dy/dx', color='red', linestyle='dashed')\n",
    "plt.ylim(-2,2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dy/dx')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# test 2\n",
    "plt.subplot(132)\n",
    "new_y = np.cos(2*np.pi*new_x)\n",
    "new_dy = -np.sin(2*np.pi*new_x)\n",
    "predicted_derivative = model.predict(new_y)\n",
    "\n",
    "plt.plot(new_x[0], new_y[0], label='Input', color='black')\n",
    "plt.plot(new_x[0], new_dy[0], label='True dy/dx', color='blue')\n",
    "plt.plot(new_x[0], predicted_derivative[0], label='Predicted dy/dx', color='red', linestyle='dashed')\n",
    "plt.ylim(-2,2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dy/dx')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# test 3\n",
    "plt.subplot(133)\n",
    "new_y = new_x ** 2\n",
    "new_dy = 2 * new_x\n",
    "predicted_derivative = model.predict(new_y)\n",
    "\n",
    "plt.plot(new_x[0], new_y[0], label='Input', color='black')\n",
    "plt.plot(new_x[0], new_dy[0], label='True dy/dx', color='blue')\n",
    "plt.plot(new_x[0], predicted_derivative[0], label='Predicted dy/dx', color='red', linestyle='dashed')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dy/dx')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48535c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
